{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "import data_func.read_data as read_data\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"catboost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = read_data.get_training_data()\n",
    "X_frames_train = dataframes[0]\n",
    "Y_frames_train = dataframes[1]\n",
    "X_frames_test = read_data.get_test_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that target values line up with x_values\n",
    "\n",
    "def data_allign(x_train, y_train):\n",
    "  \n",
    "  y_train.dropna(inplace=True)\n",
    "  combined_data = pd.merge(x_train, y_train, left_on='date_forecast', right_on='time')\n",
    "  y_train = combined_data['pv_measurement']\n",
    "\n",
    "  if 'time' and 'pv_measurement' in combined_data.columns:\n",
    "    combined_data.drop(columns=['time', 'pv_measurement'], inplace=True)\n",
    "    \n",
    "  return combined_data, y_train\n",
    "\n",
    "import data_func.aggregation as data_agg\n",
    "\n",
    "categorical_features = ['date_forecast', 'dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx', 'precip_type_5min:idx']\n",
    "categorical_features_to_drop = ['dew_or_rime:idx', 'is_day:idx', 'is_in_shadow:idx', 'precip_type_5min:idx']\n",
    "\n",
    "for i in range(len(X_frames_train)):\n",
    "  categorical_frame = X_frames_train[i][categorical_features]\n",
    "  categorical_frame = data_agg.gen_agg(categorical_frame, data_agg.stocastic_median)\n",
    "\n",
    "  X_frames_train[i] = X_frames_train[i].drop(columns=categorical_features_to_drop)\n",
    "  X_frames_train[i] = data_agg.gen_agg(X_frames_train[i], 'mean')\n",
    "  X_frames_train[i] = pd.merge(X_frames_train[i], categorical_frame, on='date_forecast')\n",
    "  X_frames_train[i], Y_frames_train[i] = data_allign(X_frames_train[i], Y_frames_train[i])\n",
    "\n",
    "for j in range(len(X_frames_test)):\n",
    "  categorical_frame = X_frames_test[i][categorical_features]\n",
    "  categorical_frame = data_agg.gen_agg(categorical_frame, data_agg.stocastic_median)\n",
    "\n",
    "  X_frames_test[i] = X_frames_test[i].drop(columns=categorical_features_to_drop)\n",
    "  X_frames_test[i] = pd.merge(X_frames_test[i], categorical_frame, on='date_forecast')\n",
    "  X_frames_test[j] = data_agg.gen_agg(X_frames_test[j], 'mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_func.timeseasonality as DTS\n",
    "import data_func.date_forecast as DTF\n",
    "import data_func.one_hot_encoding as OHE\n",
    "\n",
    "categorical_features_to_one_hot = ['dew_or_rime:idx', 'precip_type_5min:idx']\n",
    "\n",
    "for i in range(len(X_frames_train)):\n",
    "    X_frames_train[i] = DTS.append_seasonal_columns(X_frames_train[i])\n",
    "    X_frames_train[i] = DTF.date_forecast_columns(X_frames_train[i])\n",
    "    # X_frames_train[i] = OHE.one_hot_encode(X_frames_train[i], categorical_features_to_one_hot)\n",
    "    X_frames_train[i].drop(columns=['snow_drift:idx'], inplace=True)\n",
    "\n",
    "    # X_frames_train[i].drop(columns=['absolute_humidity_2m:gm3'], inplace=True)\n",
    "    # X_frames_train[i].drop(columns=['air_density_2m:kgm3'], inplace=True)\n",
    "    # X_frames_train[i]['ceiling_height_agl:m'] = X_frames_train[i]['ceiling_height_agl:m'].fillna(0)\n",
    "    # X_frames_train[i]['cloud_base_agl:m'] = X_frames_train[i]['cloud_base_agl:m'].fillna(100000)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(X_frames_test)):\n",
    "    X_frames_test[i] = DTS.append_seasonal_columns(X_frames_test[i])\n",
    "    X_frames_test[i] = DTF.date_forecast_columns(X_frames_test[i])\n",
    "    # X_frames_test[i] = OHE.one_hot_encode(X_frames_test[i], categorical_features_to_one_hot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb # Change to model to optimize\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train_a, x_val_a, y_train_a, y_val_a = train_test_split(X_frames_train[0], Y_frames_train[0], test_size=0.17, random_state=None)\n",
    "x_train_b, x_val_b, y_train_b, y_val_b = train_test_split(X_frames_train[1], Y_frames_train[1], test_size=0.17, random_state=None)\n",
    "x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(X_frames_train[2], Y_frames_train[2], test_size=0.17, random_state=None)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBRegressor(**params) # Change to model to optimize\n",
    "    model.fit(x_train_c, y_train_c)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred = model.predict(x_val_c)\n",
    "\n",
    "    # Calculate the Mean Squared Error (MSE) as the metric to optimize\n",
    "    mae = mean_absolute_error(y_val_c, y_pred)\n",
    "\n",
    "    return mae\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "best = study.best_params\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", best)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import catboost as cat # Change to model to optimize\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x_train_a, x_val_a, y_train_a, y_val_a = train_test_split(X_frames_train[0], Y_frames_train[0], test_size=0.17, random_state=None)\n",
    "x_train_b, x_val_b, y_train_b, y_val_b = train_test_split(X_frames_train[1], Y_frames_train[1], test_size=0.17, random_state=None)\n",
    "x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(X_frames_train[2], Y_frames_train[2], test_size=0.17, random_state=None)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    params = {\n",
    "        \"iterations\": 1000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "        \"cat_features\": categorical_features_to_drop,\n",
    "        \"random_seed\": 42,\n",
    "        \"loss_function\": \"MAE\",\n",
    "        \"eval_metric\": \"MAE\",\n",
    "    }\n",
    "\n",
    "    model = cat.CatBoostRegressor(**params) # Change to model to optimize\n",
    "    model.fit(x_train_c, y_train_c)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred = model.predict(x_val_c)\n",
    "\n",
    "    # Calculate the Mean Squared Error (MSE) as the metric to optimize\n",
    "    mae = mean_absolute_error(y_val_c, y_pred)\n",
    "\n",
    "    return mae\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "best = study.best_params\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
