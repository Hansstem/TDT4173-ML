{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1674,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1675,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')\n",
    "\n",
    "train_targets_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "train_targets_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "train_targets_c = pd.read_parquet('../data/C/train_targets.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1676,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data set A, B and C clean up\n",
    "\n",
    "def data_clean_up(x_train_est, x_train_observe, y_train):\n",
    "\n",
    "  if 'date_calc' in x_train_est.columns:\n",
    "    x_train_est.drop(columns=\"date_calc\", inplace=True)\n",
    "\n",
    "  x_train = pd.concat([x_train_observe, x_train_est])\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_train.columns[1:]}\n",
    "  X_train_downscaled = x_train.groupby(x_train.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  y_train.dropna(inplace=True)\n",
    "  combined_data = pd.merge(X_train_downscaled, y_train, left_on='date_forecast', right_on='time')\n",
    "  y_train = combined_data['pv_measurement']\n",
    "\n",
    "  if 'date_forecast' and 'time' and 'pv_measurement' in combined_data.columns:\n",
    "    combined_data.drop(columns=\"date_forecast\", inplace=True)\n",
    "    combined_data.drop(columns=\"time\", inplace=True)\n",
    "    combined_data.drop(columns=\"pv_measurement\", inplace=True)\n",
    "\n",
    "\n",
    "  return combined_data, y_train\n",
    "\n",
    "x_train_a, y_train_a = data_clean_up(X_train_estimated_a, X_train_observed_a, train_targets_a)\n",
    "x_train_b, y_train_b = data_clean_up(X_train_estimated_b, X_train_observed_b, train_targets_b)\n",
    "x_train_c, y_train_c = data_clean_up(X_train_estimated_c, X_train_observed_c, train_targets_c)\n",
    "\n",
    "\n",
    "def data_clean_up_test(x_test_est):\n",
    "\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_test_est.columns[1:]}\n",
    "  X_test_downscaled = x_test_est.groupby(x_test_est.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  if 'date_forecast' in X_test_downscaled.columns:\n",
    "    X_test_downscaled.drop(columns=\"date_forecast\", inplace=True)\n",
    "\n",
    "  return X_test_downscaled\n",
    "\n",
    "X_test_estimated_a = data_clean_up_test(X_test_estimated_a)\n",
    "X_test_estimated_b = data_clean_up_test(X_test_estimated_b)\n",
    "X_test_estimated_c = data_clean_up_test(X_test_estimated_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1677,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do feature selection etc.\n",
    "\n",
    "# Polynomial features of degree 2 of most important features\n",
    "\n",
    "def polynomial_feature(x_dataset, features):\n",
    "  \n",
    "  for feature in features:\n",
    "    x_dataset[feature + ':squared'] = x_dataset[feature] ** 2\n",
    "    x_dataset[feature + ':cubed'] = x_dataset[feature] ** 3\n",
    "    x_dataset[feature + ':sqrt'] = np.sqrt(x_dataset[feature])\n",
    "\n",
    "  return x_dataset\n",
    "\n",
    "\n",
    "# x_train_a = polynomial_feature(x_train_a, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "# x_train_b = polynomial_feature(x_train_b, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "# x_train_c = polynomial_feature(x_train_c, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "\n",
    "# X_test_estimated_a = polynomial_feature(X_test_estimated_a, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "# X_test_estimated_b = polynomial_feature(X_test_estimated_b, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "# X_test_estimated_c = polynomial_feature(X_test_estimated_c, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "\n",
    "def drop_features(x_dataset, features):\n",
    "  for feature in features:\n",
    "    x_dataset.drop(columns=feature, inplace=True)\n",
    "\n",
    "  return x_dataset\n",
    "\n",
    "# x_train_a = drop_features(x_train_a, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "# x_train_b = drop_features(x_train_b, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "# x_train_c = drop_features(x_train_c, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "\n",
    "# X_test_estimated_a = drop_features(X_test_estimated_a, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "# X_test_estimated_b = drop_features(X_test_estimated_b, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "# X_test_estimated_c = drop_features(X_test_estimated_c, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-205 {color: black;background-color: white;}#sk-container-id-205 pre{padding: 0;}#sk-container-id-205 div.sk-toggleable {background-color: white;}#sk-container-id-205 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-205 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-205 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-205 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-205 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-205 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-205 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-205 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-205 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-205 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-205 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-205 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-205 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-205 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-205 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-205 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-205 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-205 div.sk-item {position: relative;z-index: 1;}#sk-container-id-205 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-205 div.sk-item::before, #sk-container-id-205 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-205 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-205 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-205 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-205 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-205 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-205 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-205 div.sk-label-container {text-align: center;}#sk-container-id-205 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-205 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-205\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=8, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.24, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=3, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-205\" type=\"checkbox\" checked><label for=\"sk-estimator-id-205\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=8, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.24, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=3, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=8, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.24, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "             min_child_weight=3, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 1678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "x_train_a, x_val_a, y_train_a, y_val_a = train_test_split(x_train_a, y_train_a, test_size=0.17, random_state=42)\n",
    "x_train_b, x_val_b, y_train_b, y_val_b = train_test_split(x_train_b, y_train_b, test_size=0.17, random_state=42)\n",
    "x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(x_train_c, y_train_c, test_size=0.17, random_state=42)\n",
    "\n",
    "model_a = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.25, max_depth=10, min_child_weight=2, gamma=150, reg_lambda=20)\n",
    "model_b = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.25, max_depth=10, min_child_weight=4, gamma=34, reg_lambda=20)\n",
    "model_c = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.24, max_depth=10, min_child_weight=3, gamma=8, reg_lambda=44)\n",
    "# max_depth = 6 gives best\n",
    "\n",
    "\n",
    "model_a.fit(x_train_a, y_train_a)\n",
    "model_b.fit(x_train_b, y_train_b)\n",
    "model_c.fit(x_train_c, y_train_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1679,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_pred_a = model_a.predict(X_test_estimated_a)\n",
    "y_pred_b = model_b.predict(X_test_estimated_b)\n",
    "y_pred_c = model_c.predict(X_test_estimated_c)\n",
    "\n",
    "y_pred = np.concatenate((y_pred_a, y_pred_b, y_pred_c), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1680,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for A:  162445.32803435044\n",
      "MSE for B:  4843.660688571223\n",
      "MSE for C:  2423.0632593393907\n",
      "Mean MSE:  56570.68399408702\n",
      "Score A:  0.8817545740855711\n",
      "Score B:  0.8742617169876488\n",
      "Score C:  0.9188062828577899\n",
      "                           Feature  Importance\n",
      "36                 sun_elevation:d    0.740617\n",
      "4                  clear_sky_rad:W    0.104682\n",
      "10                    direct_rad:W    0.027994\n",
      "11                 direct_rad_1h:J    0.023406\n",
      "30               snow_density:kgm3    0.018142\n",
      "16               fresh_snow_24h:cm    0.011605\n",
      "38                     t_1000hPa:K    0.006030\n",
      "23            precip_type_5min:idx    0.005982\n",
      "27                 rain_water:kgm2    0.004892\n",
      "24               pressure_100m:hPa    0.004855\n",
      "7                   dew_point_2m:K    0.004421\n",
      "14               fresh_snow_12h:cm    0.004297\n",
      "22                  precip_5min:mm    0.003959\n",
      "3            clear_sky_energy_1h:J    0.002940\n",
      "29                sfc_pressure:hPa    0.002762\n",
      "21                msl_pressure:hPa    0.002660\n",
      "40                    visibility:m    0.002192\n",
      "34                 snow_water:kgm2    0.002161\n",
      "28     relative_humidity_1000hPa:p    0.002020\n",
      "25                pressure_50m:hPa    0.001924\n",
      "12         effective_cloud_cover:p    0.001902\n",
      "39             total_cloud_cover:p    0.001870\n",
      "42             wind_speed_u_10m:ms    0.001859\n",
      "5                 cloud_base_agl:m    0.001812\n",
      "44         wind_speed_w_1000hPa:ms    0.001701\n",
      "9                 diffuse_rad_1h:J    0.001681\n",
      "37  super_cooled_liquid_water:kgm2    0.001424\n",
      "0         absolute_humidity_2m:gm3    0.001412\n",
      "2             ceiling_height_agl:m    0.001369\n",
      "8                    diffuse_rad:W    0.001358\n",
      "43             wind_speed_v_10m:ms    0.001294\n",
      "41               wind_speed_10m:ms    0.001253\n",
      "1              air_density_2m:kgm3    0.001251\n",
      "35                   sun_azimuth:d    0.001218\n",
      "33              snow_melt_10min:mm    0.000323\n",
      "20                is_in_shadow:idx    0.000209\n",
      "6                  dew_or_rime:idx    0.000208\n",
      "17                fresh_snow_3h:cm    0.000109\n",
      "18                fresh_snow_6h:cm    0.000098\n",
      "26                     prob_rime:p    0.000051\n",
      "31                   snow_depth:cm    0.000032\n",
      "19                      is_day:idx    0.000022\n",
      "15                fresh_snow_1h:cm    0.000003\n",
      "13                     elevation:m    0.000000\n",
      "32                  snow_drift:idx    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model based on the validation data\n",
    "\n",
    "mse_a = mean_squared_error(y_val_a, model_a.predict(x_val_a))\n",
    "print(\"MSE for A: \", mse_a)\n",
    "mse_b = mean_squared_error(y_val_b, model_b.predict(x_val_b))\n",
    "print(\"MSE for B: \", mse_b)\n",
    "mse_c = mean_squared_error(y_val_c, model_c.predict(x_val_c))\n",
    "print(\"MSE for C: \", mse_c)\n",
    "print(\"Mean MSE: \", (mse_a + mse_b + mse_c) / 3)\n",
    "\n",
    "# Evaluate the predictions\n",
    "\n",
    "score_a = model_a.score(x_val_a, y_val_a)\n",
    "score_b = model_b.score(x_val_b, y_val_b)\n",
    "score_c = model_c.score(x_val_c, y_val_c)\n",
    "\n",
    "print(\"Score A: \", score_a)\n",
    "print(\"Score B: \", score_b)\n",
    "print(\"Score C: \", score_c)\n",
    "\n",
    "# Get feature importance scores\n",
    "model_a.fit(x_train_a, y_train_a)\n",
    "feature_importance_scores = model_a.feature_importances_\n",
    "\n",
    "# Create a DataFrame to associate features with their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': x_train_a.columns, 'Importance': feature_importance_scores})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print or visualize the feature importance scores\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1681,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_pred\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test['prediction'] = y_test_pred\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
