{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')\n",
    "\n",
    "train_targets_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "train_targets_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "train_targets_c = pd.read_parquet('../data/C/train_targets.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pop() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     combined_data \u001b[39m=\u001b[39m combined_data\u001b[39m.\u001b[39mpop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdirect_rad:W\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclear_sky_rad:W\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdate_forecast\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m combined_data, y_train\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m x_train_a, y_train_a \u001b[39m=\u001b[39m data_clean_up(X_train_estimated_a, X_train_observed_a, train_targets_a)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x_train_b, y_train_b \u001b[39m=\u001b[39m data_clean_up(X_train_estimated_b, X_train_observed_b, train_targets_b)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x_train_c, y_train_c \u001b[39m=\u001b[39m data_clean_up(X_train_estimated_c, X_train_observed_c, train_targets_c)\n",
      "\u001b[1;32m/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdate_forecast\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpv_measurement\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m combined_data\u001b[39m.\u001b[39mcolumns:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m   \u001b[39m#combined_data.drop(columns=\"date_forecast\", inplace=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m   combined_data\u001b[39m.\u001b[39mdrop(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mtime\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpv_measurement\u001b[39m\u001b[39m'\u001b[39m], inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m   combined_data \u001b[39m=\u001b[39m combined_data\u001b[39m.\u001b[39;49mpop(columns\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mdirect_rad:W\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mclear_sky_rad:W\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mdate_forecast\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/xgboost.ipynb#W5sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m combined_data, y_train\n",
      "\u001b[0;31mTypeError\u001b[0m: pop() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data set A, B and C clean up\n",
    "\n",
    "def data_clean_up(x_train_est, x_train_observe, y_train):\n",
    "\n",
    "  if 'date_calc' in x_train_est.columns:\n",
    "    x_train_est.drop(columns=\"date_calc\", inplace=True)\n",
    "\n",
    "  x_train = pd.concat([x_train_observe, x_train_est])\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_train.columns[1:]}\n",
    "  X_train_downscaled = x_train.groupby(x_train.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  y_train.dropna(inplace=True)\n",
    "  combined_data = pd.merge(X_train_downscaled, y_train, left_on='date_forecast', right_on='time')\n",
    "  y_train = combined_data['pv_measurement']\n",
    "\n",
    "  if 'date_forecast' and 'time' and 'pv_measurement' in combined_data.columns:\n",
    "    #combined_data.drop(columns=\"date_forecast\", inplace=True)\n",
    "    combined_data.drop(columns=['time', 'pv_measurement'], inplace=True)\n",
    "    combined_data = combined_data.pop(columns=['direct_rad:W', 'clear_sky_rad:W', 'date_forecast'])\n",
    "\n",
    "\n",
    "  return combined_data, y_train\n",
    "\n",
    "x_train_a, y_train_a = data_clean_up(X_train_estimated_a, X_train_observed_a, train_targets_a)\n",
    "x_train_b, y_train_b = data_clean_up(X_train_estimated_b, X_train_observed_b, train_targets_b)\n",
    "x_train_c, y_train_c = data_clean_up(X_train_estimated_c, X_train_observed_c, train_targets_c)\n",
    "\n",
    "\n",
    "def data_clean_up_test(x_test_est):\n",
    "\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_test_est.columns[1:]}\n",
    "  X_test_downscaled = x_test_est.groupby(x_test_est.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  #if 'date_forecast' in X_test_downscaled.columns:\n",
    "    #X_test_downscaled.drop(columns=\"date_forecast\", inplace=True)\n",
    "\n",
    "  return X_test_downscaled\n",
    "\n",
    "X_test_estimated_a = data_clean_up_test(X_test_estimated_a)\n",
    "X_test_estimated_b = data_clean_up_test(X_test_estimated_b)\n",
    "X_test_estimated_c = data_clean_up_test(X_test_estimated_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do feature selection etc.\n",
    "\n",
    "# Polynomial features of degree 2 of most important features\n",
    "\n",
    "def polynomial_feature(x_dataset, features):\n",
    "  \n",
    "  for feature in features:\n",
    "    x_dataset[feature + ':squared'] = x_dataset[feature] ** 2\n",
    "    x_dataset[feature + ':cubed'] = x_dataset[feature] ** 3\n",
    "    x_dataset[feature + ':sqrt'] = np.sqrt(x_dataset[feature])\n",
    "\n",
    "  return x_dataset\n",
    "\n",
    "\n",
    "# x_train_a = polynomial_feature(x_train_a, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "# x_train_b = polynomial_feature(x_train_b, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "# x_train_c = polynomial_feature(x_train_c, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "\n",
    "# X_test_estimated_a = polynomial_feature(X_test_estimated_a, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "# X_test_estimated_b = polynomial_feature(X_test_estimated_b, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "# X_test_estimated_c = polynomial_feature(X_test_estimated_c, ['direct_rad:W', 'diffuse_rad:W', 'sun_elevation:d'])\n",
    "\n",
    "def drop_features(x_dataset, features):\n",
    "  for feature in features:\n",
    "    x_dataset.drop(columns=feature, inplace=True)\n",
    "\n",
    "  return x_dataset\n",
    "\n",
    "# x_train_a = drop_features(x_train_a, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "# x_train_b = drop_features(x_train_b, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "# x_train_c = drop_features(x_train_c, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "\n",
    "# X_test_estimated_a = drop_features(X_test_estimated_a, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "# X_test_estimated_b = drop_features(X_test_estimated_b, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "# X_test_estimated_c = drop_features(X_test_estimated_c, ['snow_density:kgm3', 'fresh_snow_3h:cm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unixtime(datetime: pd.Series) -> pd.Series:\n",
    "    unixtime = (datetime - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "    return unixtime\n",
    "\n",
    "xat_datetime = x_train_a['date_forecast']\n",
    "xbt_datetime = x_train_b['date_forecast']\n",
    "xct_datetime = x_train_c['date_forecast']\n",
    "\n",
    "xat_unixtime = get_unixtime(xat_datetime)\n",
    "xbt_unixtime = get_unixtime(xbt_datetime)\n",
    "xct_unixtime = get_unixtime(xct_datetime)\n",
    "\n",
    "XTA_unix = get_unixtime(X_test_estimated_a['date_forecast'])\n",
    "XTB_unix = get_unixtime(X_test_estimated_b['date_forecast'])\n",
    "XTC_unix = get_unixtime(X_test_estimated_c['date_forecast'])\n",
    "\n",
    "## We now need functions for assigning daily and yearly cycles (described in datanalysis docu on Peter branch)\n",
    "# plus 2 avoids 0 and negative values\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "def sinus_day(unix_time):\n",
    "    return 2 + np.sin(unix_time * (2 * np.pi / day)) # since it is seconds since 1.1.1970 we divide by seconds in a day to get seasonal changes throughout the dat\n",
    "\n",
    "def sinus_year(unix_time):\n",
    "    return 2+ np.sin(unix_time * (2 * np.pi / year))\n",
    "\n",
    "def cosinus_day(unix_time):\n",
    "    return 2+np.cos(unix_time * (2 * np.pi / day))\n",
    "\n",
    "def cosinus_year(unix_time):\n",
    "    return 2+np.cos(unix_time * (2 * np.pi / year))\n",
    "\n",
    "# function for returning two series with the daily cycles (sine and cosine)\n",
    "def get_daycycle(unixtime: pd.Series) -> (pd.Series, pd.Series):\n",
    "    sinus_daytime = unixtime.apply(sinus_day)\n",
    "    sinus_daytime = sinus_daytime.rename('sinus_day') \n",
    "    cosinus_daytime = unixtime.apply(cosinus_day)\n",
    "    cosinus_daytime = cosinus_daytime.rename('cosine_day')\n",
    "    return sinus_daytime, cosinus_daytime\n",
    "\n",
    "# Function for returning two series with the yearly cycles\n",
    "def get_yearcycle(unixtime: pd.Series) -> (pd.Series, pd.Series):\n",
    "    sinus_yeartime = unixtime.apply(sinus_year)\n",
    "    sinus_yeartime = sinus_yeartime.rename('sinus_year')\n",
    "    cosinus_yeartime = unixtime.apply(cosinus_year)\n",
    "    cosinus_yeartime = cosinus_yeartime.rename('cosinus_year')\n",
    "    return sinus_yeartime, cosinus_yeartime\n",
    "\n",
    "xat_day_sin, xat_day_cos = get_daycycle(xat_unixtime)\n",
    "xat_year_sin, xat_year_cos = get_yearcycle(xat_unixtime)\n",
    "xbt_day_sin, xbt_day_cos = get_daycycle(xbt_unixtime)\n",
    "xbt_year_sin, xbt_year_cos = get_yearcycle(xbt_unixtime)\n",
    "xct_day_sin, xct_day_cos = get_daycycle(xct_unixtime)\n",
    "xct_year_sin, xct_year_cos = get_yearcycle(xct_unixtime)\n",
    "\n",
    "XTA_day_sin, XTA_day_cos = get_daycycle(XTA_unix)\n",
    "XTA_year_sin, XTA_year_cos = get_yearcycle(XTA_unix)\n",
    "XTB_day_sin, XTB_day_cos = get_daycycle(XTB_unix)\n",
    "XTB_year_sin, XTB_year_cos = get_yearcycle(XTB_unix)\n",
    "XTC_day_sin, XTC_day_cos = get_daycycle(XTC_unix)\n",
    "XTC_year_sin, XTC_year_cos = get_yearcycle(XTC_unix)\n",
    "\n",
    "x_train_a = x_train_a.join([xat_day_sin, xat_day_cos, xat_year_sin, xat_year_cos])\n",
    "x_train_b = x_train_b.join([xbt_day_sin, xbt_day_cos, xbt_year_sin, xbt_year_cos])\n",
    "x_train_c = x_train_c.join([xct_day_sin, xct_day_cos, xct_year_sin, xct_year_cos])\n",
    "x_train_a.drop(columns=['date_forecast'], inplace=True)\n",
    "x_train_b.drop(columns=['date_forecast'], inplace=True)\n",
    "x_train_c.drop(columns=['date_forecast'], inplace=True)\n",
    "\n",
    "X_test_estimated_a = X_test_estimated_a.join([XTA_day_sin, XTA_day_cos, XTA_year_sin, XTA_year_cos])\n",
    "X_test_estimated_b = X_test_estimated_b.join([XTB_day_sin, XTB_day_cos, XTB_year_sin, XTB_year_cos])\n",
    "X_test_estimated_c = X_test_estimated_c.join([XTC_day_sin, XTC_day_cos, XTC_year_sin, XTC_year_cos])\n",
    "X_test_estimated_a.drop(columns=['date_forecast'], inplace=True)\n",
    "X_test_estimated_b.drop(columns=['date_forecast'], inplace=True)\n",
    "X_test_estimated_c.drop(columns=['date_forecast'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different loations has different priorities on the dataset which might mean that we can drop different cells to get better results per location. \n",
    "```python\n",
    "data.drop(columns=['elevation:m', \n",
    "                                #'is_day:idx',\n",
    "                                'elevation'\n",
    "                                'wind_speed_u_10m:ms',\n",
    "                                'wind_speed_v_10m:ms',\n",
    "                                'sfc_pressure:hPa',\n",
    "                                'pressure_100m:hPa',\n",
    "                                'pressure_50m:hPa',\n",
    "                                'msl_pressure:hPa',\n",
    "                                #'diffuse_rad_1h:J',\n",
    "                                #'direct_rad_1h:J',\n",
    "                                'air_density_2m:kgm3'], inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "x_train_a, x_val_a, y_train_a, y_val_a = train_test_split(x_train_a, y_train_a, test_size=0.17, random_state=42)\n",
    "x_train_b, x_val_b, y_train_b, y_val_b = train_test_split(x_train_b, y_train_b, test_size=0.17, random_state=42)\n",
    "x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(x_train_c, y_train_c, test_size=0.17, random_state=42)\n",
    "\n",
    "model_a = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.25, max_depth=10, min_child_weight=2, gamma=150, reg_lambda=20)\n",
    "model_b = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.25, max_depth=10, min_child_weight=4, gamma=34, reg_lambda=20)\n",
    "model_c = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.24, max_depth=10, min_child_weight=3, gamma=8, reg_lambda=44)\n",
    "# max_depth = 6 gives best\n",
    "\n",
    "\n",
    "model_a.fit(x_train_a, y_train_a)\n",
    "model_b.fit(x_train_b, y_train_b)\n",
    "model_c.fit(x_train_c, y_train_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model based on the validation data\n",
    "\n",
    "mse_a = mean_squared_error(y_val_a, model_a.predict(x_val_a))\n",
    "print(\"MSE for A: \", mse_a)\n",
    "mse_b = mean_squared_error(y_val_b, model_b.predict(x_val_b))\n",
    "print(\"MSE for B: \", mse_b)\n",
    "mse_c = mean_squared_error(y_val_c, model_c.predict(x_val_c))\n",
    "print(\"MSE for C: \", mse_c)\n",
    "print(\"Mean MSE: \", (mse_a + mse_b + mse_c) / 3)\n",
    "\n",
    "# Evaluate the predictions\n",
    "\n",
    "score_a = model_a.score(x_val_a, y_val_a)\n",
    "score_b = model_b.score(x_val_b, y_val_b)\n",
    "score_c = model_c.score(x_val_c, y_val_c)\n",
    "\n",
    "print(\"Score A: \", score_a)\n",
    "print(\"Score B: \", score_b)\n",
    "print(\"Score C: \", score_c)\n",
    "print('')\n",
    "\n",
    "# Get feature importance scores\n",
    "models = [(model_a, 'A'), (model_b, 'B'), (model_c, 'C')]\n",
    "for model in models:\n",
    "\n",
    "    feature_importance_scores = model[0].feature_importances_\n",
    "\n",
    "# Create a DataFrame to associate features with their importance scores\n",
    "    feature_importance_df1 = pd.DataFrame({'Feature': x_train_a.columns, 'Importance': feature_importance_scores})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "    feature_importance_df1 = feature_importance_df1.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print or visualize the feature importance scores\n",
    "    \n",
    "    print(f'Model {model[1]}')\n",
    "    print(feature_importance_df1.head(10))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Most_common = ['direct_rad:W', 'clear_sky_rad:W']\n",
    "\n",
    "MSE for A:  155326.11984010294\n",
    "MSE for B:  4311.822664627681\n",
    "MSE for C:  2484.332046556924\n",
    "Mean MSE:  54040.75818376252\n",
    "Score A:  0.8869367102250868\n",
    "Score B:  0.8880678863853381\n",
    "Score C:  0.9167532450100108\n",
    "\n",
    "Model A\n",
    "                 Feature  Importance\n",
    "9           direct_rad:W    0.558548 <-------- 3\n",
    "7          diffuse_rad:W    0.077622 <-------- 2\n",
    "18      is_in_shadow:idx    0.028293 <--------2\n",
    "3        clear_sky_rad:W    0.026308 <-------- 3\n",
    "40          cosinus_year    0.024731\n",
    "24     snow_density:kgm3    0.022149 <-------- 2\n",
    "29         sun_azimuth:d    0.021677\n",
    "20  precip_type_5min:idx    0.016280\n",
    "6         dew_point_2m:K    0.015750\n",
    "19        precip_5min:mm    0.014176 <-------- 2\n",
    "\n",
    "Model B\n",
    "             Feature  Importance\n",
    "9       direct_rad:W    0.369634     <--------- 3\n",
    "30   sun_elevation:d    0.165624     <-------- 2\n",
    "3    clear_sky_rad:W    0.084479     <--------- 3\n",
    "18  is_in_shadow:idx    0.074470     <---------\n",
    "17        is_day:idx    0.037519\n",
    "22   rain_water:kgm2    0.028054\n",
    "40      cosinus_year    0.024928\n",
    "7      diffuse_rad:W    0.019828     <--------- 2\n",
    "39        sinus_year    0.016917\n",
    "16  fresh_snow_6h:cm    0.015372     <-------- 1/2\n",
    "\n",
    "Model C\n",
    "                 Feature  Importance\n",
    "30       sun_elevation:d    0.737467 <--------- 2\n",
    "3        clear_sky_rad:W    0.110762 <--------- 3\n",
    "9           direct_rad:W    0.029844 <--------- 3\n",
    "10       direct_rad_1h:J    0.023283\n",
    "20  precip_type_5min:idx    0.010948\n",
    "12     fresh_snow_12h:cm    0.010607\n",
    "24     snow_density:kgm3    0.009615 <-------- 2\n",
    "14     fresh_snow_24h:cm    0.007877 <-------- 1/2\n",
    "6         dew_point_2m:K    0.005590\n",
    "19        precip_5min:mm    0.005087 <-------- 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.25, max_depth=10, min_child_weight=2, gamma=150, reg_lambda=20)\n",
    "model_b = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.25, max_depth=10, min_child_weight=4, gamma=34, reg_lambda=20)\n",
    "model_c = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.24, max_depth=10, min_child_weight=3, gamma=8, reg_lambda=44)\n",
    "\n",
    "# max_depth = 6 gives best\n",
    "model_a.fit(x_train_a, y_train_a)\n",
    "model_b.fit(x_train_b, y_train_b)\n",
    "model_c.fit(x_train_c, y_train_c)\n",
    "\n",
    "y_pred_a = model_a.predict(X_test_estimated_a)\n",
    "y_pred_b = model_b.predict(X_test_estimated_b)\n",
    "y_pred_c = model_c.predict(X_test_estimated_c)\n",
    "\n",
    "y_pred = np.concatenate((y_pred_a, y_pred_b, y_pred_c), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] < 0: \n",
    "        y_pred[i] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_pred\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test['prediction'] = y_test_pred\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
