{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')\n",
    "\n",
    "train_targets_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "train_targets_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "train_targets_c = pd.read_parquet('../data/C/train_targets.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "researching which columns have the most null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: ceiling_height_agl:m has 3919 NULL values\n",
      "Column: cloud_base_agl:m has 2094 NULL values\n",
      "Column: snow_density:kgm3 has 15769 NULL values\n",
      "Column: ceiling_height_agl:m has 22247 NULL values\n",
      "Column: cloud_base_agl:m has 8066 NULL values\n",
      "Column: snow_density:kgm3 has 115945 NULL values\n"
     ]
    }
   ],
   "source": [
    "for column in X_train_estimated_a.columns:\n",
    "    null_c =  X_train_estimated_a[column].isna().sum()\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} NULL values')\n",
    "\n",
    "for column in X_train_observed_a.columns:\n",
    "    null_c = X_train_observed_a[column].isna().sum()\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} NULL values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on output we choose to drop `snow_density:kgm3`, `ceiling_height_agl:m`, `cloud_base_agl:m`\n",
    "\n",
    "```\n",
    "X_ESTIMATED\n",
    "Column: ceiling_height_agl:m has 3919 NULL values\n",
    "Column: cloud_base_agl:m has 2094 NULL values\n",
    "Column: snow_density:kgm3 has 15769 NULL values\n",
    "\n",
    "X_OBSERVED:\n",
    "Column: ceiling_height_agl:m has 22247 NULL values\n",
    "Column: cloud_base_agl:m has 8066 NULL values\n",
    "Column: snow_density:kgm3 has 115945 NULL values\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for column in X_train_estimated_a.columns:\\n    null_c =  X_train_estimated_a[column].value_counts()[0]\\n    if null_c > 0: \\n        print(f'Column: {column} has {null_c} 0 values')\\n\\nfor column in X_train_observed_a.columns:\\n    null_c = X_train_observed_a[column].value_counts()[0]\\n    if null_c > 0: \\n        print(f'Column: {column} has {null_c} 0 values') \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for column in X_train_estimated_a.columns:\n",
    "    null_c =  X_train_estimated_a[column].value_counts()[0]\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} 0 values')\n",
    "\n",
    "for column in X_train_observed_a.columns:\n",
    "    null_c = X_train_observed_a[column].value_counts()[0]\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} 0 values') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data set A, B and C clean up\n",
    "\n",
    "def data_clean_up(x_train_est, x_train_observe, y_train):\n",
    "\n",
    "  if 'date_calc' in x_train_est.columns:\n",
    "    x_train_est.drop(columns=\"date_calc\", inplace=True)\n",
    "\n",
    "  x_train = pd.concat([x_train_observe, x_train_est])\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_train.columns[1:]}\n",
    "  X_train_downscaled = x_train.groupby(x_train.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  y_train.dropna(inplace=True)\n",
    "  combined_data = pd.merge(X_train_downscaled, y_train, left_on='date_forecast', right_on='time')\n",
    "  combined_data.drop(columns=['snow_density:kgm3', 'ceiling_height_agl:m', 'cloud_base_agl:m'], inplace=True)\n",
    "  combined_data.dropna(inplace=True)\n",
    "  y_train = combined_data[['pv_measurement', 'date_forecast']]\n",
    "\n",
    "  if 'time' and 'pv_measurement' in combined_data.columns:\n",
    "    combined_data.drop(columns=['time', 'pv_measurement'], inplace=True)\n",
    "    combined_data.drop(columns=\"pv_measurement\", inplace=True)\n",
    "\n",
    "  return combined_data, y_train\n",
    "\n",
    "def count_null_in_column(df: pd.DataFrame, column_name: str):\n",
    "  return df[column_name].value_counts(None)\n",
    "\n",
    "x_train_a, y_train_a = data_clean_up(X_train_estimated_a, X_train_observed_a, train_targets_a)\n",
    "x_train_b, y_train_b = data_clean_up(X_train_estimated_b, X_train_observed_b, train_targets_b)\n",
    "x_train_c, y_train_c = data_clean_up(X_train_estimated_c, X_train_observed_c, train_targets_c)\n",
    "\n",
    "\n",
    "def data_clean_up_test(x_test_est):\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_test_est.columns[1:]}\n",
    "  X_test_downscaled = x_test_est.groupby(x_test_est.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  \"\"\"\n",
    "  if 'date_forecast' in X_test_downscaled.columns:\n",
    "    X_test_downscaled.drop(columns=\"date_forecast\", inplace=True)\n",
    "  \"\"\"\n",
    "  return X_test_downscaled\n",
    "\n",
    "X_test_estimated_a = data_clean_up_test(X_test_estimated_a)\n",
    "X_test_estimated_b = data_clean_up_test(X_test_estimated_b)\n",
    "X_test_estimated_c = data_clean_up_test(X_test_estimated_c)\n",
    "print(len(X_test_estimated_a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping this average for now since its the best aggreagation we have at this point. However i do believe that using avg or mean together with delta (total change within the hour) might be a better solution. Also some columns might need other aggregations than avg. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to tell the model something about how time seasonality works. E.g. night and day, as well as yearly seasonality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   absolute_humidity_2m:gm3  air_density_2m:kgm3  clear_sky_energy_1h:J  \\\n",
      "0                    8.0250             1.230625               0.000000   \n",
      "1                    7.9000             1.228750               0.000000   \n",
      "2                    8.0125             1.224750               0.000000   \n",
      "3                    8.3125             1.223250             104.324997   \n",
      "4                    8.6625             1.222500           16234.075195   \n",
      "\n",
      "   clear_sky_rad:W  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  \\\n",
      "0            0.000              0.5      280.787506         0.0000   \n",
      "1            0.000              0.5      280.574982         0.0000   \n",
      "2            0.000              0.5      280.787506         0.0000   \n",
      "3            0.375              0.5      281.362488         0.1500   \n",
      "4           11.550              0.5      281.924988         5.9875   \n",
      "\n",
      "   diffuse_rad_1h:J  direct_rad:W  direct_rad_1h:J  ...  total_cloud_cover:p  \\\n",
      "0          0.000000         0.000         0.000000  ...           100.000000   \n",
      "1          0.000000         0.000         0.000000  ...           100.000000   \n",
      "2          0.000000         0.000         0.000000  ...           100.000000   \n",
      "3        263.387512         0.000         0.000000  ...           100.000000   \n",
      "4      11034.474609         0.075       141.487503  ...            99.612503   \n",
      "\n",
      "   visibility:m  wind_speed_10m:ms  wind_speed_u_10m:ms  wind_speed_v_10m:ms  \\\n",
      "0  30549.500000             2.1500              -1.9500               0.0750   \n",
      "1  19697.412109             2.0625              -1.4625               0.4000   \n",
      "2   8417.962891             2.2750              -0.9500               0.8625   \n",
      "3   2782.675049             2.1500              -0.8000               1.0375   \n",
      "4   7081.625000             2.3750              -0.2625               1.0500   \n",
      "\n",
      "   wind_speed_w_1000hPa:ms  sinus_day  cosine_day  sinus_year  cosinus_year  \n",
      "0                      0.0   1.500000    2.866025    2.487287      1.126758  \n",
      "1                      0.0   1.741181    2.965926    2.486661      1.126409  \n",
      "2                      0.0   2.000000    3.000000    2.486034      1.126060  \n",
      "3                      0.0   2.258819    2.965926    2.485408      1.125712  \n",
      "4                      0.0   2.500000    2.866025    2.484781      1.125364  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_unixtime(datetime: pd.Series) -> pd.Series:\n",
    "    unixtime = (datetime - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "    return unixtime\n",
    "\n",
    "xat_datetime = x_train_a['date_forecast']\n",
    "xat_unixtime = get_unixtime(xat_datetime)\n",
    "\n",
    "## We now need functions for assigning daily and yearly cycles (described in datanalysis docu on Peter branch)\n",
    "# plus 2 avoids 0 and negative values\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "def sinus_day(unix_time):\n",
    "    return 2 + np.sin(unix_time * (2 * np.pi / day)) # since it is seconds since 1.1.1970 we divide by seconds in a day to get seasonal changes throughout the dat\n",
    "\n",
    "def sinus_year(unix_time):\n",
    "    return 2+ np.sin(unix_time * (2 * np.pi / year))\n",
    "\n",
    "def cosinus_day(unix_time):\n",
    "    return 2+np.cos(unix_time * (2 * np.pi / day))\n",
    "\n",
    "def cosinus_year(unix_time):\n",
    "    return 2+np.cos(unix_time * (2 * np.pi / year))\n",
    "\n",
    "# function for returning two series with the daily cycles (sine and cosine)\n",
    "def get_daycycle(unixtime: pd.Series) -> (pd.Series, pd.Series):\n",
    "    sinus_daytime = unixtime.apply(sinus_day)\n",
    "    sinus_daytime = sinus_daytime.rename('sinus_day') \n",
    "    cosinus_daytime = unixtime.apply(cosinus_day)\n",
    "    cosinus_daytime = cosinus_daytime.rename('cosine_day')\n",
    "    return sinus_daytime, cosinus_daytime\n",
    "\n",
    "# Function for returning two series with the yearly cycles\n",
    "def get_yearcycle(unixtime: pd.Series) -> (pd.Series, pd.Series):\n",
    "    sinus_yeartime = unixtime.apply(sinus_year)\n",
    "    sinus_yeartime = sinus_yeartime.rename('sinus_year')\n",
    "    cosinus_yeartime = unixtime.apply(cosinus_year)\n",
    "    cosinus_yeartime = cosinus_yeartime.rename('cosinus_year')\n",
    "    return sinus_yeartime, cosinus_yeartime\n",
    "\n",
    "xat_day_sin, xat_day_cos = get_daycycle(xat_unixtime)\n",
    "xat_year_sin, xat_year_cos = get_yearcycle(xat_unixtime)\n",
    "\n",
    "xta_feat = x_train_a.join([xat_day_sin, xat_day_cos, xat_year_sin, xat_year_cos])\n",
    "xta_feat.drop(columns=['date_forecast'], inplace=True)\n",
    "print(xta_feat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to do some extra feature engineering with regards to different datatypes in the dataset. A lot of this is explained in the TensorFlow documentation https://www.tensorflow.org/tutorials/load_data/pandas_dataframe. \n",
    "\n",
    "### Setting categorical and binary values as int\n",
    "\n",
    "Tensorflow usuallt interprets data as float32. However for categorical and binary data we want it to be interpreted as integers. Therefore we set these specific columns as datatype int, and the rest as datatype float. Here we utilize the dictionary way of creating a model as opposed to the numpy array way. This is because our data is NOT homogenous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'absolute_humidity_2m:gm3': <tf.Tensor: shape=(), dtype=float32, numpy=8.025>, 'air_density_2m:kgm3': <tf.Tensor: shape=(), dtype=float32, numpy=1.230625>, 'clear_sky_energy_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'clear_sky_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'dew_or_rime:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.5>, 'dew_point_2m:K': <tf.Tensor: shape=(), dtype=float32, numpy=280.7875>, 'diffuse_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'diffuse_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'direct_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'direct_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'effective_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=99.5375>, 'elevation:m': <tf.Tensor: shape=(), dtype=float32, numpy=6.0>, 'fresh_snow_12h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_1h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_24h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_3h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_6h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'is_day:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'is_in_shadow:idx': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>, 'msl_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1003.45>, 'precip_5min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'precip_type_5min:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'pressure_100m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=990.9>, 'pressure_50m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=996.9125>, 'prob_rime:p': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'rain_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0375>, 'relative_humidity_1000hPa:p': <tf.Tensor: shape=(), dtype=float32, numpy=82.174995>, 'sfc_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1002.975>, 'snow_depth:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_drift:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_melt_10min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.325>, 'sun_azimuth:d': <tf.Tensor: shape=(), dtype=float32, numpy=347.92975>, 'sun_elevation:d': <tf.Tensor: shape=(), dtype=float32, numpy=-21.41975>, 'super_cooled_liquid_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 't_1000hPa:K': <tf.Tensor: shape=(), dtype=float32, numpy=285.45>, 'total_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=100.0>, 'visibility:m': <tf.Tensor: shape=(), dtype=float32, numpy=30549.5>, 'wind_speed_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=2.15>, 'wind_speed_u_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=-1.9499999>, 'wind_speed_v_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.074999996>, 'wind_speed_w_1000hPa:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'sinus_day': <tf.Tensor: shape=(), dtype=float64, numpy=1.4999999999946427>, 'cosine_day': <tf.Tensor: shape=(), dtype=float64, numpy=2.8660254037813457>, 'sinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=2.487286792810544>, 'cosinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.1267580051598447>}, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.])>)\n",
      "({'absolute_humidity_2m:gm3': <tf.Tensor: shape=(), dtype=float32, numpy=7.9>, 'air_density_2m:kgm3': <tf.Tensor: shape=(), dtype=float32, numpy=1.22875>, 'clear_sky_energy_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'clear_sky_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'dew_or_rime:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.5>, 'dew_point_2m:K': <tf.Tensor: shape=(), dtype=float32, numpy=280.57498>, 'diffuse_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'diffuse_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'direct_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'direct_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'effective_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=99.875>, 'elevation:m': <tf.Tensor: shape=(), dtype=float32, numpy=6.0>, 'fresh_snow_12h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_1h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_24h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_3h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_6h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'is_day:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'is_in_shadow:idx': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>, 'msl_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1002.3625>, 'precip_5min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'precip_type_5min:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'pressure_100m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=989.8375>, 'pressure_50m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=995.825>, 'prob_rime:p': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'rain_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0625>, 'relative_humidity_1000hPa:p': <tf.Tensor: shape=(), dtype=float32, numpy=80.475>, 'sfc_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1001.8625>, 'snow_depth:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_drift:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_melt_10min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.45>, 'sun_azimuth:d': <tf.Tensor: shape=(), dtype=float32, numpy=94.3645>, 'sun_elevation:d': <tf.Tensor: shape=(), dtype=float32, numpy=-21.869751>, 'super_cooled_liquid_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 't_1000hPa:K': <tf.Tensor: shape=(), dtype=float32, numpy=285.6125>, 'total_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=100.0>, 'visibility:m': <tf.Tensor: shape=(), dtype=float32, numpy=19697.412>, 'wind_speed_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=2.0625>, 'wind_speed_u_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=-1.4625>, 'wind_speed_v_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.4>, 'wind_speed_w_1000hPa:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'sinus_day': <tf.Tensor: shape=(), dtype=float64, numpy=1.741180954886406>, 'cosine_day': <tf.Tensor: shape=(), dtype=float64, numpy=2.9659258262861012>, 'sinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=2.4866607432119068>, 'cosinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.1264089509293065>}, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.])>)\n",
      "({'absolute_humidity_2m:gm3': <tf.Tensor: shape=(), dtype=float32, numpy=8.0125>, 'air_density_2m:kgm3': <tf.Tensor: shape=(), dtype=float32, numpy=1.22475>, 'clear_sky_energy_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'clear_sky_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'dew_or_rime:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.5>, 'dew_point_2m:K': <tf.Tensor: shape=(), dtype=float32, numpy=280.7875>, 'diffuse_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'diffuse_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'direct_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'direct_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'effective_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=100.0>, 'elevation:m': <tf.Tensor: shape=(), dtype=float32, numpy=6.0>, 'fresh_snow_12h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_1h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_24h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_3h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'fresh_snow_6h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'is_day:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'is_in_shadow:idx': <tf.Tensor: shape=(), dtype=float32, numpy=1.0>, 'msl_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1000.75>, 'precip_5min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.058749996>, 'precip_type_5min:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.5>, 'pressure_100m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=988.2125>, 'pressure_50m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=994.2>, 'prob_rime:p': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'rain_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.15>, 'relative_humidity_1000hPa:p': <tf.Tensor: shape=(), dtype=float32, numpy=78.575>, 'sfc_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1000.2125>, 'snow_depth:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_drift:idx': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_melt_10min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.7875>, 'sun_azimuth:d': <tf.Tensor: shape=(), dtype=float32, numpy=19.587875>, 'sun_elevation:d': <tf.Tensor: shape=(), dtype=float32, numpy=-20.675>, 'super_cooled_liquid_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.1>, 't_1000hPa:K': <tf.Tensor: shape=(), dtype=float32, numpy=285.4125>, 'total_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=100.0>, 'visibility:m': <tf.Tensor: shape=(), dtype=float32, numpy=8417.963>, 'wind_speed_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=2.275>, 'wind_speed_u_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=-0.95>, 'wind_speed_v_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.8625>, 'wind_speed_w_1000hPa:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'sinus_day': <tf.Tensor: shape=(), dtype=float64, numpy=1.9999999999978102>, 'cosine_day': <tf.Tensor: shape=(), dtype=float64, numpy=3.0>, 'sinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=2.486034443578194>, 'cosinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.1260603455297185>}, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([0.])>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/wm45lcfj7sv746_0rnyvdk840000gn/T/ipykernel_32375/1542623841.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target.drop(columns='date_forecast', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "target = y_train_a\n",
    "target.drop(columns='date_forecast', inplace=True)\n",
    "xta_numeric_dict_ds = tf.data.Dataset.from_tensor_slices((dict(xta_feat), target))\n",
    "for row in xta_numeric_dict_ds.take(3):\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'absolute_humidity_2m:gm3': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'absolute_humidity_2m:gm3')>,\n",
       " 'air_density_2m:kgm3': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'air_density_2m:kgm3')>,\n",
       " 'clear_sky_energy_1h:J': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'clear_sky_energy_1h:J')>,\n",
       " 'clear_sky_rad:W': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'clear_sky_rad:W')>,\n",
       " 'dew_or_rime:idx': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'dew_or_rime:idx')>,\n",
       " 'dew_point_2m:K': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'dew_point_2m:K')>,\n",
       " 'diffuse_rad:W': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'diffuse_rad:W')>,\n",
       " 'diffuse_rad_1h:J': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'diffuse_rad_1h:J')>,\n",
       " 'direct_rad:W': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'direct_rad:W')>,\n",
       " 'direct_rad_1h:J': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'direct_rad_1h:J')>,\n",
       " 'effective_cloud_cover:p': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'effective_cloud_cover:p')>,\n",
       " 'elevation:m': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'elevation:m')>,\n",
       " 'fresh_snow_12h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_12h:cm')>,\n",
       " 'fresh_snow_1h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_1h:cm')>,\n",
       " 'fresh_snow_24h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_24h:cm')>,\n",
       " 'fresh_snow_3h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_3h:cm')>,\n",
       " 'fresh_snow_6h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_6h:cm')>,\n",
       " 'is_day:idx': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_day:idx')>,\n",
       " 'is_in_shadow:idx': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'is_in_shadow:idx')>,\n",
       " 'msl_pressure:hPa': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'msl_pressure:hPa')>,\n",
       " 'precip_5min:mm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'precip_5min:mm')>,\n",
       " 'precip_type_5min:idx': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'precip_type_5min:idx')>,\n",
       " 'pressure_100m:hPa': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'pressure_100m:hPa')>,\n",
       " 'pressure_50m:hPa': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'pressure_50m:hPa')>,\n",
       " 'prob_rime:p': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'prob_rime:p')>,\n",
       " 'rain_water:kgm2': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'rain_water:kgm2')>,\n",
       " 'relative_humidity_1000hPa:p': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'relative_humidity_1000hPa:p')>,\n",
       " 'sfc_pressure:hPa': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sfc_pressure:hPa')>,\n",
       " 'snow_depth:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'snow_depth:cm')>,\n",
       " 'snow_drift:idx': <KerasTensor: shape=(None,) dtype=int64 (created by layer 'snow_drift:idx')>,\n",
       " 'snow_melt_10min:mm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'snow_melt_10min:mm')>,\n",
       " 'snow_water:kgm2': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'snow_water:kgm2')>,\n",
       " 'sun_azimuth:d': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sun_azimuth:d')>,\n",
       " 'sun_elevation:d': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sun_elevation:d')>,\n",
       " 'super_cooled_liquid_water:kgm2': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'super_cooled_liquid_water:kgm2')>,\n",
       " 't_1000hPa:K': <KerasTensor: shape=(None,) dtype=float32 (created by layer 't_1000hPa:K')>,\n",
       " 'total_cloud_cover:p': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'total_cloud_cover:p')>,\n",
       " 'visibility:m': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'visibility:m')>,\n",
       " 'wind_speed_10m:ms': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'wind_speed_10m:ms')>,\n",
       " 'wind_speed_u_10m:ms': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'wind_speed_u_10m:ms')>,\n",
       " 'wind_speed_v_10m:ms': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'wind_speed_v_10m:ms')>,\n",
       " 'wind_speed_w_1000hPa:ms': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'wind_speed_w_1000hPa:ms')>,\n",
       " 'sinus_day': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sinus_day')>,\n",
       " 'cosine_day': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'cosine_day')>,\n",
       " 'sinus_year': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sinus_year')>,\n",
       " 'cosinus_year': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'cosinus_year')>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_featurenames = ['is_day:idx', 'is_in_shadow:idx']\n",
    "categorical_featurenames = ['dew_or_rime:idx', 'precip_type_5min:idx', 'snow_drift:idx']\n",
    "\n",
    "inputs = {}\n",
    "for name, column in xta_feat.items():\n",
    "  if type(column[0]) == str:\n",
    "    dtype = tf.string\n",
    "  elif (name in categorical_featurenames or\n",
    "        name in binary_featurenames):\n",
    "    dtype = tf.int64\n",
    "  else:\n",
    "    dtype = tf.float32\n",
    "  inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'tf.cast')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'tf.cast_1')>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = []\n",
    "\n",
    "for name in binary_featurenames:\n",
    "  inp = inputs[name]\n",
    "  inp = inp[:, tf.newaxis]\n",
    "  float_value = tf.cast(inp, tf.float32)\n",
    "  preprocessed.append(float_value)\n",
    "\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'tf.cast')>,\n",
       " <KerasTensor: shape=(None, 1) dtype=float32 (created by layer 'tf.cast_1')>,\n",
       " <KerasTensor: shape=(None, 41) dtype=float32 (created by layer 'normalization_1')>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stack_dict(inputs, fun=tf.stack):\n",
    "    values = []\n",
    "    for key in sorted(inputs.keys()):\n",
    "      values.append(tf.cast(inputs[key], tf.float32))\n",
    "\n",
    "    return fun(values, axis=-1)\n",
    "\n",
    "# getting numerical features\n",
    "numeric_feature_names = []\n",
    "for col in xta_feat.columns:\n",
    "   if col not in binary_featurenames and col not in categorical_featurenames:\n",
    "    numeric_feature_names.append(col)\n",
    "numeric_features = xta_feat[numeric_feature_names]\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(stack_dict(dict(numeric_features)))\n",
    "\n",
    "# creating new dict to send to normalizer and append to preprocessed\n",
    "numeric_inputs = {}\n",
    "for name in numeric_feature_names:\n",
    "  numeric_inputs[name]=inputs[name]\n",
    "\n",
    "numeric_inputs = stack_dict(numeric_inputs)\n",
    "numeric_normalized = normalizer(numeric_inputs)\n",
    "\n",
    "preprocessed.append(numeric_normalized)\n",
    "\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: dew_or_rime:idx\n",
      "vocab: [-1.0, -0.75, -0.5, -0.375, -0.25, -0.125, 0.0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 1.0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 13:34:30.167129: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at lookup_table_op.cc:1066 : FAILED_PRECONDITION: HashTable has different value for same key. Key 0 has 1 and trying to add value 2\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "{{function_node __wrapped__LookupTableImportV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} HashTable has different value for same key. Key 0 has 1 and trying to add value 2 [Op:LookupTableImportV2] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb Cell 21\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m   lookup \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mStringLookup(vocabulary\u001b[39m=\u001b[39mvocab, output_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mone_hot\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   lookup \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mIntegerLookup(vocabulary\u001b[39m=\u001b[39;49mvocab, output_mode\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mone_hot\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m x \u001b[39m=\u001b[39m inputs[name][:, tf\u001b[39m.\u001b[39mnewaxis]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m lookup(x)\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/keras/src/layers/preprocessing/integer_lookup.py:394\u001b[0m, in \u001b[0;36mIntegerLookup.__init__\u001b[0;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary, vocabulary_dtype, idf_weights, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m mask_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m mask_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mint64(mask_token)\n\u001b[1;32m    392\u001b[0m oov_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m oov_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mint64(oov_token)\n\u001b[0;32m--> 394\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    395\u001b[0m     max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m    396\u001b[0m     num_oov_indices\u001b[39m=\u001b[39;49mnum_oov_indices,\n\u001b[1;32m    397\u001b[0m     mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    398\u001b[0m     oov_token\u001b[39m=\u001b[39;49moov_token,\n\u001b[1;32m    399\u001b[0m     vocabulary\u001b[39m=\u001b[39;49mvocabulary,\n\u001b[1;32m    400\u001b[0m     vocabulary_dtype\u001b[39m=\u001b[39;49mvocabulary_dtype,\n\u001b[1;32m    401\u001b[0m     idf_weights\u001b[39m=\u001b[39;49midf_weights,\n\u001b[1;32m    402\u001b[0m     invert\u001b[39m=\u001b[39;49minvert,\n\u001b[1;32m    403\u001b[0m     output_mode\u001b[39m=\u001b[39;49moutput_mode,\n\u001b[1;32m    404\u001b[0m     sparse\u001b[39m=\u001b[39;49msparse,\n\u001b[1;32m    405\u001b[0m     pad_to_max_tokens\u001b[39m=\u001b[39;49mpad_to_max_tokens,\n\u001b[1;32m    406\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    407\u001b[0m )\n\u001b[1;32m    408\u001b[0m base_preprocessing_layer\u001b[39m.\u001b[39mkeras_kpl_gauge\u001b[39m.\u001b[39mget_cell(\u001b[39m\"\u001b[39m\u001b[39mIntegerLookup\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mset(\n\u001b[1;32m    409\u001b[0m     \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m )\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/keras/src/layers/preprocessing/index_lookup.py:323\u001b[0m, in \u001b[0;36mIndexLookup.__init__\u001b[0;34m(self, max_tokens, num_oov_indices, mask_token, oov_token, vocabulary_dtype, vocabulary, idf_weights, invert, output_mode, sparse, pad_to_max_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf_weights_const \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midf_weights\u001b[39m.\u001b[39mvalue()\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m vocabulary \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_vocabulary(vocabulary, idf_weights)\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39m# When restoring from a keras SavedModel, the loading code will\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[39m# expect to find and restore a lookup_table attribute on the layer.\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[39m# This table needs to be uninitialized as a StaticHashTable cannot\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[39m# be initialized twice.\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_uninitialized_lookup_table()\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/keras/src/layers/preprocessing/index_lookup.py:582\u001b[0m, in \u001b[0;36mIndexLookup.set_vocabulary\u001b[0;34m(self, vocabulary, idf_weights)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m (new_vocab_size \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens):\n\u001b[1;32m    576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to set a vocabulary larger than the maximum vocab \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msize. Passed vocab size is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, max vocab size is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    579\u001b[0m             new_vocab_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_tokens\n\u001b[1;32m    580\u001b[0m         )\n\u001b[1;32m    581\u001b[0m     )\n\u001b[0;32m--> 582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlookup_table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lookup_table_from_tokens(tokens)\n\u001b[1;32m    583\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_record_vocabulary_size()\n\u001b[1;32m    585\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_mode \u001b[39m==\u001b[39m TF_IDF \u001b[39mand\u001b[39;00m idf_weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/keras/src/layers/preprocessing/index_lookup.py:885\u001b[0m, in \u001b[0;36mIndexLookup._lookup_table_from_tokens\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    879\u001b[0m keys, values \u001b[39m=\u001b[39m (\n\u001b[1;32m    880\u001b[0m     (indices, tokens) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minvert \u001b[39melse\u001b[39;00m (tokens, indices)\n\u001b[1;32m    881\u001b[0m )\n\u001b[1;32m    882\u001b[0m initializer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlookup\u001b[39m.\u001b[39mKeyValueTensorInitializer(\n\u001b[1;32m    883\u001b[0m     keys, values, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_key_dtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_dtype\n\u001b[1;32m    884\u001b[0m )\n\u001b[0;32m--> 885\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mlookup\u001b[39m.\u001b[39;49mStaticHashTable(initializer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_value)\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/trackable/resource.py:103\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m getter \u001b[39min\u001b[39;00m resource_creator_stack[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_resource_type()]:\n\u001b[1;32m    101\u001b[0m   previous_getter \u001b[39m=\u001b[39m _make_getter(getter, previous_getter)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m previous_getter(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/trackable/resource.py:98\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.<lambda>\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     95\u001b[0m   obj\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m     96\u001b[0m   \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m---> 98\u001b[0m previous_getter \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: default_resource_creator(\u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     99\u001b[0m resource_creator_stack \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\u001b[39m.\u001b[39m_resource_creator_stack\n\u001b[1;32m    100\u001b[0m \u001b[39mfor\u001b[39;00m getter \u001b[39min\u001b[39;00m resource_creator_stack[\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_resource_type()]:\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/trackable/resource.py:95\u001b[0m, in \u001b[0;36m_ResourceMetaclass.__call__.<locals>.default_resource_creator\u001b[0;34m(next_creator, *a, **kw)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39massert\u001b[39;00m next_creator \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     94\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__new__\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m---> 95\u001b[0m obj\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m     96\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/ops/lookup_ops.py:348\u001b[0m, in \u001b[0;36mStaticHashTable.__init__\u001b[0;34m(self, initializer, default_value, name, experimental_is_anonymous)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhash_table\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    347\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_table_name \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[39msuper\u001b[39;49m(StaticHashTable, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(default_value, initializer)\n\u001b[1;32m    349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_default_value\u001b[39m.\u001b[39mget_shape()\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/ops/lookup_ops.py:205\u001b[0m, in \u001b[0;36mInitializableLookupTableBase.__init__\u001b[0;34m(self, default_value, initializer)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize()\n\u001b[1;32m    204\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize()\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/ops/lookup_ops.py:208\u001b[0m, in \u001b[0;36mInitializableLookupTableBase._initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_initialize\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 208\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initializer\u001b[39m.\u001b[39;49minitialize(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/ops/lookup_ops.py:578\u001b[0m, in \u001b[0;36mKeyValueTensorInitializer.initialize\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m    575\u001b[0m check_table_dtypes(table, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys\u001b[39m.\u001b[39mdtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    576\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(\n\u001b[1;32m    577\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, values\u001b[39m=\u001b[39m(table\u001b[39m.\u001b[39mresource_handle, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values)):\n\u001b[0;32m--> 578\u001b[0m   init_op \u001b[39m=\u001b[39m gen_lookup_ops\u001b[39m.\u001b[39;49mlookup_table_import_v2(table\u001b[39m.\u001b[39;49mresource_handle,\n\u001b[1;32m    579\u001b[0m                                                   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_keys, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values)\n\u001b[1;32m    580\u001b[0m ops\u001b[39m.\u001b[39madd_to_collection(ops\u001b[39m.\u001b[39mGraphKeys\u001b[39m.\u001b[39mTABLE_INITIALIZERS, init_op)\n\u001b[1;32m    581\u001b[0m \u001b[39mreturn\u001b[39;00m init_op\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/ops/gen_lookup_ops.py:1093\u001b[0m, in \u001b[0;36mlookup_table_import_v2\u001b[0;34m(table_handle, keys, values, name)\u001b[0m\n\u001b[1;32m   1091\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   1092\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 1093\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   1094\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   1095\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:5888\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5886\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NoReturn:\n\u001b[1;32m   5887\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m-> 5888\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: {{function_node __wrapped__LookupTableImportV2_device_/job:localhost/replica:0/task:0/device:CPU:0}} HashTable has different value for same key. Key 0 has 1 and trying to add value 2 [Op:LookupTableImportV2] name: "
     ]
    }
   ],
   "source": [
    "for name in categorical_featurenames:\n",
    "  vocab = sorted(set(xta_feat[name]))\n",
    "  print(f'name: {name}')\n",
    "  print(f'vocab: {vocab}\\n')\n",
    "\n",
    "  if type(vocab[0]) is str:\n",
    "    lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "  else:\n",
    "    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "\n",
    "  x = inputs[name][:, tf.newaxis]\n",
    "  x = lookup(x)\n",
    "  preprocessed.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER = 500\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "model.fit(dict(xta_feat), target, epochs=5, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some more stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_pred\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test['prediction'] = y_test_pred\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
