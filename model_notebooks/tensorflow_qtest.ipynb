{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')\n",
    "\n",
    "train_targets_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "train_targets_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "train_targets_c = pd.read_parquet('../data/C/train_targets.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "researching which columns have the most null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: ceiling_height_agl:m has 3919 NULL values\n",
      "Column: cloud_base_agl:m has 2094 NULL values\n",
      "Column: snow_density:kgm3 has 15769 NULL values\n",
      "Column: ceiling_height_agl:m has 22247 NULL values\n",
      "Column: cloud_base_agl:m has 8066 NULL values\n",
      "Column: snow_density:kgm3 has 115945 NULL values\n"
     ]
    }
   ],
   "source": [
    "for column in X_train_estimated_a.columns:\n",
    "    null_c =  X_train_estimated_a[column].isna().sum()\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} NULL values')\n",
    "\n",
    "for column in X_train_observed_a.columns:\n",
    "    null_c = X_train_observed_a[column].isna().sum()\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} NULL values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on output we choose to drop `snow_density:kgm3`, `ceiling_height_agl:m`, `cloud_base_agl:m`\n",
    "\n",
    "```\n",
    "X_ESTIMATED\n",
    "Column: ceiling_height_agl:m has 3919 NULL values\n",
    "Column: cloud_base_agl:m has 2094 NULL values\n",
    "Column: snow_density:kgm3 has 15769 NULL values\n",
    "\n",
    "X_OBSERVED:\n",
    "Column: ceiling_height_agl:m has 22247 NULL values\n",
    "Column: cloud_base_agl:m has 8066 NULL values\n",
    "Column: snow_density:kgm3 has 115945 NULL values\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for column in X_train_estimated_a.columns:\\n    null_c =  X_train_estimated_a[column].value_counts()[0]\\n    if null_c > 0: \\n        print(f'Column: {column} has {null_c} 0 values')\\n\\nfor column in X_train_observed_a.columns:\\n    null_c = X_train_observed_a[column].value_counts()[0]\\n    if null_c > 0: \\n        print(f'Column: {column} has {null_c} 0 values') \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for column in X_train_estimated_a.columns:\n",
    "    null_c =  X_train_estimated_a[column].value_counts()[0]\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} 0 values')\n",
    "\n",
    "for column in X_train_observed_a.columns:\n",
    "    null_c = X_train_observed_a[column].value_counts()[0]\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} 0 values') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data set A, B and C clean up\n",
    "binary_featurenames = ['is_day:idx', 'is_in_shadow:idx'] # ONLY FOR REMOVAL IN QUICK TEST\n",
    "categorical_featurenames = ['dew_or_rime:idx', 'precip_type_5min:idx', 'snow_drift:idx']\n",
    "has_null = ['ceiling_height_agl:m', 'cloud_base_agl:m', 'snow_density:kgm3']\n",
    "\n",
    "def data_clean_up(x_train_est, x_train_observe, y_train):\n",
    "\n",
    "  if 'date_calc' in x_train_est.columns:\n",
    "    x_train_est.drop(columns=\"date_calc\", inplace=True)\n",
    "\n",
    "  x_train = pd.concat([x_train_observe, x_train_est])\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_train.columns[1:]}\n",
    "  X_train_downscaled = x_train.groupby(x_train.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  y_train.dropna(inplace=True)\n",
    "  combined_data = pd.merge(X_train_downscaled, y_train, left_on='date_forecast', right_on='time')\n",
    "  #combined_data.dropna(inplace=True)\n",
    "  y_train = combined_data[['pv_measurement', 'date_forecast']]\n",
    "\n",
    "  if 'time' and 'pv_measurement' in combined_data.columns:\n",
    "    combined_data.drop(columns=['time', 'pv_measurement'], inplace=True)\n",
    "    combined_data.drop(columns=binary_featurenames, inplace=True)\n",
    "    combined_data.drop(columns=categorical_featurenames, inplace=True)\n",
    "    combined_data.drop(columns=has_null, inplace=True)\n",
    "\n",
    "  return combined_data, y_train\n",
    "\n",
    "def count_null_in_column(df: pd.DataFrame, column_name: str):\n",
    "  return df[column_name].value_counts(None)\n",
    "\n",
    "x_train_a, y_train_a = data_clean_up(X_train_estimated_a, X_train_observed_a, train_targets_a)\n",
    "x_train_b, y_train_b = data_clean_up(X_train_estimated_b, X_train_observed_b, train_targets_b)\n",
    "x_train_c, y_train_c = data_clean_up(X_train_estimated_c, X_train_observed_c, train_targets_c)\n",
    "\n",
    "print(len(x_train_a))\n",
    "\n",
    "\n",
    "def data_clean_up_test(x_test_est):\n",
    "\n",
    "  x_test_est.drop(columns=binary_featurenames, inplace=True)\n",
    "  x_test_est.drop(columns=categorical_featurenames, inplace=True)\n",
    "  x_test_est.drop(columns=has_null, inplace=True)\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_test_est.columns[1:]}\n",
    "  X_test_downscaled = x_test_est.groupby(x_test_est.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "  \n",
    "\n",
    "  \"\"\"\n",
    "  if 'date_forecast' in X_test_downscaled.columns:\n",
    "    X_test_downscaled.drop(columns=\"date_forecast\", inplace=True)\n",
    "  \"\"\"\n",
    "  return X_test_downscaled\n",
    "\n",
    "X_test_estimated_a = data_clean_up_test(X_test_estimated_a)\n",
    "X_test_estimated_b = data_clean_up_test(X_test_estimated_b)\n",
    "X_test_estimated_c = data_clean_up_test(X_test_estimated_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping this average for now since its the best aggreagation we have at this point. However i do believe that using avg or mean together with delta (total change within the hour) might be a better solution. Also some columns might need other aggregations than avg. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to tell the model something about how time seasonality works. E.g. night and day, as well as yearly seasonality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "968\n"
     ]
    }
   ],
   "source": [
    "def get_unixtime(datetime: pd.Series) -> pd.Series:\n",
    "    unixtime = (datetime - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "    return unixtime\n",
    "\n",
    "xat_datetime = x_train_a['date_forecast']\n",
    "xat_unixtime = get_unixtime(xat_datetime)\n",
    "\n",
    "## We now need functions for assigning daily and yearly cycles (described in datanalysis docu on Peter branch)\n",
    "# plus 2 avoids 0 and negative values\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "def sinus_day(unix_time):\n",
    "    return 2 + np.sin(unix_time * (2 * np.pi / day)) # since it is seconds since 1.1.1970 we divide by seconds in a day to get seasonal changes throughout the dat\n",
    "\n",
    "def sinus_year(unix_time):\n",
    "    return 2+ np.sin(unix_time * (2 * np.pi / year))\n",
    "\n",
    "def cosinus_day(unix_time):\n",
    "    return 2+np.cos(unix_time * (2 * np.pi / day))\n",
    "\n",
    "def cosinus_year(unix_time):\n",
    "    return 2+np.cos(unix_time * (2 * np.pi / year))\n",
    "\n",
    "# function for returning two series with the daily cycles (sine and cosine)\n",
    "def get_daycycle(unixtime: pd.Series) -> (pd.Series, pd.Series):\n",
    "    sinus_daytime = unixtime.apply(sinus_day)\n",
    "    sinus_daytime = sinus_daytime.rename('sinus_day') \n",
    "    cosinus_daytime = unixtime.apply(cosinus_day)\n",
    "    cosinus_daytime = cosinus_daytime.rename('cosine_day')\n",
    "    return sinus_daytime, cosinus_daytime\n",
    "\n",
    "# Function for returning two series with the yearly cycles\n",
    "def get_yearcycle(unixtime: pd.Series) -> (pd.Series, pd.Series):\n",
    "    sinus_yeartime = unixtime.apply(sinus_year)\n",
    "    sinus_yeartime = sinus_yeartime.rename('sinus_year')\n",
    "    cosinus_yeartime = unixtime.apply(cosinus_year)\n",
    "    cosinus_yeartime = cosinus_yeartime.rename('cosinus_year')\n",
    "    return sinus_yeartime, cosinus_yeartime\n",
    "\n",
    "xat_day_sin, xat_day_cos = get_daycycle(xat_unixtime)\n",
    "xat_year_sin, xat_year_cos = get_yearcycle(xat_unixtime)\n",
    "\n",
    "xta_feat = x_train_a.join([xat_day_sin, xat_day_cos, xat_year_sin, xat_year_cos])\n",
    "xta_feat.drop(columns=['date_forecast'], inplace=True)\n",
    "print(len(xta_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to do some extra feature engineering with regards to different datatypes in the dataset. A lot of this is explained in the TensorFlow documentation https://www.tensorflow.org/tutorials/load_data/pandas_dataframe. \n",
    "\n",
    "### Setting categorical and binary values as int\n",
    "\n",
    "Tensorflow usuallt interprets data as float32. However for categorical and binary data we want it to be interpreted as integers. Therefore we set these specific columns as datatype int, and the rest as datatype float. Here we utilize the dictionary way of creating a model as opposed to the numpy array way. This is because our data is NOT homogenous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'absolute_humidity_2m:gm3': <tf.Tensor: shape=(), dtype=float32, numpy=6.725>, 'air_density_2m:kgm3': <tf.Tensor: shape=(), dtype=float32, numpy=1.264375>, 'clear_sky_energy_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=49736.727>, 'clear_sky_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=27.1375>, 'dew_point_2m:K': <tf.Tensor: shape=(), dtype=float32, numpy=275.625>, 'diffuse_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=14.2875>, 'diffuse_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=30097.05>, 'direct_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=12.2>, 'direct_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=23929.3>, 'effective_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=53.5125>, 'elevation:m': <tf.Tensor: shape=(), dtype=float32, numpy=6.0>, 'fresh_snow_12h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.35>, 'fresh_snow_1h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.075>, 'fresh_snow_24h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.85>, 'fresh_snow_3h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.075>, 'fresh_snow_6h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.075>, 'msl_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1006.8625>, 'precip_5min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'pressure_100m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=993.95>, 'pressure_50m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1000.125>, 'prob_rime:p': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'rain_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'relative_humidity_1000hPa:p': <tf.Tensor: shape=(), dtype=float32, numpy=91.9625>, 'sfc_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1006.3>, 'snow_depth:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0625>, 'snow_melt_10min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.025>, 'sun_azimuth:d': <tf.Tensor: shape=(), dtype=float32, numpy=64.55512>, 'sun_elevation:d': <tf.Tensor: shape=(), dtype=float32, numpy=-13.617001>, 'super_cooled_liquid_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 't_1000hPa:K': <tf.Tensor: shape=(), dtype=float32, numpy=278.6>, 'total_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=54.425>, 'visibility:m': <tf.Tensor: shape=(), dtype=float32, numpy=21870.525>, 'wind_speed_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=1.2625>, 'wind_speed_u_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.35>, 'wind_speed_v_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=1.2125>, 'wind_speed_w_1000hPa:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'sinus_day': <tf.Tensor: shape=(), dtype=float64, numpy=2.707106781182894>, 'cosine_day': <tf.Tensor: shape=(), dtype=float64, numpy=2.707106781190201>, 'sinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.7014868092372153>, 'cosinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.045594491350442>}, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([179.08])>)\n",
      "({'absolute_humidity_2m:gm3': <tf.Tensor: shape=(), dtype=float32, numpy=6.9625>, 'air_density_2m:kgm3': <tf.Tensor: shape=(), dtype=float32, numpy=1.26>, 'clear_sky_energy_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=174013.97>, 'clear_sky_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=72.962494>, 'dew_point_2m:K': <tf.Tensor: shape=(), dtype=float32, numpy=276.2125>, 'diffuse_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=25.05>, 'diffuse_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=70818.19>, 'direct_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=42.875>, 'direct_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=99118.586>, 'effective_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=56.0375>, 'elevation:m': <tf.Tensor: shape=(), dtype=float32, numpy=6.0>, 'fresh_snow_12h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.5875>, 'fresh_snow_1h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.47500002>, 'fresh_snow_24h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=1.325>, 'fresh_snow_3h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.55>, 'fresh_snow_6h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.55>, 'msl_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1007.0>, 'precip_5min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.026250001>, 'pressure_100m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=994.025>, 'pressure_50m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1000.175>, 'prob_rime:p': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'rain_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'relative_humidity_1000hPa:p': <tf.Tensor: shape=(), dtype=float32, numpy=91.9125>, 'sfc_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1006.375>, 'snow_depth:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.125>, 'snow_melt_10min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.125>, 'sun_azimuth:d': <tf.Tensor: shape=(), dtype=float32, numpy=78.04>, 'sun_elevation:d': <tf.Tensor: shape=(), dtype=float32, numpy=-7.38925>, 'super_cooled_liquid_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 't_1000hPa:K': <tf.Tensor: shape=(), dtype=float32, numpy=278.75>, 'total_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=56.0375>, 'visibility:m': <tf.Tensor: shape=(), dtype=float32, numpy=21176.838>, 'wind_speed_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=1.3>, 'wind_speed_u_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.17500001>, 'wind_speed_v_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.8>, 'wind_speed_w_1000hPa:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'sinus_day': <tf.Tensor: shape=(), dtype=float64, numpy=2.8660254037792163>, 'cosine_day': <tf.Tensor: shape=(), dtype=float64, numpy=2.5000000000090457>, 'sinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.7008027849300635>, 'cosinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.0458087055027203>}, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([531.3])>)\n",
      "({'absolute_humidity_2m:gm3': <tf.Tensor: shape=(), dtype=float32, numpy=7.125>, 'air_density_2m:kgm3': <tf.Tensor: shape=(), dtype=float32, numpy=1.255625>, 'clear_sky_energy_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=368724.5>, 'clear_sky_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=133.4125>, 'dew_point_2m:K': <tf.Tensor: shape=(), dtype=float32, numpy=276.725>, 'diffuse_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=31.975>, 'diffuse_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=102651.484>, 'direct_rad:W': <tf.Tensor: shape=(), dtype=float32, numpy=90.35>, 'direct_rad_1h:J': <tf.Tensor: shape=(), dtype=float32, numpy=239799.92>, 'effective_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=54.75>, 'elevation:m': <tf.Tensor: shape=(), dtype=float32, numpy=6.0>, 'fresh_snow_12h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=1.0375>, 'fresh_snow_1h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.45000002>, 'fresh_snow_24h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=1.8>, 'fresh_snow_3h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=1.0375>, 'fresh_snow_6h:cm': <tf.Tensor: shape=(), dtype=float32, numpy=1.0375>, 'msl_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1007.0>, 'precip_5min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.00875>, 'pressure_100m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=994.05>, 'pressure_50m:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1000.1875>, 'prob_rime:p': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'rain_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'relative_humidity_1000hPa:p': <tf.Tensor: shape=(), dtype=float32, numpy=90.475>, 'sfc_pressure:hPa': <tf.Tensor: shape=(), dtype=float32, numpy=1006.35>, 'snow_depth:cm': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>, 'snow_melt_10min:mm': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'snow_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.175>, 'sun_azimuth:d': <tf.Tensor: shape=(), dtype=float32, numpy=91.11925>, 'sun_elevation:d': <tf.Tensor: shape=(), dtype=float32, numpy=-0.8506248>, 'super_cooled_liquid_water:kgm2': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 't_1000hPa:K': <tf.Tensor: shape=(), dtype=float32, numpy=279.0>, 'total_cloud_cover:p': <tf.Tensor: shape=(), dtype=float32, numpy=55.275>, 'visibility:m': <tf.Tensor: shape=(), dtype=float32, numpy=21296.824>, 'wind_speed_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=2.05>, 'wind_speed_u_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.21249999>, 'wind_speed_v_10m:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.625>, 'wind_speed_w_1000hPa:ms': <tf.Tensor: shape=(), dtype=float32, numpy=0.0>, 'sinus_day': <tf.Tensor: shape=(), dtype=float64, numpy=2.9659258262887653>, 'cosine_day': <tf.Tensor: shape=(), dtype=float64, numpy=2.258819045103652>, 'sinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.7001189143435465>, 'cosinus_year': <tf.Tensor: shape=(), dtype=float64, numpy=1.0460234098964971>}, <tf.Tensor: shape=(1,), dtype=float64, numpy=array([1294.7])>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/wm45lcfj7sv746_0rnyvdk840000gn/T/ipykernel_33922/1542623841.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target.drop(columns='date_forecast', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "target = y_train_a\n",
    "target.drop(columns='date_forecast', inplace=True)\n",
    "xta_numeric_dict_ds = tf.data.Dataset.from_tensor_slices((dict(xta_feat), target))\n",
    "for row in xta_numeric_dict_ds.take(3):\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline (not in quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'absolute_humidity_2m:gm3': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'absolute_humidity_2m:gm3')>,\n",
       " 'air_density_2m:kgm3': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'air_density_2m:kgm3')>,\n",
       " 'clear_sky_energy_1h:J': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'clear_sky_energy_1h:J')>,\n",
       " 'clear_sky_rad:W': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'clear_sky_rad:W')>,\n",
       " 'dew_point_2m:K': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'dew_point_2m:K')>,\n",
       " 'diffuse_rad:W': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'diffuse_rad:W')>,\n",
       " 'diffuse_rad_1h:J': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'diffuse_rad_1h:J')>,\n",
       " 'direct_rad:W': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'direct_rad:W')>,\n",
       " 'direct_rad_1h:J': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'direct_rad_1h:J')>,\n",
       " 'effective_cloud_cover:p': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'effective_cloud_cover:p')>,\n",
       " 'elevation:m': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'elevation:m')>,\n",
       " 'fresh_snow_12h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_12h:cm')>,\n",
       " 'fresh_snow_1h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_1h:cm')>,\n",
       " 'fresh_snow_24h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_24h:cm')>,\n",
       " 'fresh_snow_3h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_3h:cm')>,\n",
       " 'fresh_snow_6h:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'fresh_snow_6h:cm')>,\n",
       " 'msl_pressure:hPa': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'msl_pressure:hPa')>,\n",
       " 'precip_5min:mm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'precip_5min:mm')>,\n",
       " 'pressure_100m:hPa': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'pressure_100m:hPa')>,\n",
       " 'pressure_50m:hPa': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'pressure_50m:hPa')>,\n",
       " 'prob_rime:p': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'prob_rime:p')>,\n",
       " 'rain_water:kgm2': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'rain_water:kgm2')>,\n",
       " 'relative_humidity_1000hPa:p': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'relative_humidity_1000hPa:p')>,\n",
       " 'sfc_pressure:hPa': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sfc_pressure:hPa')>,\n",
       " 'snow_depth:cm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'snow_depth:cm')>,\n",
       " 'snow_melt_10min:mm': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'snow_melt_10min:mm')>,\n",
       " 'snow_water:kgm2': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'snow_water:kgm2')>,\n",
       " 'sun_azimuth:d': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sun_azimuth:d')>,\n",
       " 'sun_elevation:d': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sun_elevation:d')>,\n",
       " 'super_cooled_liquid_water:kgm2': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'super_cooled_liquid_water:kgm2')>,\n",
       " 't_1000hPa:K': <KerasTensor: shape=(None,) dtype=float32 (created by layer 't_1000hPa:K')>,\n",
       " 'total_cloud_cover:p': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'total_cloud_cover:p')>,\n",
       " 'visibility:m': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'visibility:m')>,\n",
       " 'wind_speed_10m:ms': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'wind_speed_10m:ms')>,\n",
       " 'wind_speed_u_10m:ms': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'wind_speed_u_10m:ms')>,\n",
       " 'wind_speed_v_10m:ms': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'wind_speed_v_10m:ms')>,\n",
       " 'wind_speed_w_1000hPa:ms': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'wind_speed_w_1000hPa:ms')>,\n",
       " 'sinus_day': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sinus_day')>,\n",
       " 'cosine_day': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'cosine_day')>,\n",
       " 'sinus_year': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'sinus_year')>,\n",
       " 'cosinus_year': <KerasTensor: shape=(None,) dtype=float32 (created by layer 'cosinus_year')>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_featurenames = ['is_day:idx', 'is_in_shadow:idx']\n",
    "categorical_featurenames = ['dew_or_rime:idx', 'precip_type_5min:idx', 'snow_drift:idx']\n",
    "\n",
    "inputs = {}\n",
    "for name, column in xta_feat.items():\n",
    "  dtype = tf.float32\n",
    "  inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' preprocessed = []\\n\\nfor name in binary_featurenames:\\n  inp = inputs[name]\\n  inp = inp[:, tf.newaxis]\\n  float_value = tf.cast(inp, tf.float32)\\n  preprocessed.append(float_value)\\n\\npreprocessed '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" preprocessed = []\n",
    "\n",
    "for name in binary_featurenames:\n",
    "  inp = inputs[name]\n",
    "  inp = inp[:, tf.newaxis]\n",
    "  float_value = tf.cast(inp, tf.float32)\n",
    "  preprocessed.append(float_value)\n",
    "\n",
    "preprocessed \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 41) dtype=float32 (created by layer 'normalization')>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed = []\n",
    "def stack_dict(inputs, fun=tf.stack):\n",
    "    values = []\n",
    "    for key in sorted(inputs.keys()):\n",
    "      values.append(tf.cast(inputs[key], tf.float32))\n",
    "\n",
    "    return fun(values, axis=-1)\n",
    "\n",
    "# getting numerical features\n",
    "numeric_feature_names = []\n",
    "for col in xta_feat.columns:\n",
    "   if col not in binary_featurenames and col not in categorical_featurenames:\n",
    "    numeric_feature_names.append(col)\n",
    "numeric_features = xta_feat[numeric_feature_names]\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(stack_dict(dict(numeric_features)))\n",
    "\n",
    "# creating new dict to send to normalizer and append to preprocessed\n",
    "numeric_inputs = {}\n",
    "for name in numeric_feature_names:\n",
    "  numeric_inputs[name]=inputs[name]\n",
    "\n",
    "numeric_inputs = stack_dict(numeric_inputs)\n",
    "numeric_normalized = normalizer(numeric_inputs)\n",
    "\n",
    "preprocessed.append(numeric_normalized)\n",
    "\n",
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for name in categorical_featurenames:\\n  vocab = sorted(set(xta_feat[name]))\\n  print(f'name: {name}')\\n  print(f'vocab: {vocab}\\n')\\n\\n  if type(vocab[0]) is str:\\n    lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\\n  else:\\n    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\\n\\n  x = inputs[name][:, tf.newaxis]\\n  x = lookup(x)\\n  preprocessed.append(x) \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for name in categorical_featurenames:\n",
    "  vocab = sorted(set(xta_feat[name]))\n",
    "  print(f'name: {name}')\n",
    "  print(f'vocab: {vocab}\\n')\n",
    "\n",
    "  if type(vocab[0]) is str:\n",
    "    lookup = tf.keras.layers.StringLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "  else:\n",
    "    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')\n",
    "\n",
    "  x = inputs[name][:, tf.newaxis]\n",
    "  x = lookup(x)\n",
    "  preprocessed.append(x) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 41) dtype=float32 (created by layer 'tf.identity')>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocesssed_result = tf.concat(preprocessed, axis=-1)\n",
    "preprocesssed_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "preprocessor = tf.keras.Model(inputs, preprocesssed_result)\n",
    "tf.keras.utils.plot_model(preprocessor, rankdir=\"LR\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`y` argument is not supported when using dataset as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow_qtest.ipynb Cell 31\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow_qtest.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m SHUFFLE_BUFFER \u001b[39m=\u001b[39m \u001b[39m500\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow_qtest.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow_qtest.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(xta_numeric_dict_ds, target, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE)\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/keras/src/engine/data_adapter.py:808\u001b[0m, in \u001b[0;36mDatasetAdapter._validate_args\u001b[0;34m(self, y, sample_weights, steps, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[39m# Arguments that shouldn't be passed.\u001b[39;00m\n\u001b[1;32m    807\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_none_or_empty(y):\n\u001b[0;32m--> 808\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    809\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`y` argument is not supported when using dataset as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    810\u001b[0m     )\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_none_or_empty(sample_weights):\n\u001b[1;32m    812\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    813\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`sample_weight` argument is not supported when using \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    814\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    815\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: `y` argument is not supported when using dataset as input."
     ]
    }
   ],
   "source": [
    "SHUFFLE_BUFFER = 500\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "model.fit(dict(xta_feat), target, epochs=5, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some more stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_pred\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test['prediction'] = y_test_pred\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
