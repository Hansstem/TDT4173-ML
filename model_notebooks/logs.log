2023-11-02 12:48:17,339:INFO:PyCaret RegressionExperiment
2023-11-02 12:48:17,339:INFO:Logging name: reg-default-name
2023-11-02 12:48:17,340:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-02 12:48:17,340:INFO:version 3.1.0
2023-11-02 12:48:17,340:INFO:Initializing setup()
2023-11-02 12:48:17,340:INFO:self.USI: b61b
2023-11-02 12:48:17,340:INFO:self._variable_keys: {'pipeline', 'data', 'log_plots_param', 'n_jobs_param', 'memory', 'X_test', 'seed', 'X', 'X_train', '_available_plots', 'exp_name_log', 'USI', 'target_param', 'html_param', 'gpu_n_jobs_param', '_ml_usecase', 'fold_shuffle_param', 'gpu_param', 'fold_groups_param', 'y', 'y_train', 'logging_param', 'fold_generator', 'idx', 'y_test', 'transform_target_param', 'exp_id'}
2023-11-02 12:48:17,340:INFO:Checking environment
2023-11-02 12:48:17,341:INFO:python_version: 3.9.6
2023-11-02 12:48:17,341:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-02 12:48:17,341:INFO:machine: x86_64
2023-11-02 12:48:17,341:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-02 12:48:17,342:INFO:Memory: svmem(total=8589934592, available=2813247488, percent=67.2, used=5199163392, free=64573440, active=2753372160, inactive=2733961216, wired=2445791232)
2023-11-02 12:48:17,342:INFO:Physical Core: 4
2023-11-02 12:48:17,342:INFO:Logical Core: 8
2023-11-02 12:48:17,342:INFO:Checking libraries
2023-11-02 12:48:17,342:INFO:System:
2023-11-02 12:48:17,342:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-02 12:48:17,342:INFO:executable: /usr/local/bin/python3.9
2023-11-02 12:48:17,342:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-02 12:48:17,342:INFO:PyCaret required dependencies:
2023-11-02 12:48:17,343:INFO:                 pip: 23.3.1
2023-11-02 12:48:17,343:INFO:          setuptools: 56.0.0
2023-11-02 12:48:17,343:INFO:             pycaret: 3.1.0
2023-11-02 12:48:17,343:INFO:             IPython: 7.28.0
2023-11-02 12:48:17,343:INFO:          ipywidgets: 8.1.1
2023-11-02 12:48:17,343:INFO:                tqdm: 4.66.1
2023-11-02 12:48:17,343:INFO:               numpy: 1.23.5
2023-11-02 12:48:17,343:INFO:              pandas: 1.5.3
2023-11-02 12:48:17,343:INFO:              jinja2: 3.0.1
2023-11-02 12:48:17,343:INFO:               scipy: 1.10.1
2023-11-02 12:48:17,343:INFO:              joblib: 1.3.2
2023-11-02 12:48:17,343:INFO:             sklearn: 1.1.3
2023-11-02 12:48:17,343:INFO:                pyod: 1.1.1
2023-11-02 12:48:17,343:INFO:            imblearn: 0.11.0
2023-11-02 12:48:17,343:INFO:   category_encoders: 2.6.3
2023-11-02 12:48:17,343:INFO:            lightgbm: 4.1.0
2023-11-02 12:48:17,343:INFO:               numba: 0.58.1
2023-11-02 12:48:17,343:INFO:            requests: 2.31.0
2023-11-02 12:48:17,343:INFO:          matplotlib: 3.4.2
2023-11-02 12:48:17,343:INFO:          scikitplot: 0.3.7
2023-11-02 12:48:17,343:INFO:         yellowbrick: 1.5
2023-11-02 12:48:17,343:INFO:              plotly: 5.18.0
2023-11-02 12:48:17,343:INFO:    plotly-resampler: Not installed
2023-11-02 12:48:17,343:INFO:             kaleido: 0.2.1
2023-11-02 12:48:17,344:INFO:           schemdraw: 0.15
2023-11-02 12:48:17,344:INFO:         statsmodels: 0.14.0
2023-11-02 12:48:17,344:INFO:              sktime: 0.21.1
2023-11-02 12:48:17,344:INFO:               tbats: 1.1.3
2023-11-02 12:48:17,344:INFO:            pmdarima: 2.0.4
2023-11-02 12:48:17,344:INFO:              psutil: 5.9.6
2023-11-02 12:48:17,344:INFO:          markupsafe: 2.1.3
2023-11-02 12:48:17,344:INFO:             pickle5: Not installed
2023-11-02 12:48:17,344:INFO:         cloudpickle: 2.2.1
2023-11-02 12:48:17,344:INFO:         deprecation: 2.1.0
2023-11-02 12:48:17,344:INFO:              xxhash: 3.4.1
2023-11-02 12:48:17,344:INFO:           wurlitzer: 3.0.3
2023-11-02 12:48:17,344:INFO:PyCaret optional dependencies:
2023-11-02 12:48:17,344:INFO:                shap: Not installed
2023-11-02 12:48:17,344:INFO:           interpret: Not installed
2023-11-02 12:48:17,344:INFO:                umap: Not installed
2023-11-02 12:48:17,344:INFO:     ydata_profiling: Not installed
2023-11-02 12:48:17,344:INFO:  explainerdashboard: Not installed
2023-11-02 12:48:17,344:INFO:             autoviz: Not installed
2023-11-02 12:48:17,344:INFO:           fairlearn: Not installed
2023-11-02 12:48:17,344:INFO:          deepchecks: Not installed
2023-11-02 12:48:17,344:INFO:             xgboost: 2.0.0
2023-11-02 12:48:17,345:INFO:            catboost: Not installed
2023-11-02 12:48:17,345:INFO:              kmodes: Not installed
2023-11-02 12:48:17,345:INFO:             mlxtend: Not installed
2023-11-02 12:48:17,345:INFO:       statsforecast: Not installed
2023-11-02 12:48:17,345:INFO:        tune_sklearn: Not installed
2023-11-02 12:48:17,345:INFO:                 ray: Not installed
2023-11-02 12:48:17,345:INFO:            hyperopt: 0.2.7
2023-11-02 12:48:17,345:INFO:              optuna: 3.4.0
2023-11-02 12:48:17,345:INFO:               skopt: Not installed
2023-11-02 12:48:17,345:INFO:              mlflow: Not installed
2023-11-02 12:48:17,345:INFO:              gradio: Not installed
2023-11-02 12:48:17,345:INFO:             fastapi: Not installed
2023-11-02 12:48:17,345:INFO:             uvicorn: Not installed
2023-11-02 12:48:17,345:INFO:              m2cgen: Not installed
2023-11-02 12:48:17,345:INFO:           evidently: Not installed
2023-11-02 12:48:17,345:INFO:               fugue: Not installed
2023-11-02 12:48:17,345:INFO:           streamlit: Not installed
2023-11-02 12:48:17,345:INFO:             prophet: Not installed
2023-11-02 12:48:17,345:INFO:None
2023-11-02 12:48:17,345:INFO:Set up data.
2023-11-02 12:48:17,444:INFO:Set up folding strategy.
2023-11-02 12:48:17,444:INFO:Set up train/test split.
2023-11-02 12:48:17,546:INFO:Set up index.
2023-11-02 12:48:17,552:INFO:Assigning column types.
2023-11-02 12:48:17,603:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-02 12:48:17,605:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,612:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,618:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,743:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,819:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,823:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:17,829:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:17,830:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,837:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,844:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,959:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,018:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,019:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:18,024:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:18,024:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-02 12:48:18,031:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,037:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,156:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,236:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,238:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:18,247:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:18,301:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,373:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,624:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,708:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,709:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:18,713:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:18,714:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-02 12:48:18,731:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,876:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,944:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,945:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:18,949:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:18,963:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,096:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,177:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,178:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:19,182:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:19,183:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-02 12:48:19,310:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,373:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,373:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:19,377:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:19,528:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,601:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,602:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:19,606:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:19,607:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-02 12:48:19,770:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,845:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:19,848:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:19,970:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:20,039:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:20,045:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:20,046:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-02 12:48:20,258:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:20,262:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:20,485:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:20,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:20,494:INFO:Preparing preprocessing pipeline...
2023-11-02 12:48:20,494:INFO:Set up simple imputation.
2023-11-02 12:48:20,500:INFO:Set up column name cleaning.
2023-11-02 12:48:20,688:INFO:Finished creating preprocessing pipeline.
2023-11-02 12:48:20,698:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['is_day:idx', 'is_in_shadow:idx',
                                             'absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'di...
                                             'pressure_50m:hPa', 'prob_rime:p',
                                             'rain_water:kgm2',
                                             'relative_humidity_1000hPa:p',
                                             'sfc_pressure:hPa',
                                             'snow_depth:cm', 'snow_drift:idx',
                                             'snow_melt_10min:mm', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-02 12:48:20,698:INFO:Creating final display dataframe.
2023-11-02 12:48:21,357:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    pv_measurement
2                   Target type        Regression
3           Original data shape       (89684, 57)
4        Transformed data shape       (89684, 57)
5   Transformed train set shape       (62778, 57)
6    Transformed test set shape       (26906, 57)
7              Numeric features                56
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              b61b
2023-11-02 12:48:21,681:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:21,688:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:21,950:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:21,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:21,955:INFO:setup() successfully completed in 4.62s...............
2023-11-02 12:48:22,111:INFO:Initializing compare_models()
2023-11-02 12:48:22,111:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-11-02 12:48:22,112:INFO:Checking exceptions
2023-11-02 12:48:22,153:INFO:Preparing display monitor
2023-11-02 12:48:22,275:INFO:Initializing Linear Regression
2023-11-02 12:48:22,276:INFO:Total runtime is 9.453296661376953e-06 minutes
2023-11-02 12:48:22,289:INFO:SubProcess create_model() called ==================================
2023-11-02 12:48:22,290:INFO:Initializing create_model()
2023-11-02 12:48:22,291:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:48:22,291:INFO:Checking exceptions
2023-11-02 12:48:22,291:INFO:Importing libraries
2023-11-02 12:48:22,291:INFO:Copying training dataset
2023-11-02 12:48:22,364:INFO:Defining folds
2023-11-02 12:48:22,364:INFO:Declaring metric variables
2023-11-02 12:48:22,373:INFO:Importing untrained model
2023-11-02 12:48:22,384:INFO:Linear Regression Imported successfully
2023-11-02 12:48:22,420:INFO:Starting cross validation
2023-11-02 12:48:22,423:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:48:36,812:INFO:Calculating mean and std
2023-11-02 12:48:36,822:INFO:Creating metrics dataframe
2023-11-02 12:48:36,848:INFO:Uploading results into container
2023-11-02 12:48:36,850:INFO:Uploading model into container now
2023-11-02 12:48:36,852:INFO:_master_model_container: 1
2023-11-02 12:48:36,852:INFO:_display_container: 2
2023-11-02 12:48:36,853:INFO:LinearRegression(n_jobs=-1)
2023-11-02 12:48:36,853:INFO:create_model() successfully completed......................................
2023-11-02 12:48:37,415:INFO:SubProcess create_model() end ==================================
2023-11-02 12:48:37,415:INFO:Creating metrics dataframe
2023-11-02 12:48:37,428:INFO:Initializing Lasso Regression
2023-11-02 12:48:37,429:INFO:Total runtime is 0.25255595048268636 minutes
2023-11-02 12:48:37,433:INFO:SubProcess create_model() called ==================================
2023-11-02 12:48:37,433:INFO:Initializing create_model()
2023-11-02 12:48:37,434:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:48:37,434:INFO:Checking exceptions
2023-11-02 12:48:37,434:INFO:Importing libraries
2023-11-02 12:48:37,434:INFO:Copying training dataset
2023-11-02 12:48:37,510:INFO:Defining folds
2023-11-02 12:48:37,510:INFO:Declaring metric variables
2023-11-02 12:48:37,515:INFO:Importing untrained model
2023-11-02 12:48:37,521:INFO:Lasso Regression Imported successfully
2023-11-02 12:48:37,532:INFO:Starting cross validation
2023-11-02 12:48:37,534:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:48:46,649:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.600e+09, tolerance: 3.438e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:46,764:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.528e+09, tolerance: 3.416e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:46,799:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.607e+09, tolerance: 3.432e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:46,819:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.522e+09, tolerance: 3.397e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:46,949:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.537e+09, tolerance: 3.422e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:47,044:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.570e+09, tolerance: 3.437e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:47,062:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.577e+09, tolerance: 3.429e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:47,071:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.532e+09, tolerance: 3.415e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:49,177:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.550e+09, tolerance: 3.415e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:49,271:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.623e+09, tolerance: 3.442e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:49,418:INFO:Calculating mean and std
2023-11-02 12:48:49,421:INFO:Creating metrics dataframe
2023-11-02 12:48:49,428:INFO:Uploading results into container
2023-11-02 12:48:49,430:INFO:Uploading model into container now
2023-11-02 12:48:49,430:INFO:_master_model_container: 2
2023-11-02 12:48:49,431:INFO:_display_container: 2
2023-11-02 12:48:49,431:INFO:Lasso(random_state=123)
2023-11-02 12:48:49,431:INFO:create_model() successfully completed......................................
2023-11-02 12:48:49,734:INFO:SubProcess create_model() end ==================================
2023-11-02 12:48:49,734:INFO:Creating metrics dataframe
2023-11-02 12:48:49,745:INFO:Initializing Ridge Regression
2023-11-02 12:48:49,746:INFO:Total runtime is 0.4578392028808594 minutes
2023-11-02 12:48:49,750:INFO:SubProcess create_model() called ==================================
2023-11-02 12:48:49,751:INFO:Initializing create_model()
2023-11-02 12:48:49,751:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:48:49,751:INFO:Checking exceptions
2023-11-02 12:48:49,751:INFO:Importing libraries
2023-11-02 12:48:49,751:INFO:Copying training dataset
2023-11-02 12:48:49,807:INFO:Defining folds
2023-11-02 12:48:49,807:INFO:Declaring metric variables
2023-11-02 12:48:49,811:INFO:Importing untrained model
2023-11-02 12:48:49,821:INFO:Ridge Regression Imported successfully
2023-11-02 12:48:49,830:INFO:Starting cross validation
2023-11-02 12:48:49,833:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:48:50,241:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.62328e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,242:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.64158e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,305:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.62804e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,357:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.63451e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,414:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.61666e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,468:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.57377e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,517:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.63188e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,540:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.63479e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,669:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.60506e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,686:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.63581e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,830:INFO:Calculating mean and std
2023-11-02 12:48:50,836:INFO:Creating metrics dataframe
2023-11-02 12:48:50,844:INFO:Uploading results into container
2023-11-02 12:48:50,845:INFO:Uploading model into container now
2023-11-02 12:48:50,848:INFO:_master_model_container: 3
2023-11-02 12:48:50,848:INFO:_display_container: 2
2023-11-02 12:48:50,849:INFO:Ridge(random_state=123)
2023-11-02 12:48:50,850:INFO:create_model() successfully completed......................................
2023-11-02 12:48:51,203:INFO:SubProcess create_model() end ==================================
2023-11-02 12:48:51,204:INFO:Creating metrics dataframe
2023-11-02 12:48:51,235:INFO:Initializing Elastic Net
2023-11-02 12:48:51,236:INFO:Total runtime is 0.4826743364334107 minutes
2023-11-02 12:48:51,244:INFO:SubProcess create_model() called ==================================
2023-11-02 12:48:51,245:INFO:Initializing create_model()
2023-11-02 12:48:51,245:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:48:51,245:INFO:Checking exceptions
2023-11-02 12:48:51,245:INFO:Importing libraries
2023-11-02 12:48:51,245:INFO:Copying training dataset
2023-11-02 12:48:51,323:INFO:Defining folds
2023-11-02 12:48:51,324:INFO:Declaring metric variables
2023-11-02 12:48:51,329:INFO:Importing untrained model
2023-11-02 12:48:51,336:INFO:Elastic Net Imported successfully
2023-11-02 12:48:51,346:INFO:Starting cross validation
2023-11-02 12:48:51,349:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:03,192:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.431e+09, tolerance: 3.397e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,285:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.428e+09, tolerance: 3.416e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,332:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.520e+09, tolerance: 3.438e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,476:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.529e+09, tolerance: 3.432e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,694:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.448e+09, tolerance: 3.422e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,754:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.486e+09, tolerance: 3.429e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,761:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.482e+09, tolerance: 3.437e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,762:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.444e+09, tolerance: 3.415e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:06,310:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.545e+09, tolerance: 3.442e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:06,403:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+09, tolerance: 3.415e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:06,548:INFO:Calculating mean and std
2023-11-02 12:49:06,552:INFO:Creating metrics dataframe
2023-11-02 12:49:06,561:INFO:Uploading results into container
2023-11-02 12:49:06,563:INFO:Uploading model into container now
2023-11-02 12:49:06,564:INFO:_master_model_container: 4
2023-11-02 12:49:06,565:INFO:_display_container: 2
2023-11-02 12:49:06,565:INFO:ElasticNet(random_state=123)
2023-11-02 12:49:06,566:INFO:create_model() successfully completed......................................
2023-11-02 12:49:06,879:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:06,880:INFO:Creating metrics dataframe
2023-11-02 12:49:06,890:INFO:Initializing Least Angle Regression
2023-11-02 12:49:06,891:INFO:Total runtime is 0.7435892184575399 minutes
2023-11-02 12:49:06,894:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:06,895:INFO:Initializing create_model()
2023-11-02 12:49:06,895:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:06,895:INFO:Checking exceptions
2023-11-02 12:49:06,895:INFO:Importing libraries
2023-11-02 12:49:06,895:INFO:Copying training dataset
2023-11-02 12:49:06,948:INFO:Defining folds
2023-11-02 12:49:06,948:INFO:Declaring metric variables
2023-11-02 12:49:06,952:INFO:Importing untrained model
2023-11-02 12:49:06,957:INFO:Least Angle Regression Imported successfully
2023-11-02 12:49:06,967:INFO:Starting cross validation
2023-11-02 12:49:06,969:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:07,345:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,345:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,556:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,626:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,642:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,827:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,907:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:08,050:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:08,274:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:08,288:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:08,363:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.107e-03, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-02 12:49:08,365:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.976e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-02 12:49:08,553:INFO:Calculating mean and std
2023-11-02 12:49:08,558:INFO:Creating metrics dataframe
2023-11-02 12:49:08,565:INFO:Uploading results into container
2023-11-02 12:49:08,566:INFO:Uploading model into container now
2023-11-02 12:49:08,567:INFO:_master_model_container: 5
2023-11-02 12:49:08,567:INFO:_display_container: 2
2023-11-02 12:49:08,569:INFO:Lars(random_state=123)
2023-11-02 12:49:08,569:INFO:create_model() successfully completed......................................
2023-11-02 12:49:08,913:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:08,913:INFO:Creating metrics dataframe
2023-11-02 12:49:08,927:INFO:Initializing Lasso Least Angle Regression
2023-11-02 12:49:08,928:INFO:Total runtime is 0.7775471727053325 minutes
2023-11-02 12:49:08,934:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:08,935:INFO:Initializing create_model()
2023-11-02 12:49:08,935:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:08,935:INFO:Checking exceptions
2023-11-02 12:49:08,935:INFO:Importing libraries
2023-11-02 12:49:08,935:INFO:Copying training dataset
2023-11-02 12:49:09,015:INFO:Defining folds
2023-11-02 12:49:09,016:INFO:Declaring metric variables
2023-11-02 12:49:09,023:INFO:Importing untrained model
2023-11-02 12:49:09,035:INFO:Lasso Least Angle Regression Imported successfully
2023-11-02 12:49:09,051:INFO:Starting cross validation
2023-11-02 12:49:09,053:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:09,495:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,516:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,537:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,639:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,741:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,773:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,794:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,822:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:10,145:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:10,178:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:10,432:INFO:Calculating mean and std
2023-11-02 12:49:10,436:INFO:Creating metrics dataframe
2023-11-02 12:49:10,444:INFO:Uploading results into container
2023-11-02 12:49:10,446:INFO:Uploading model into container now
2023-11-02 12:49:10,446:INFO:_master_model_container: 6
2023-11-02 12:49:10,446:INFO:_display_container: 2
2023-11-02 12:49:10,447:INFO:LassoLars(random_state=123)
2023-11-02 12:49:10,447:INFO:create_model() successfully completed......................................
2023-11-02 12:49:10,765:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:10,765:INFO:Creating metrics dataframe
2023-11-02 12:49:10,781:INFO:Initializing Orthogonal Matching Pursuit
2023-11-02 12:49:10,782:INFO:Total runtime is 0.8084387222925822 minutes
2023-11-02 12:49:10,788:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:10,788:INFO:Initializing create_model()
2023-11-02 12:49:10,789:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:10,789:INFO:Checking exceptions
2023-11-02 12:49:10,789:INFO:Importing libraries
2023-11-02 12:49:10,790:INFO:Copying training dataset
2023-11-02 12:49:10,865:INFO:Defining folds
2023-11-02 12:49:10,865:INFO:Declaring metric variables
2023-11-02 12:49:10,872:INFO:Importing untrained model
2023-11-02 12:49:10,881:INFO:Orthogonal Matching Pursuit Imported successfully
2023-11-02 12:49:10,893:INFO:Starting cross validation
2023-11-02 12:49:10,895:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:11,248:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,350:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,365:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,373:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,466:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,523:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,644:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,759:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:12,200:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:12,252:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:12,490:INFO:Calculating mean and std
2023-11-02 12:49:12,493:INFO:Creating metrics dataframe
2023-11-02 12:49:12,498:INFO:Uploading results into container
2023-11-02 12:49:12,500:INFO:Uploading model into container now
2023-11-02 12:49:12,501:INFO:_master_model_container: 7
2023-11-02 12:49:12,501:INFO:_display_container: 2
2023-11-02 12:49:12,501:INFO:OrthogonalMatchingPursuit()
2023-11-02 12:49:12,501:INFO:create_model() successfully completed......................................
2023-11-02 12:49:12,809:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:12,810:INFO:Creating metrics dataframe
2023-11-02 12:49:12,829:INFO:Initializing Bayesian Ridge
2023-11-02 12:49:12,829:INFO:Total runtime is 0.8425685365994772 minutes
2023-11-02 12:49:12,836:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:12,836:INFO:Initializing create_model()
2023-11-02 12:49:12,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:12,837:INFO:Checking exceptions
2023-11-02 12:49:12,837:INFO:Importing libraries
2023-11-02 12:49:12,837:INFO:Copying training dataset
2023-11-02 12:49:12,904:INFO:Defining folds
2023-11-02 12:49:12,904:INFO:Declaring metric variables
2023-11-02 12:49:12,911:INFO:Importing untrained model
2023-11-02 12:49:12,915:INFO:Bayesian Ridge Imported successfully
2023-11-02 12:49:12,926:INFO:Starting cross validation
2023-11-02 12:49:12,928:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:15,753:INFO:Calculating mean and std
2023-11-02 12:49:15,757:INFO:Creating metrics dataframe
2023-11-02 12:49:15,765:INFO:Uploading results into container
2023-11-02 12:49:15,767:INFO:Uploading model into container now
2023-11-02 12:49:15,768:INFO:_master_model_container: 8
2023-11-02 12:49:15,768:INFO:_display_container: 2
2023-11-02 12:49:15,769:INFO:BayesianRidge()
2023-11-02 12:49:15,769:INFO:create_model() successfully completed......................................
2023-11-02 12:49:16,072:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:16,072:INFO:Creating metrics dataframe
2023-11-02 12:49:16,087:INFO:Initializing Passive Aggressive Regressor
2023-11-02 12:49:16,087:INFO:Total runtime is 0.8968613346417745 minutes
2023-11-02 12:49:16,091:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:16,092:INFO:Initializing create_model()
2023-11-02 12:49:16,092:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:16,092:INFO:Checking exceptions
2023-11-02 12:49:16,092:INFO:Importing libraries
2023-11-02 12:49:16,092:INFO:Copying training dataset
2023-11-02 12:49:16,153:INFO:Defining folds
2023-11-02 12:49:16,154:INFO:Declaring metric variables
2023-11-02 12:49:16,159:INFO:Importing untrained model
2023-11-02 12:49:16,170:INFO:Passive Aggressive Regressor Imported successfully
2023-11-02 12:49:16,188:INFO:Starting cross validation
2023-11-02 12:49:16,190:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:18,711:INFO:Calculating mean and std
2023-11-02 12:49:18,718:INFO:Creating metrics dataframe
2023-11-02 12:49:18,727:INFO:Uploading results into container
2023-11-02 12:49:18,728:INFO:Uploading model into container now
2023-11-02 12:49:18,730:INFO:_master_model_container: 9
2023-11-02 12:49:18,730:INFO:_display_container: 2
2023-11-02 12:49:18,731:INFO:PassiveAggressiveRegressor(random_state=123)
2023-11-02 12:49:18,731:INFO:create_model() successfully completed......................................
2023-11-02 12:49:19,050:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:19,050:INFO:Creating metrics dataframe
2023-11-02 12:49:19,065:INFO:Initializing Huber Regressor
2023-11-02 12:49:19,065:INFO:Total runtime is 0.9465027372042338 minutes
2023-11-02 12:49:19,071:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:19,071:INFO:Initializing create_model()
2023-11-02 12:49:19,072:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:19,072:INFO:Checking exceptions
2023-11-02 12:49:19,072:INFO:Importing libraries
2023-11-02 12:49:19,072:INFO:Copying training dataset
2023-11-02 12:49:19,143:INFO:Defining folds
2023-11-02 12:49:19,143:INFO:Declaring metric variables
2023-11-02 12:49:19,147:INFO:Importing untrained model
2023-11-02 12:49:19,153:INFO:Huber Regressor Imported successfully
2023-11-02 12:49:19,162:INFO:Starting cross validation
2023-11-02 12:49:19,164:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:32,289:INFO:Calculating mean and std
2023-11-02 12:49:32,295:INFO:Creating metrics dataframe
2023-11-02 12:49:32,302:INFO:Uploading results into container
2023-11-02 12:49:32,304:INFO:Uploading model into container now
2023-11-02 12:49:32,305:INFO:_master_model_container: 10
2023-11-02 12:49:32,306:INFO:_display_container: 2
2023-11-02 12:49:32,306:INFO:HuberRegressor()
2023-11-02 12:49:32,307:INFO:create_model() successfully completed......................................
2023-11-02 12:49:32,603:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:32,603:INFO:Creating metrics dataframe
2023-11-02 12:49:32,616:INFO:Initializing K Neighbors Regressor
2023-11-02 12:49:32,617:INFO:Total runtime is 1.172360916932424 minutes
2023-11-02 12:49:32,623:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:32,624:INFO:Initializing create_model()
2023-11-02 12:49:32,624:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:32,625:INFO:Checking exceptions
2023-11-02 12:49:32,625:INFO:Importing libraries
2023-11-02 12:49:32,625:INFO:Copying training dataset
2023-11-02 12:49:32,696:INFO:Defining folds
2023-11-02 12:49:32,697:INFO:Declaring metric variables
2023-11-02 12:49:32,703:INFO:Importing untrained model
2023-11-02 12:49:32,723:INFO:K Neighbors Regressor Imported successfully
2023-11-02 12:49:32,733:INFO:Starting cross validation
2023-11-02 12:49:32,735:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:12:44,193:INFO:Calculating mean and std
2023-11-02 13:12:44,543:INFO:Creating metrics dataframe
2023-11-02 13:12:44,767:INFO:Uploading results into container
2023-11-02 13:12:44,778:INFO:Uploading model into container now
2023-11-02 13:12:44,803:INFO:_master_model_container: 11
2023-11-02 13:12:44,804:INFO:_display_container: 2
2023-11-02 13:12:44,834:INFO:KNeighborsRegressor(n_jobs=-1)
2023-11-02 13:12:44,834:INFO:create_model() successfully completed......................................
2023-11-02 13:12:49,199:INFO:SubProcess create_model() end ==================================
2023-11-02 13:12:49,199:INFO:Creating metrics dataframe
2023-11-02 13:12:49,236:INFO:Initializing Decision Tree Regressor
2023-11-02 13:12:49,236:INFO:Total runtime is 24.4493457198143 minutes
2023-11-02 13:12:49,241:INFO:SubProcess create_model() called ==================================
2023-11-02 13:12:49,242:INFO:Initializing create_model()
2023-11-02 13:12:49,242:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:12:49,242:INFO:Checking exceptions
2023-11-02 13:12:49,242:INFO:Importing libraries
2023-11-02 13:12:49,248:INFO:Copying training dataset
2023-11-02 13:12:49,431:INFO:Defining folds
2023-11-02 13:12:49,432:INFO:Declaring metric variables
2023-11-02 13:12:49,438:INFO:Importing untrained model
2023-11-02 13:12:49,442:INFO:Decision Tree Regressor Imported successfully
2023-11-02 13:12:49,452:INFO:Starting cross validation
2023-11-02 13:12:49,454:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:13:00,198:INFO:Calculating mean and std
2023-11-02 13:13:00,202:INFO:Creating metrics dataframe
2023-11-02 13:13:00,209:INFO:Uploading results into container
2023-11-02 13:13:00,211:INFO:Uploading model into container now
2023-11-02 13:13:00,213:INFO:_master_model_container: 12
2023-11-02 13:13:00,213:INFO:_display_container: 2
2023-11-02 13:13:00,215:INFO:DecisionTreeRegressor(random_state=123)
2023-11-02 13:13:00,215:INFO:create_model() successfully completed......................................
2023-11-02 13:13:00,539:INFO:SubProcess create_model() end ==================================
2023-11-02 13:13:00,539:INFO:Creating metrics dataframe
2023-11-02 13:13:00,555:INFO:Initializing Random Forest Regressor
2023-11-02 13:13:00,555:INFO:Total runtime is 24.637998783588408 minutes
2023-11-02 13:13:00,559:INFO:SubProcess create_model() called ==================================
2023-11-02 13:13:00,560:INFO:Initializing create_model()
2023-11-02 13:13:00,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:13:00,560:INFO:Checking exceptions
2023-11-02 13:13:00,560:INFO:Importing libraries
2023-11-02 13:13:00,560:INFO:Copying training dataset
2023-11-02 13:13:00,634:INFO:Defining folds
2023-11-02 13:13:00,634:INFO:Declaring metric variables
2023-11-02 13:13:00,640:INFO:Importing untrained model
2023-11-02 13:13:00,647:INFO:Random Forest Regressor Imported successfully
2023-11-02 13:13:00,660:INFO:Starting cross validation
2023-11-02 13:13:00,663:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:20:55,295:INFO:Calculating mean and std
2023-11-02 13:20:55,312:INFO:Creating metrics dataframe
2023-11-02 13:20:55,335:INFO:Uploading results into container
2023-11-02 13:20:55,339:INFO:Uploading model into container now
2023-11-02 13:20:55,342:INFO:_master_model_container: 13
2023-11-02 13:20:55,342:INFO:_display_container: 2
2023-11-02 13:20:55,345:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:20:55,345:INFO:create_model() successfully completed......................................
2023-11-02 13:20:56,260:INFO:SubProcess create_model() end ==================================
2023-11-02 13:20:56,261:INFO:Creating metrics dataframe
2023-11-02 13:20:56,278:INFO:Initializing Extra Trees Regressor
2023-11-02 13:20:56,278:INFO:Total runtime is 32.566712435086565 minutes
2023-11-02 13:20:56,282:INFO:SubProcess create_model() called ==================================
2023-11-02 13:20:56,283:INFO:Initializing create_model()
2023-11-02 13:20:56,283:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:20:56,283:INFO:Checking exceptions
2023-11-02 13:20:56,283:INFO:Importing libraries
2023-11-02 13:20:56,283:INFO:Copying training dataset
2023-11-02 13:20:56,383:INFO:Defining folds
2023-11-02 13:20:56,383:INFO:Declaring metric variables
2023-11-02 13:20:56,387:INFO:Importing untrained model
2023-11-02 13:20:56,392:INFO:Extra Trees Regressor Imported successfully
2023-11-02 13:20:56,399:INFO:Starting cross validation
2023-11-02 13:20:56,401:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:23:46,594:INFO:Calculating mean and std
2023-11-02 13:23:46,612:INFO:Creating metrics dataframe
2023-11-02 13:23:46,636:INFO:Uploading results into container
2023-11-02 13:23:46,638:INFO:Uploading model into container now
2023-11-02 13:23:46,641:INFO:_master_model_container: 14
2023-11-02 13:23:46,641:INFO:_display_container: 2
2023-11-02 13:23:46,643:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:23:46,643:INFO:create_model() successfully completed......................................
2023-11-02 13:23:47,544:INFO:SubProcess create_model() end ==================================
2023-11-02 13:23:47,544:INFO:Creating metrics dataframe
2023-11-02 13:23:47,564:INFO:Initializing AdaBoost Regressor
2023-11-02 13:23:47,564:INFO:Total runtime is 35.42147845427195 minutes
2023-11-02 13:23:47,568:INFO:SubProcess create_model() called ==================================
2023-11-02 13:23:47,569:INFO:Initializing create_model()
2023-11-02 13:23:47,569:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:23:47,569:INFO:Checking exceptions
2023-11-02 13:23:47,569:INFO:Importing libraries
2023-11-02 13:23:47,569:INFO:Copying training dataset
2023-11-02 13:23:47,681:INFO:Defining folds
2023-11-02 13:23:47,682:INFO:Declaring metric variables
2023-11-02 13:23:47,686:INFO:Importing untrained model
2023-11-02 13:23:47,694:INFO:AdaBoost Regressor Imported successfully
2023-11-02 13:23:47,704:INFO:Starting cross validation
2023-11-02 13:23:47,706:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:24:22,261:INFO:Calculating mean and std
2023-11-02 13:24:22,264:INFO:Creating metrics dataframe
2023-11-02 13:24:22,270:INFO:Uploading results into container
2023-11-02 13:24:22,271:INFO:Uploading model into container now
2023-11-02 13:24:22,272:INFO:_master_model_container: 15
2023-11-02 13:24:22,272:INFO:_display_container: 2
2023-11-02 13:24:22,273:INFO:AdaBoostRegressor(random_state=123)
2023-11-02 13:24:22,273:INFO:create_model() successfully completed......................................
2023-11-02 13:24:22,552:INFO:SubProcess create_model() end ==================================
2023-11-02 13:24:22,552:INFO:Creating metrics dataframe
2023-11-02 13:24:22,567:INFO:Initializing Gradient Boosting Regressor
2023-11-02 13:24:22,568:INFO:Total runtime is 36.00487538576126 minutes
2023-11-02 13:24:22,571:INFO:SubProcess create_model() called ==================================
2023-11-02 13:24:22,572:INFO:Initializing create_model()
2023-11-02 13:24:22,572:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:24:22,572:INFO:Checking exceptions
2023-11-02 13:24:22,573:INFO:Importing libraries
2023-11-02 13:24:22,573:INFO:Copying training dataset
2023-11-02 13:24:22,625:INFO:Defining folds
2023-11-02 13:24:22,626:INFO:Declaring metric variables
2023-11-02 13:24:22,630:INFO:Importing untrained model
2023-11-02 13:24:22,638:INFO:Gradient Boosting Regressor Imported successfully
2023-11-02 13:24:22,648:INFO:Starting cross validation
2023-11-02 13:24:22,650:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:26:55,883:INFO:Calculating mean and std
2023-11-02 13:26:55,889:INFO:Creating metrics dataframe
2023-11-02 13:26:55,898:INFO:Uploading results into container
2023-11-02 13:26:55,899:INFO:Uploading model into container now
2023-11-02 13:26:55,901:INFO:_master_model_container: 16
2023-11-02 13:26:55,901:INFO:_display_container: 2
2023-11-02 13:26:55,902:INFO:GradientBoostingRegressor(random_state=123)
2023-11-02 13:26:55,903:INFO:create_model() successfully completed......................................
2023-11-02 13:26:56,186:INFO:SubProcess create_model() end ==================================
2023-11-02 13:26:56,186:INFO:Creating metrics dataframe
2023-11-02 13:26:56,199:INFO:Initializing Extreme Gradient Boosting
2023-11-02 13:26:56,199:INFO:Total runtime is 38.56540245612462 minutes
2023-11-02 13:26:56,203:INFO:SubProcess create_model() called ==================================
2023-11-02 13:26:56,203:INFO:Initializing create_model()
2023-11-02 13:26:56,203:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:26:56,203:INFO:Checking exceptions
2023-11-02 13:26:56,203:INFO:Importing libraries
2023-11-02 13:26:56,204:INFO:Copying training dataset
2023-11-02 13:26:56,252:INFO:Defining folds
2023-11-02 13:26:56,253:INFO:Declaring metric variables
2023-11-02 13:26:56,259:INFO:Importing untrained model
2023-11-02 13:26:56,263:INFO:Extreme Gradient Boosting Imported successfully
2023-11-02 13:26:56,271:INFO:Starting cross validation
2023-11-02 13:26:56,273:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:27:06,939:INFO:Calculating mean and std
2023-11-02 13:27:06,944:INFO:Creating metrics dataframe
2023-11-02 13:27:06,953:INFO:Uploading results into container
2023-11-02 13:27:06,955:INFO:Uploading model into container now
2023-11-02 13:27:06,955:INFO:_master_model_container: 17
2023-11-02 13:27:06,955:INFO:_display_container: 2
2023-11-02 13:27:06,957:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-02 13:27:06,957:INFO:create_model() successfully completed......................................
2023-11-02 13:27:07,275:INFO:SubProcess create_model() end ==================================
2023-11-02 13:27:07,275:INFO:Creating metrics dataframe
2023-11-02 13:27:07,292:INFO:Initializing Light Gradient Boosting Machine
2023-11-02 13:27:07,292:INFO:Total runtime is 38.750277403990424 minutes
2023-11-02 13:27:07,296:INFO:SubProcess create_model() called ==================================
2023-11-02 13:27:07,296:INFO:Initializing create_model()
2023-11-02 13:27:07,296:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:27:07,297:INFO:Checking exceptions
2023-11-02 13:27:07,297:INFO:Importing libraries
2023-11-02 13:27:07,297:INFO:Copying training dataset
2023-11-02 13:27:07,346:INFO:Defining folds
2023-11-02 13:27:07,346:INFO:Declaring metric variables
2023-11-02 13:27:07,350:INFO:Importing untrained model
2023-11-02 13:27:07,355:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-02 13:27:07,362:INFO:Starting cross validation
2023-11-02 13:27:07,364:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:27:28,213:INFO:Calculating mean and std
2023-11-02 13:27:28,216:INFO:Creating metrics dataframe
2023-11-02 13:27:28,224:INFO:Uploading results into container
2023-11-02 13:27:28,226:INFO:Uploading model into container now
2023-11-02 13:27:28,227:INFO:_master_model_container: 18
2023-11-02 13:27:28,227:INFO:_display_container: 2
2023-11-02 13:27:28,228:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:27:28,228:INFO:create_model() successfully completed......................................
2023-11-02 13:27:28,547:INFO:SubProcess create_model() end ==================================
2023-11-02 13:27:28,547:INFO:Creating metrics dataframe
2023-11-02 13:27:28,566:INFO:Initializing Dummy Regressor
2023-11-02 13:27:28,566:INFO:Total runtime is 39.10485244989395 minutes
2023-11-02 13:27:28,571:INFO:SubProcess create_model() called ==================================
2023-11-02 13:27:28,572:INFO:Initializing create_model()
2023-11-02 13:27:28,572:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:27:28,572:INFO:Checking exceptions
2023-11-02 13:27:28,572:INFO:Importing libraries
2023-11-02 13:27:28,573:INFO:Copying training dataset
2023-11-02 13:27:28,760:INFO:Defining folds
2023-11-02 13:27:28,760:INFO:Declaring metric variables
2023-11-02 13:27:28,768:INFO:Importing untrained model
2023-11-02 13:27:28,778:INFO:Dummy Regressor Imported successfully
2023-11-02 13:27:28,789:INFO:Starting cross validation
2023-11-02 13:27:28,792:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:27:29,608:INFO:Calculating mean and std
2023-11-02 13:27:29,610:INFO:Creating metrics dataframe
2023-11-02 13:27:29,614:INFO:Uploading results into container
2023-11-02 13:27:29,615:INFO:Uploading model into container now
2023-11-02 13:27:29,616:INFO:_master_model_container: 19
2023-11-02 13:27:29,616:INFO:_display_container: 2
2023-11-02 13:27:29,616:INFO:DummyRegressor()
2023-11-02 13:27:29,617:INFO:create_model() successfully completed......................................
2023-11-02 13:27:29,935:INFO:SubProcess create_model() end ==================================
2023-11-02 13:27:29,935:INFO:Creating metrics dataframe
2023-11-02 13:27:29,984:INFO:Initializing create_model()
2023-11-02 13:27:29,984:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:27:29,984:INFO:Checking exceptions
2023-11-02 13:27:29,994:INFO:Importing libraries
2023-11-02 13:27:29,995:INFO:Copying training dataset
2023-11-02 13:27:30,066:INFO:Defining folds
2023-11-02 13:27:30,066:INFO:Declaring metric variables
2023-11-02 13:27:30,067:INFO:Importing untrained model
2023-11-02 13:27:30,067:INFO:Declaring custom model
2023-11-02 13:27:30,069:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-02 13:27:30,070:INFO:Cross validation set to False
2023-11-02 13:27:30,070:INFO:Fitting Model
2023-11-02 13:27:30,454:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030541 seconds.
2023-11-02 13:27:30,454:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-02 13:27:30,455:INFO:[LightGBM] [Info] Total Bins 9347
2023-11-02 13:27:30,457:INFO:[LightGBM] [Info] Number of data points in the train set: 62778, number of used features: 54
2023-11-02 13:27:30,460:INFO:[LightGBM] [Info] Start training from score 292.766212
2023-11-02 13:27:31,122:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:27:31,122:INFO:create_model() successfully completed......................................
2023-11-02 13:27:31,527:INFO:_master_model_container: 19
2023-11-02 13:27:31,528:INFO:_display_container: 2
2023-11-02 13:27:31,528:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:27:31,529:INFO:compare_models() successfully completed......................................
2023-11-02 13:27:31,668:INFO:Initializing evaluate_model()
2023-11-02 13:27:31,668:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-02 13:27:31,712:INFO:Initializing plot_model()
2023-11-02 13:27:31,712:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, system=True)
2023-11-02 13:27:31,712:INFO:Checking exceptions
2023-11-02 13:27:31,736:INFO:Preloading libraries
2023-11-02 13:27:31,754:INFO:Copying training dataset
2023-11-02 13:27:31,754:INFO:Plot type: pipeline
2023-11-02 13:27:32,028:INFO:Visual Rendered Successfully
2023-11-02 13:27:32,341:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:32,346:INFO:Initializing evaluate_model()
2023-11-02 13:27:32,346:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cd460>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-02 13:27:32,414:INFO:Initializing plot_model()
2023-11-02 13:27:32,414:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cd460>, system=True)
2023-11-02 13:27:32,414:INFO:Checking exceptions
2023-11-02 13:27:32,433:INFO:Preloading libraries
2023-11-02 13:27:32,441:INFO:Copying training dataset
2023-11-02 13:27:32,441:INFO:Plot type: pipeline
2023-11-02 13:27:32,656:INFO:Visual Rendered Successfully
2023-11-02 13:27:33,078:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:33,105:INFO:Initializing evaluate_model()
2023-11-02 13:27:33,106:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cdee0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-02 13:27:33,178:INFO:Initializing plot_model()
2023-11-02 13:27:33,178:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cdee0>, system=True)
2023-11-02 13:27:33,179:INFO:Checking exceptions
2023-11-02 13:27:33,192:INFO:Preloading libraries
2023-11-02 13:27:33,198:INFO:Copying training dataset
2023-11-02 13:27:33,198:INFO:Plot type: pipeline
2023-11-02 13:27:33,431:INFO:Visual Rendered Successfully
2023-11-02 13:27:33,748:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:33,752:INFO:Initializing plot_model()
2023-11-02 13:27:33,753:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, system=True)
2023-11-02 13:27:33,753:INFO:Checking exceptions
2023-11-02 13:27:33,786:INFO:Preloading libraries
2023-11-02 13:27:33,791:INFO:Copying training dataset
2023-11-02 13:27:33,791:INFO:Plot type: feature
2023-11-02 13:27:33,792:WARNING:No coef_ found. Trying feature_importances_
2023-11-02 13:27:34,139:INFO:Visual Rendered Successfully
2023-11-02 13:27:34,419:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:34,420:INFO:Initializing plot_model()
2023-11-02 13:27:34,420:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cd460>, system=True)
2023-11-02 13:27:34,420:INFO:Checking exceptions
2023-11-02 13:27:34,433:INFO:Preloading libraries
2023-11-02 13:27:34,437:INFO:Copying training dataset
2023-11-02 13:27:34,437:INFO:Plot type: feature
2023-11-02 13:27:34,438:WARNING:No coef_ found. Trying feature_importances_
2023-11-02 13:27:34,755:INFO:Visual Rendered Successfully
2023-11-02 13:27:35,034:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:35,035:INFO:Initializing plot_model()
2023-11-02 13:27:35,035:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cdee0>, system=True)
2023-11-02 13:27:35,035:INFO:Checking exceptions
2023-11-02 13:27:35,044:INFO:Preloading libraries
2023-11-02 13:27:35,049:INFO:Copying training dataset
2023-11-02 13:27:35,049:INFO:Plot type: feature
2023-11-02 13:27:35,050:WARNING:No coef_ found. Trying feature_importances_
2023-11-02 13:27:35,384:INFO:Visual Rendered Successfully
2023-11-02 13:27:35,673:INFO:plot_model() successfully completed......................................
2023-11-02 13:30:12,936:INFO:Initializing predict_model()
2023-11-02 13:30:12,939:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fda80f4e820>)
2023-11-02 13:30:12,939:INFO:Checking exceptions
2023-11-02 13:30:12,939:INFO:Preloading libraries
2023-11-02 13:30:12,957:INFO:Set up data.
2023-11-02 13:30:13,067:INFO:Set up index.
2023-11-02 13:37:45,977:WARNING:<ipython-input-127-c47b07307b6d>:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  y_pred[i] = 0

2023-11-02 13:38:03,324:WARNING:<ipython-input-128-d8c14def8f49>:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  y_pred[i] = 0

2023-11-03 14:09:42,873:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-03 14:09:42,875:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-03 14:09:42,875:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-03 14:09:42,875:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-03 14:43:45,077:WARNING:<ipython-input-30-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:43:45,197:WARNING:<ipython-input-30-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:43:45,286:WARNING:<ipython-input-30-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:45:58,325:WARNING:<ipython-input-44-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:45:58,528:WARNING:<ipython-input-44-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:45:58,671:WARNING:<ipython-input-44-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:45:59,123:INFO:PyCaret RegressionExperiment
2023-11-03 14:45:59,123:INFO:Logging name: reg-default-name
2023-11-03 14:45:59,124:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-03 14:45:59,124:INFO:version 3.1.0
2023-11-03 14:45:59,124:INFO:Initializing setup()
2023-11-03 14:45:59,124:INFO:self.USI: 43bf
2023-11-03 14:45:59,124:INFO:self._variable_keys: {'n_jobs_param', 'X_train', 'idx', 'X', 'exp_id', 'gpu_param', 'gpu_n_jobs_param', 'fold_generator', 'y_train', 'target_param', 'fold_groups_param', 'data', 'X_test', '_available_plots', 'log_plots_param', 'fold_shuffle_param', 'USI', '_ml_usecase', 'y', 'pipeline', 'y_test', 'exp_name_log', 'transform_target_param', 'seed', 'logging_param', 'memory', 'html_param'}
2023-11-03 14:45:59,124:INFO:Checking environment
2023-11-03 14:45:59,124:INFO:python_version: 3.9.6
2023-11-03 14:45:59,125:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-03 14:45:59,125:INFO:machine: x86_64
2023-11-03 14:45:59,371:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:45:59,372:INFO:Memory: svmem(total=8589934592, available=2331639808, percent=72.9, used=4797435904, free=16560128, active=2314715136, inactive=2313048064, wired=2482720768)
2023-11-03 14:45:59,372:INFO:Physical Core: 4
2023-11-03 14:45:59,373:INFO:Logical Core: 8
2023-11-03 14:45:59,373:INFO:Checking libraries
2023-11-03 14:45:59,377:INFO:System:
2023-11-03 14:45:59,380:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-03 14:45:59,380:INFO:executable: /usr/local/bin/python3.9
2023-11-03 14:45:59,380:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:45:59,381:INFO:PyCaret required dependencies:
2023-11-03 14:46:00,709:INFO:                 pip: 23.3.1
2023-11-03 14:46:00,713:INFO:          setuptools: 56.0.0
2023-11-03 14:46:00,713:INFO:             pycaret: 3.1.0
2023-11-03 14:46:00,713:INFO:             IPython: 7.28.0
2023-11-03 14:46:00,713:INFO:          ipywidgets: 8.1.1
2023-11-03 14:46:00,714:INFO:                tqdm: 4.66.1
2023-11-03 14:46:00,714:INFO:               numpy: 1.23.5
2023-11-03 14:46:00,714:INFO:              pandas: 1.5.3
2023-11-03 14:46:00,714:INFO:              jinja2: 3.0.1
2023-11-03 14:46:00,714:INFO:               scipy: 1.10.1
2023-11-03 14:46:00,714:INFO:              joblib: 1.3.2
2023-11-03 14:46:00,714:INFO:             sklearn: 1.1.3
2023-11-03 14:46:00,714:INFO:                pyod: 1.1.1
2023-11-03 14:46:00,714:INFO:            imblearn: 0.11.0
2023-11-03 14:46:00,714:INFO:   category_encoders: 2.6.3
2023-11-03 14:46:00,714:INFO:            lightgbm: 4.1.0
2023-11-03 14:46:00,714:INFO:               numba: 0.58.1
2023-11-03 14:46:00,714:INFO:            requests: 2.31.0
2023-11-03 14:46:00,714:INFO:          matplotlib: 3.4.2
2023-11-03 14:46:00,714:INFO:          scikitplot: 0.3.7
2023-11-03 14:46:00,714:INFO:         yellowbrick: 1.5
2023-11-03 14:46:00,714:INFO:              plotly: 5.18.0
2023-11-03 14:46:00,714:INFO:    plotly-resampler: Not installed
2023-11-03 14:46:00,714:INFO:             kaleido: 0.2.1
2023-11-03 14:46:00,714:INFO:           schemdraw: 0.15
2023-11-03 14:46:00,715:INFO:         statsmodels: 0.14.0
2023-11-03 14:46:00,715:INFO:              sktime: 0.21.1
2023-11-03 14:46:00,715:INFO:               tbats: 1.1.3
2023-11-03 14:46:00,715:INFO:            pmdarima: 2.0.4
2023-11-03 14:46:00,715:INFO:              psutil: 5.9.6
2023-11-03 14:46:00,715:INFO:          markupsafe: 2.1.3
2023-11-03 14:46:00,715:INFO:             pickle5: Not installed
2023-11-03 14:46:00,715:INFO:         cloudpickle: 2.2.1
2023-11-03 14:46:00,715:INFO:         deprecation: 2.1.0
2023-11-03 14:46:00,715:INFO:              xxhash: 3.4.1
2023-11-03 14:46:00,715:INFO:           wurlitzer: 3.0.3
2023-11-03 14:46:00,715:INFO:PyCaret optional dependencies:
2023-11-03 14:46:01,168:INFO:                shap: Not installed
2023-11-03 14:46:01,169:INFO:           interpret: Not installed
2023-11-03 14:46:01,169:INFO:                umap: Not installed
2023-11-03 14:46:01,169:INFO:     ydata_profiling: Not installed
2023-11-03 14:46:01,169:INFO:  explainerdashboard: Not installed
2023-11-03 14:46:01,169:INFO:             autoviz: Not installed
2023-11-03 14:46:01,169:INFO:           fairlearn: Not installed
2023-11-03 14:46:01,169:INFO:          deepchecks: Not installed
2023-11-03 14:46:01,169:INFO:             xgboost: 2.0.0
2023-11-03 14:46:01,169:INFO:            catboost: Not installed
2023-11-03 14:46:01,169:INFO:              kmodes: Not installed
2023-11-03 14:46:01,169:INFO:             mlxtend: Not installed
2023-11-03 14:46:01,169:INFO:       statsforecast: Not installed
2023-11-03 14:46:01,169:INFO:        tune_sklearn: Not installed
2023-11-03 14:46:01,169:INFO:                 ray: Not installed
2023-11-03 14:46:01,169:INFO:            hyperopt: 0.2.7
2023-11-03 14:46:01,169:INFO:              optuna: 3.4.0
2023-11-03 14:46:01,169:INFO:               skopt: Not installed
2023-11-03 14:46:01,170:INFO:              mlflow: Not installed
2023-11-03 14:46:01,170:INFO:              gradio: Not installed
2023-11-03 14:46:01,170:INFO:             fastapi: Not installed
2023-11-03 14:46:01,170:INFO:             uvicorn: Not installed
2023-11-03 14:46:01,170:INFO:              m2cgen: Not installed
2023-11-03 14:46:01,170:INFO:           evidently: Not installed
2023-11-03 14:46:01,170:INFO:               fugue: Not installed
2023-11-03 14:46:01,170:INFO:           streamlit: Not installed
2023-11-03 14:46:01,170:INFO:             prophet: Not installed
2023-11-03 14:46:01,170:INFO:None
2023-11-03 14:46:01,170:INFO:Set up data.
2023-11-03 14:46:01,251:INFO:Set up folding strategy.
2023-11-03 14:46:01,251:INFO:Set up train/test split.
2023-11-03 14:46:01,314:INFO:Set up index.
2023-11-03 14:46:01,317:INFO:Assigning column types.
2023-11-03 14:46:01,352:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-03 14:46:01,352:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,360:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,371:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,511:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,598:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,599:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:01,605:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:01,606:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,614:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,622:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,729:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,797:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,798:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:01,802:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:01,802:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-03 14:46:01,810:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,817:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,912:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,996:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,997:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:02,005:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:02,016:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,028:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,231:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,344:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,346:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:02,355:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:02,357:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-03 14:46:02,405:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,601:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,707:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,708:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:02,713:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:02,730:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,033:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,264:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,265:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:03,274:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:03,276:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-03 14:46:03,501:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,614:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,616:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:03,622:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:03,883:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:04,056:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:04,058:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:04,066:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:04,067:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-03 14:46:04,507:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:04,626:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:04,632:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:04,794:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:04,950:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:04,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:04,968:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-03 14:46:06,127:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:06,148:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:06,753:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:06,764:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:06,775:INFO:Preparing preprocessing pipeline...
2023-11-03 14:46:06,775:INFO:Set up simple imputation.
2023-11-03 14:46:06,783:INFO:Set up column name cleaning.
2023-11-03 14:46:07,034:INFO:Finished creating preprocessing pipeline.
2023-11-03 14:46:07,058:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['is_day:idx', 'is_in_shadow:idx',
                                             'absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'di...
                                             'pressure_50m:hPa', 'prob_rime:p',
                                             'rain_water:kgm2',
                                             'relative_humidity_1000hPa:p',
                                             'sfc_pressure:hPa',
                                             'snow_depth:cm',
                                             'snow_melt_10min:mm',
                                             'snow_water:kgm2', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-03 14:46:07,059:INFO:Creating final display dataframe.
2023-11-03 14:46:08,333:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    pv_measurement
2                   Target type        Regression
3           Original data shape       (34060, 60)
4        Transformed data shape       (34060, 60)
5   Transformed train set shape       (23842, 60)
6    Transformed test set shape       (10218, 60)
7              Numeric features                59
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              43bf
2023-11-03 14:46:08,876:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:08,885:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:09,531:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:09,550:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:09,554:INFO:setup() successfully completed in 10.45s...............
2023-11-03 14:46:09,563:INFO:PyCaret RegressionExperiment
2023-11-03 14:46:09,564:INFO:Logging name: reg-default-name
2023-11-03 14:46:09,566:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-03 14:46:09,566:INFO:version 3.1.0
2023-11-03 14:46:09,566:INFO:Initializing setup()
2023-11-03 14:46:09,566:INFO:self.USI: b087
2023-11-03 14:46:09,567:INFO:self._variable_keys: {'n_jobs_param', 'X_train', 'idx', 'X', 'exp_id', 'gpu_param', 'gpu_n_jobs_param', 'fold_generator', 'y_train', 'target_param', 'fold_groups_param', 'data', 'X_test', '_available_plots', 'log_plots_param', 'fold_shuffle_param', 'USI', '_ml_usecase', 'y', 'pipeline', 'y_test', 'exp_name_log', 'transform_target_param', 'seed', 'logging_param', 'memory', 'html_param'}
2023-11-03 14:46:09,567:INFO:Checking environment
2023-11-03 14:46:09,567:INFO:python_version: 3.9.6
2023-11-03 14:46:09,567:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-03 14:46:09,568:INFO:machine: x86_64
2023-11-03 14:46:09,568:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:46:09,568:INFO:Memory: svmem(total=8589934592, available=2770939904, percent=67.7, used=4941959168, free=392626176, active=2383290368, inactive=2355232768, wired=2558668800)
2023-11-03 14:46:09,568:INFO:Physical Core: 4
2023-11-03 14:46:09,568:INFO:Logical Core: 8
2023-11-03 14:46:09,569:INFO:Checking libraries
2023-11-03 14:46:09,569:INFO:System:
2023-11-03 14:46:09,569:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-03 14:46:09,569:INFO:executable: /usr/local/bin/python3.9
2023-11-03 14:46:09,569:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:46:09,570:INFO:PyCaret required dependencies:
2023-11-03 14:46:09,570:INFO:                 pip: 23.3.1
2023-11-03 14:46:09,571:INFO:          setuptools: 56.0.0
2023-11-03 14:46:09,574:INFO:             pycaret: 3.1.0
2023-11-03 14:46:09,574:INFO:             IPython: 7.28.0
2023-11-03 14:46:09,574:INFO:          ipywidgets: 8.1.1
2023-11-03 14:46:09,574:INFO:                tqdm: 4.66.1
2023-11-03 14:46:09,574:INFO:               numpy: 1.23.5
2023-11-03 14:46:09,575:INFO:              pandas: 1.5.3
2023-11-03 14:46:09,575:INFO:              jinja2: 3.0.1
2023-11-03 14:46:09,575:INFO:               scipy: 1.10.1
2023-11-03 14:46:09,575:INFO:              joblib: 1.3.2
2023-11-03 14:46:09,575:INFO:             sklearn: 1.1.3
2023-11-03 14:46:09,575:INFO:                pyod: 1.1.1
2023-11-03 14:46:09,576:INFO:            imblearn: 0.11.0
2023-11-03 14:46:09,576:INFO:   category_encoders: 2.6.3
2023-11-03 14:46:09,576:INFO:            lightgbm: 4.1.0
2023-11-03 14:46:09,576:INFO:               numba: 0.58.1
2023-11-03 14:46:09,576:INFO:            requests: 2.31.0
2023-11-03 14:46:09,577:INFO:          matplotlib: 3.4.2
2023-11-03 14:46:09,577:INFO:          scikitplot: 0.3.7
2023-11-03 14:46:09,577:INFO:         yellowbrick: 1.5
2023-11-03 14:46:09,577:INFO:              plotly: 5.18.0
2023-11-03 14:46:09,577:INFO:    plotly-resampler: Not installed
2023-11-03 14:46:09,578:INFO:             kaleido: 0.2.1
2023-11-03 14:46:09,578:INFO:           schemdraw: 0.15
2023-11-03 14:46:09,578:INFO:         statsmodels: 0.14.0
2023-11-03 14:46:09,578:INFO:              sktime: 0.21.1
2023-11-03 14:46:09,582:INFO:               tbats: 1.1.3
2023-11-03 14:46:09,582:INFO:            pmdarima: 2.0.4
2023-11-03 14:46:09,582:INFO:              psutil: 5.9.6
2023-11-03 14:46:09,583:INFO:          markupsafe: 2.1.3
2023-11-03 14:46:09,583:INFO:             pickle5: Not installed
2023-11-03 14:46:09,583:INFO:         cloudpickle: 2.2.1
2023-11-03 14:46:09,584:INFO:         deprecation: 2.1.0
2023-11-03 14:46:09,584:INFO:              xxhash: 3.4.1
2023-11-03 14:46:09,584:INFO:           wurlitzer: 3.0.3
2023-11-03 14:46:09,584:INFO:PyCaret optional dependencies:
2023-11-03 14:46:09,585:INFO:                shap: Not installed
2023-11-03 14:46:09,585:INFO:           interpret: Not installed
2023-11-03 14:46:09,585:INFO:                umap: Not installed
2023-11-03 14:46:09,585:INFO:     ydata_profiling: Not installed
2023-11-03 14:46:09,585:INFO:  explainerdashboard: Not installed
2023-11-03 14:46:09,585:INFO:             autoviz: Not installed
2023-11-03 14:46:09,586:INFO:           fairlearn: Not installed
2023-11-03 14:46:09,586:INFO:          deepchecks: Not installed
2023-11-03 14:46:09,586:INFO:             xgboost: 2.0.0
2023-11-03 14:46:09,586:INFO:            catboost: Not installed
2023-11-03 14:46:09,586:INFO:              kmodes: Not installed
2023-11-03 14:46:09,586:INFO:             mlxtend: Not installed
2023-11-03 14:46:09,586:INFO:       statsforecast: Not installed
2023-11-03 14:46:09,587:INFO:        tune_sklearn: Not installed
2023-11-03 14:46:09,587:INFO:                 ray: Not installed
2023-11-03 14:46:09,587:INFO:            hyperopt: 0.2.7
2023-11-03 14:46:09,588:INFO:              optuna: 3.4.0
2023-11-03 14:46:09,588:INFO:               skopt: Not installed
2023-11-03 14:46:09,588:INFO:              mlflow: Not installed
2023-11-03 14:46:09,588:INFO:              gradio: Not installed
2023-11-03 14:46:09,589:INFO:             fastapi: Not installed
2023-11-03 14:46:09,589:INFO:             uvicorn: Not installed
2023-11-03 14:46:09,589:INFO:              m2cgen: Not installed
2023-11-03 14:46:09,589:INFO:           evidently: Not installed
2023-11-03 14:46:09,590:INFO:               fugue: Not installed
2023-11-03 14:46:09,590:INFO:           streamlit: Not installed
2023-11-03 14:46:09,590:INFO:             prophet: Not installed
2023-11-03 14:46:09,590:INFO:None
2023-11-03 14:46:09,590:INFO:Set up data.
2023-11-03 14:46:09,907:INFO:Set up folding strategy.
2023-11-03 14:46:09,908:INFO:Set up train/test split.
2023-11-03 14:46:10,100:INFO:Set up index.
2023-11-03 14:46:10,106:INFO:Assigning column types.
2023-11-03 14:46:10,205:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-03 14:46:10,206:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,246:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,274:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,568:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,738:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,740:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:10,754:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:10,755:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,770:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,782:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,067:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,255:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,258:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:11,269:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:11,270:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-03 14:46:11,290:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,311:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,559:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,747:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,750:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:11,758:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:11,774:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,795:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:12,103:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:12,347:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:12,351:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:12,359:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:12,361:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-03 14:46:12,406:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,045:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,195:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,199:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:13,208:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:13,234:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,460:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,549:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,550:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:13,554:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:13,555:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-03 14:46:13,682:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,789:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,790:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:13,797:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:13,960:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:14,063:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:14,064:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:14,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:14,069:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-03 14:46:14,213:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:14,312:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:14,317:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:14,508:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:14,626:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:14,638:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:14,640:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-03 14:46:15,192:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:15,206:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:15,975:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:15,988:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:15,991:INFO:Preparing preprocessing pipeline...
2023-11-03 14:46:15,991:INFO:Set up simple imputation.
2023-11-03 14:46:15,997:INFO:Set up column name cleaning.
2023-11-03 14:46:16,209:INFO:Finished creating preprocessing pipeline.
2023-11-03 14:46:16,230:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['is_day:idx', 'is_in_shadow:idx',
                                             'absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'di...
                                             'pressure_50m:hPa', 'prob_rime:p',
                                             'rain_water:kgm2',
                                             'relative_humidity_1000hPa:p',
                                             'sfc_pressure:hPa',
                                             'snow_depth:cm',
                                             'snow_melt_10min:mm',
                                             'snow_water:kgm2', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-03 14:46:16,230:INFO:Creating final display dataframe.
2023-11-03 14:46:17,622:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    pv_measurement
2                   Target type        Regression
3           Original data shape       (29596, 60)
4        Transformed data shape       (29596, 60)
5   Transformed train set shape       (20717, 60)
6    Transformed test set shape        (8879, 60)
7              Numeric features                59
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              b087
2023-11-03 14:46:18,001:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:18,008:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:18,362:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:18,370:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:18,372:INFO:setup() successfully completed in 8.82s...............
2023-11-03 14:46:18,377:INFO:PyCaret RegressionExperiment
2023-11-03 14:46:18,378:INFO:Logging name: reg-default-name
2023-11-03 14:46:18,378:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-03 14:46:18,378:INFO:version 3.1.0
2023-11-03 14:46:18,378:INFO:Initializing setup()
2023-11-03 14:46:18,378:INFO:self.USI: 9015
2023-11-03 14:46:18,378:INFO:self._variable_keys: {'n_jobs_param', 'X_train', 'idx', 'X', 'exp_id', 'gpu_param', 'gpu_n_jobs_param', 'fold_generator', 'y_train', 'target_param', 'fold_groups_param', 'data', 'X_test', '_available_plots', 'log_plots_param', 'fold_shuffle_param', 'USI', '_ml_usecase', 'y', 'pipeline', 'y_test', 'exp_name_log', 'transform_target_param', 'seed', 'logging_param', 'memory', 'html_param'}
2023-11-03 14:46:18,378:INFO:Checking environment
2023-11-03 14:46:18,378:INFO:python_version: 3.9.6
2023-11-03 14:46:18,379:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-03 14:46:18,379:INFO:machine: x86_64
2023-11-03 14:46:18,379:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:46:18,379:INFO:Memory: svmem(total=8589934592, available=2559008768, percent=70.2, used=5082755072, free=82395136, active=2479779840, inactive=2437382144, wired=2602975232)
2023-11-03 14:46:18,379:INFO:Physical Core: 4
2023-11-03 14:46:18,379:INFO:Logical Core: 8
2023-11-03 14:46:18,379:INFO:Checking libraries
2023-11-03 14:46:18,379:INFO:System:
2023-11-03 14:46:18,379:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-03 14:46:18,380:INFO:executable: /usr/local/bin/python3.9
2023-11-03 14:46:18,380:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:46:18,380:INFO:PyCaret required dependencies:
2023-11-03 14:46:18,380:INFO:                 pip: 23.3.1
2023-11-03 14:46:18,380:INFO:          setuptools: 56.0.0
2023-11-03 14:46:18,380:INFO:             pycaret: 3.1.0
2023-11-03 14:46:18,380:INFO:             IPython: 7.28.0
2023-11-03 14:46:18,380:INFO:          ipywidgets: 8.1.1
2023-11-03 14:46:18,381:INFO:                tqdm: 4.66.1
2023-11-03 14:46:18,381:INFO:               numpy: 1.23.5
2023-11-03 14:46:18,381:INFO:              pandas: 1.5.3
2023-11-03 14:46:18,381:INFO:              jinja2: 3.0.1
2023-11-03 14:46:18,381:INFO:               scipy: 1.10.1
2023-11-03 14:46:18,381:INFO:              joblib: 1.3.2
2023-11-03 14:46:18,381:INFO:             sklearn: 1.1.3
2023-11-03 14:46:18,382:INFO:                pyod: 1.1.1
2023-11-03 14:46:18,382:INFO:            imblearn: 0.11.0
2023-11-03 14:46:18,382:INFO:   category_encoders: 2.6.3
2023-11-03 14:46:18,382:INFO:            lightgbm: 4.1.0
2023-11-03 14:46:18,382:INFO:               numba: 0.58.1
2023-11-03 14:46:18,382:INFO:            requests: 2.31.0
2023-11-03 14:46:18,383:INFO:          matplotlib: 3.4.2
2023-11-03 14:46:18,383:INFO:          scikitplot: 0.3.7
2023-11-03 14:46:18,383:INFO:         yellowbrick: 1.5
2023-11-03 14:46:18,383:INFO:              plotly: 5.18.0
2023-11-03 14:46:18,383:INFO:    plotly-resampler: Not installed
2023-11-03 14:46:18,383:INFO:             kaleido: 0.2.1
2023-11-03 14:46:18,383:INFO:           schemdraw: 0.15
2023-11-03 14:46:18,383:INFO:         statsmodels: 0.14.0
2023-11-03 14:46:18,383:INFO:              sktime: 0.21.1
2023-11-03 14:46:18,383:INFO:               tbats: 1.1.3
2023-11-03 14:46:18,383:INFO:            pmdarima: 2.0.4
2023-11-03 14:46:18,384:INFO:              psutil: 5.9.6
2023-11-03 14:46:18,384:INFO:          markupsafe: 2.1.3
2023-11-03 14:46:18,384:INFO:             pickle5: Not installed
2023-11-03 14:46:18,384:INFO:         cloudpickle: 2.2.1
2023-11-03 14:46:18,384:INFO:         deprecation: 2.1.0
2023-11-03 14:46:18,384:INFO:              xxhash: 3.4.1
2023-11-03 14:46:18,385:INFO:           wurlitzer: 3.0.3
2023-11-03 14:46:18,385:INFO:PyCaret optional dependencies:
2023-11-03 14:46:18,385:INFO:                shap: Not installed
2023-11-03 14:46:18,385:INFO:           interpret: Not installed
2023-11-03 14:46:18,386:INFO:                umap: Not installed
2023-11-03 14:46:18,386:INFO:     ydata_profiling: Not installed
2023-11-03 14:46:18,386:INFO:  explainerdashboard: Not installed
2023-11-03 14:46:18,386:INFO:             autoviz: Not installed
2023-11-03 14:46:18,386:INFO:           fairlearn: Not installed
2023-11-03 14:46:18,386:INFO:          deepchecks: Not installed
2023-11-03 14:46:18,387:INFO:             xgboost: 2.0.0
2023-11-03 14:46:18,387:INFO:            catboost: Not installed
2023-11-03 14:46:18,387:INFO:              kmodes: Not installed
2023-11-03 14:46:18,387:INFO:             mlxtend: Not installed
2023-11-03 14:46:18,387:INFO:       statsforecast: Not installed
2023-11-03 14:46:18,388:INFO:        tune_sklearn: Not installed
2023-11-03 14:46:18,388:INFO:                 ray: Not installed
2023-11-03 14:46:18,388:INFO:            hyperopt: 0.2.7
2023-11-03 14:46:18,388:INFO:              optuna: 3.4.0
2023-11-03 14:46:18,388:INFO:               skopt: Not installed
2023-11-03 14:46:18,389:INFO:              mlflow: Not installed
2023-11-03 14:46:18,389:INFO:              gradio: Not installed
2023-11-03 14:46:18,389:INFO:             fastapi: Not installed
2023-11-03 14:46:18,390:INFO:             uvicorn: Not installed
2023-11-03 14:46:18,390:INFO:              m2cgen: Not installed
2023-11-03 14:46:18,390:INFO:           evidently: Not installed
2023-11-03 14:46:18,390:INFO:               fugue: Not installed
2023-11-03 14:46:18,390:INFO:           streamlit: Not installed
2023-11-03 14:46:18,390:INFO:             prophet: Not installed
2023-11-03 14:46:18,391:INFO:None
2023-11-03 14:46:18,391:INFO:Set up data.
2023-11-03 14:46:18,494:INFO:Set up folding strategy.
2023-11-03 14:46:18,495:INFO:Set up train/test split.
2023-11-03 14:46:18,547:INFO:Set up index.
2023-11-03 14:46:18,560:INFO:Assigning column types.
2023-11-03 14:46:18,583:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-03 14:46:18,583:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,601:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,612:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,818:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,972:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,973:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:18,983:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:18,983:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,002:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,016:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,291:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,457:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,460:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:19,468:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:19,469:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-03 14:46:19,484:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,501:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,803:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,035:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,037:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:20,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:20,063:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,078:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,268:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,373:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,374:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:20,380:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:20,380:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-03 14:46:20,404:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,536:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,623:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,625:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:20,632:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:20,660:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,825:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,930:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,931:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:20,936:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:20,937:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-03 14:46:21,137:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:21,301:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:21,303:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:21,315:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:21,883:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:22,063:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:22,068:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:22,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:22,087:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-03 14:46:22,276:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:22,354:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:22,358:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:22,496:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:22,588:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:22,604:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:22,605:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-03 14:46:22,860:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:22,864:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:23,079:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:23,083:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:23,084:INFO:Preparing preprocessing pipeline...
2023-11-03 14:46:23,084:INFO:Set up simple imputation.
2023-11-03 14:46:23,087:INFO:Set up column name cleaning.
2023-11-03 14:46:23,176:INFO:Finished creating preprocessing pipeline.
2023-11-03 14:46:23,184:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['is_day:idx', 'is_in_shadow:idx',
                                             'absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'di...
                                             'pressure_50m:hPa', 'prob_rime:p',
                                             'rain_water:kgm2',
                                             'relative_humidity_1000hPa:p',
                                             'sfc_pressure:hPa',
                                             'snow_depth:cm',
                                             'snow_melt_10min:mm',
                                             'snow_water:kgm2', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-03 14:46:23,184:INFO:Creating final display dataframe.
2023-11-03 14:46:23,524:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    pv_measurement
2                   Target type        Regression
3           Original data shape       (26028, 60)
4        Transformed data shape       (26028, 60)
5   Transformed train set shape       (18219, 60)
6    Transformed test set shape        (7809, 60)
7              Numeric features                59
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              9015
2023-11-03 14:46:23,712:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:23,716:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:23,882:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:23,885:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:23,886:INFO:setup() successfully completed in 5.51s...............
2023-11-03 14:46:23,899:INFO:Initializing compare_models()
2023-11-03 14:46:23,900:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-11-03 14:46:23,900:INFO:Checking exceptions
2023-11-03 14:46:23,913:INFO:Preparing display monitor
2023-11-03 14:46:24,035:INFO:Initializing Linear Regression
2023-11-03 14:46:24,035:INFO:Total runtime is 9.26653544108073e-06 minutes
2023-11-03 14:46:24,044:INFO:SubProcess create_model() called ==================================
2023-11-03 14:46:24,045:INFO:Initializing create_model()
2023-11-03 14:46:24,046:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:46:24,046:INFO:Checking exceptions
2023-11-03 14:46:24,047:INFO:Importing libraries
2023-11-03 14:46:24,047:INFO:Copying training dataset
2023-11-03 14:46:24,090:INFO:Defining folds
2023-11-03 14:46:24,090:INFO:Declaring metric variables
2023-11-03 14:46:24,095:INFO:Importing untrained model
2023-11-03 14:46:24,101:INFO:Linear Regression Imported successfully
2023-11-03 14:46:24,113:INFO:Starting cross validation
2023-11-03 14:46:24,126:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:46:38,474:INFO:Calculating mean and std
2023-11-03 14:46:38,481:INFO:Creating metrics dataframe
2023-11-03 14:46:38,492:INFO:Uploading results into container
2023-11-03 14:46:38,494:INFO:Uploading model into container now
2023-11-03 14:46:38,495:INFO:_master_model_container: 1
2023-11-03 14:46:38,496:INFO:_display_container: 2
2023-11-03 14:46:38,497:INFO:LinearRegression(n_jobs=-1)
2023-11-03 14:46:38,498:INFO:create_model() successfully completed......................................
2023-11-03 14:46:39,130:INFO:SubProcess create_model() end ==================================
2023-11-03 14:46:39,131:INFO:Creating metrics dataframe
2023-11-03 14:46:39,147:INFO:Initializing Lasso Regression
2023-11-03 14:46:39,147:INFO:Total runtime is 0.25186730225880943 minutes
2023-11-03 14:46:39,151:INFO:SubProcess create_model() called ==================================
2023-11-03 14:46:39,152:INFO:Initializing create_model()
2023-11-03 14:46:39,152:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:46:39,152:INFO:Checking exceptions
2023-11-03 14:46:39,153:INFO:Importing libraries
2023-11-03 14:46:39,153:INFO:Copying training dataset
2023-11-03 14:46:39,186:INFO:Defining folds
2023-11-03 14:46:39,186:INFO:Declaring metric variables
2023-11-03 14:46:39,192:INFO:Importing untrained model
2023-11-03 14:46:39,201:INFO:Lasso Regression Imported successfully
2023-11-03 14:46:39,212:INFO:Starting cross validation
2023-11-03 14:46:39,214:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:46:48,377:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+09, tolerance: 2.932e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:48,603:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.473e+09, tolerance: 2.949e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:48,707:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.499e+09, tolerance: 2.910e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:48,727:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.496e+09, tolerance: 2.944e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:48,967:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+09, tolerance: 2.936e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:49,135:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.447e+09, tolerance: 2.941e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:49,194:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.432e+09, tolerance: 2.932e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:49,508:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+09, tolerance: 2.936e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:52,871:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+09, tolerance: 2.925e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:52,896:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.441e+09, tolerance: 2.935e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:53,053:INFO:Calculating mean and std
2023-11-03 14:46:53,058:INFO:Creating metrics dataframe
2023-11-03 14:46:53,070:INFO:Uploading results into container
2023-11-03 14:46:53,072:INFO:Uploading model into container now
2023-11-03 14:46:53,074:INFO:_master_model_container: 2
2023-11-03 14:46:53,074:INFO:_display_container: 2
2023-11-03 14:46:53,075:INFO:Lasso(random_state=123)
2023-11-03 14:46:53,075:INFO:create_model() successfully completed......................................
2023-11-03 14:46:53,362:INFO:SubProcess create_model() end ==================================
2023-11-03 14:46:53,362:INFO:Creating metrics dataframe
2023-11-03 14:46:53,390:INFO:Initializing Ridge Regression
2023-11-03 14:46:53,390:INFO:Total runtime is 0.4892539699872335 minutes
2023-11-03 14:46:53,399:INFO:SubProcess create_model() called ==================================
2023-11-03 14:46:53,400:INFO:Initializing create_model()
2023-11-03 14:46:53,401:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:46:53,402:INFO:Checking exceptions
2023-11-03 14:46:53,402:INFO:Importing libraries
2023-11-03 14:46:53,402:INFO:Copying training dataset
2023-11-03 14:46:53,455:INFO:Defining folds
2023-11-03 14:46:53,456:INFO:Declaring metric variables
2023-11-03 14:46:53,464:INFO:Importing untrained model
2023-11-03 14:46:53,478:INFO:Ridge Regression Imported successfully
2023-11-03 14:46:53,499:INFO:Starting cross validation
2023-11-03 14:46:53,505:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:46:54,085:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.14122e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,169:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.15068e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,209:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.13958e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,358:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.15498e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,457:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.15274e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,743:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.1538e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:55,185:INFO:Calculating mean and std
2023-11-03 14:46:55,193:INFO:Creating metrics dataframe
2023-11-03 14:46:55,215:INFO:Uploading results into container
2023-11-03 14:46:55,217:INFO:Uploading model into container now
2023-11-03 14:46:55,218:INFO:_master_model_container: 3
2023-11-03 14:46:55,219:INFO:_display_container: 2
2023-11-03 14:46:55,220:INFO:Ridge(random_state=123)
2023-11-03 14:46:55,221:INFO:create_model() successfully completed......................................
2023-11-03 14:46:55,569:INFO:SubProcess create_model() end ==================================
2023-11-03 14:46:55,570:INFO:Creating metrics dataframe
2023-11-03 14:46:55,626:INFO:Initializing Elastic Net
2023-11-03 14:46:55,627:INFO:Total runtime is 0.5265276869138081 minutes
2023-11-03 14:46:55,652:INFO:SubProcess create_model() called ==================================
2023-11-03 14:46:55,654:INFO:Initializing create_model()
2023-11-03 14:46:55,654:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:46:55,655:INFO:Checking exceptions
2023-11-03 14:46:55,656:INFO:Importing libraries
2023-11-03 14:46:55,656:INFO:Copying training dataset
2023-11-03 14:46:55,744:INFO:Defining folds
2023-11-03 14:46:55,745:INFO:Declaring metric variables
2023-11-03 14:46:55,765:INFO:Importing untrained model
2023-11-03 14:46:55,784:INFO:Elastic Net Imported successfully
2023-11-03 14:46:55,814:INFO:Starting cross validation
2023-11-03 14:46:55,821:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:08,420:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.509e+09, tolerance: 2.932e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,495:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.510e+09, tolerance: 2.949e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,587:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.505e+09, tolerance: 2.936e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,587:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+09, tolerance: 2.944e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,641:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.535e+09, tolerance: 2.910e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,669:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+09, tolerance: 2.941e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,715:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+09, tolerance: 2.932e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,802:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.527e+09, tolerance: 2.936e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:12,584:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+09, tolerance: 2.935e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:12,619:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.520e+09, tolerance: 2.925e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:12,773:INFO:Calculating mean and std
2023-11-03 14:47:12,776:INFO:Creating metrics dataframe
2023-11-03 14:47:12,781:INFO:Uploading results into container
2023-11-03 14:47:12,783:INFO:Uploading model into container now
2023-11-03 14:47:12,784:INFO:_master_model_container: 4
2023-11-03 14:47:12,784:INFO:_display_container: 2
2023-11-03 14:47:12,785:INFO:ElasticNet(random_state=123)
2023-11-03 14:47:12,785:INFO:create_model() successfully completed......................................
2023-11-03 14:47:13,001:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:13,001:INFO:Creating metrics dataframe
2023-11-03 14:47:13,018:INFO:Initializing Least Angle Regression
2023-11-03 14:47:13,018:INFO:Total runtime is 0.8163832346598308 minutes
2023-11-03 14:47:13,024:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:13,025:INFO:Initializing create_model()
2023-11-03 14:47:13,025:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:13,025:INFO:Checking exceptions
2023-11-03 14:47:13,025:INFO:Importing libraries
2023-11-03 14:47:13,026:INFO:Copying training dataset
2023-11-03 14:47:13,062:INFO:Defining folds
2023-11-03 14:47:13,062:INFO:Declaring metric variables
2023-11-03 14:47:13,068:INFO:Importing untrained model
2023-11-03 14:47:13,076:INFO:Least Angle Regression Imported successfully
2023-11-03 14:47:13,091:INFO:Starting cross validation
2023-11-03 14:47:13,093:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:13,343:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,370:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,389:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,528:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.614e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,528:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=5.469e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,537:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.421e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,538:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.307e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,542:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.080e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,543:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.894e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,543:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.829e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,544:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=3.910e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,546:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.776e-01, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,551:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,633:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=4.008e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,634:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=3.844e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,660:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.424e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,661:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.821e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,661:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.730e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,666:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.863e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,667:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.683e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.599e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,668:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=9.076e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,784:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,805:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,805:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.014e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,832:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.111e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,833:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.100e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,851:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.316e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,858:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.071e+11, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,858:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.068e+11, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,859:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.372e+10, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,860:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.654e+10, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,899:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,913:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=6.602e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,914:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=3.155e+03, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,914:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.715e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,921:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=2.277e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,924:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=5.724e-01, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,936:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,981:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.400e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,982:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.363e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,988:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.100e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,989:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=6.063e-04, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,991:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.996e-04, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,030:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.741e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,030:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.247e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,034:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.716e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,060:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:14,076:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:14,106:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.921e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,108:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.835e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,108:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.098e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,109:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.887e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,122:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.604e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,288:INFO:Calculating mean and std
2023-11-03 14:47:14,293:INFO:Creating metrics dataframe
2023-11-03 14:47:14,301:INFO:Uploading results into container
2023-11-03 14:47:14,302:INFO:Uploading model into container now
2023-11-03 14:47:14,303:INFO:_master_model_container: 5
2023-11-03 14:47:14,303:INFO:_display_container: 2
2023-11-03 14:47:14,305:INFO:Lars(random_state=123)
2023-11-03 14:47:14,305:INFO:create_model() successfully completed......................................
2023-11-03 14:47:14,538:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:14,538:INFO:Creating metrics dataframe
2023-11-03 14:47:14,566:INFO:Initializing Lasso Least Angle Regression
2023-11-03 14:47:14,566:INFO:Total runtime is 0.8421889146169027 minutes
2023-11-03 14:47:14,576:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:14,577:INFO:Initializing create_model()
2023-11-03 14:47:14,577:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:14,577:INFO:Checking exceptions
2023-11-03 14:47:14,578:INFO:Importing libraries
2023-11-03 14:47:14,579:INFO:Copying training dataset
2023-11-03 14:47:14,650:INFO:Defining folds
2023-11-03 14:47:14,650:INFO:Declaring metric variables
2023-11-03 14:47:14,663:INFO:Importing untrained model
2023-11-03 14:47:14,675:INFO:Lasso Least Angle Regression Imported successfully
2023-11-03 14:47:14,702:INFO:Starting cross validation
2023-11-03 14:47:14,707:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:15,063:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,138:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,161:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,299:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,327:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,347:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,362:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,372:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,593:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,612:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,775:INFO:Calculating mean and std
2023-11-03 14:47:15,779:INFO:Creating metrics dataframe
2023-11-03 14:47:15,792:INFO:Uploading results into container
2023-11-03 14:47:15,794:INFO:Uploading model into container now
2023-11-03 14:47:15,795:INFO:_master_model_container: 6
2023-11-03 14:47:15,795:INFO:_display_container: 2
2023-11-03 14:47:15,796:INFO:LassoLars(random_state=123)
2023-11-03 14:47:15,797:INFO:create_model() successfully completed......................................
2023-11-03 14:47:16,006:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:16,007:INFO:Creating metrics dataframe
2023-11-03 14:47:16,023:INFO:Initializing Orthogonal Matching Pursuit
2023-11-03 14:47:16,023:INFO:Total runtime is 0.8664672176043193 minutes
2023-11-03 14:47:16,028:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:16,028:INFO:Initializing create_model()
2023-11-03 14:47:16,029:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:16,029:INFO:Checking exceptions
2023-11-03 14:47:16,029:INFO:Importing libraries
2023-11-03 14:47:16,029:INFO:Copying training dataset
2023-11-03 14:47:16,061:INFO:Defining folds
2023-11-03 14:47:16,062:INFO:Declaring metric variables
2023-11-03 14:47:16,067:INFO:Importing untrained model
2023-11-03 14:47:16,075:INFO:Orthogonal Matching Pursuit Imported successfully
2023-11-03 14:47:16,087:INFO:Starting cross validation
2023-11-03 14:47:16,091:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:16,274:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,279:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,353:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,406:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,457:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,503:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,563:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,589:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,685:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,702:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,884:INFO:Calculating mean and std
2023-11-03 14:47:16,890:INFO:Creating metrics dataframe
2023-11-03 14:47:16,898:INFO:Uploading results into container
2023-11-03 14:47:16,900:INFO:Uploading model into container now
2023-11-03 14:47:16,901:INFO:_master_model_container: 7
2023-11-03 14:47:16,902:INFO:_display_container: 2
2023-11-03 14:47:16,902:INFO:OrthogonalMatchingPursuit()
2023-11-03 14:47:16,903:INFO:create_model() successfully completed......................................
2023-11-03 14:47:17,094:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:17,095:INFO:Creating metrics dataframe
2023-11-03 14:47:17,108:INFO:Initializing Bayesian Ridge
2023-11-03 14:47:17,108:INFO:Total runtime is 0.8845586021741232 minutes
2023-11-03 14:47:17,112:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:17,113:INFO:Initializing create_model()
2023-11-03 14:47:17,113:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:17,113:INFO:Checking exceptions
2023-11-03 14:47:17,113:INFO:Importing libraries
2023-11-03 14:47:17,113:INFO:Copying training dataset
2023-11-03 14:47:17,144:INFO:Defining folds
2023-11-03 14:47:17,144:INFO:Declaring metric variables
2023-11-03 14:47:17,150:INFO:Importing untrained model
2023-11-03 14:47:17,157:INFO:Bayesian Ridge Imported successfully
2023-11-03 14:47:17,169:INFO:Starting cross validation
2023-11-03 14:47:17,171:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:19,002:INFO:Calculating mean and std
2023-11-03 14:47:19,008:INFO:Creating metrics dataframe
2023-11-03 14:47:19,016:INFO:Uploading results into container
2023-11-03 14:47:19,018:INFO:Uploading model into container now
2023-11-03 14:47:19,020:INFO:_master_model_container: 8
2023-11-03 14:47:19,020:INFO:_display_container: 2
2023-11-03 14:47:19,021:INFO:BayesianRidge()
2023-11-03 14:47:19,022:INFO:create_model() successfully completed......................................
2023-11-03 14:47:19,206:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:19,206:INFO:Creating metrics dataframe
2023-11-03 14:47:19,221:INFO:Initializing Passive Aggressive Regressor
2023-11-03 14:47:19,221:INFO:Total runtime is 0.9197753985722861 minutes
2023-11-03 14:47:19,226:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:19,227:INFO:Initializing create_model()
2023-11-03 14:47:19,228:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:19,228:INFO:Checking exceptions
2023-11-03 14:47:19,228:INFO:Importing libraries
2023-11-03 14:47:19,229:INFO:Copying training dataset
2023-11-03 14:47:19,324:INFO:Defining folds
2023-11-03 14:47:19,325:INFO:Declaring metric variables
2023-11-03 14:47:19,349:INFO:Importing untrained model
2023-11-03 14:47:19,358:INFO:Passive Aggressive Regressor Imported successfully
2023-11-03 14:47:19,397:INFO:Starting cross validation
2023-11-03 14:47:19,399:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:21,235:INFO:Calculating mean and std
2023-11-03 14:47:21,239:INFO:Creating metrics dataframe
2023-11-03 14:47:21,246:INFO:Uploading results into container
2023-11-03 14:47:21,247:INFO:Uploading model into container now
2023-11-03 14:47:21,249:INFO:_master_model_container: 9
2023-11-03 14:47:21,249:INFO:_display_container: 2
2023-11-03 14:47:21,250:INFO:PassiveAggressiveRegressor(random_state=123)
2023-11-03 14:47:21,251:INFO:create_model() successfully completed......................................
2023-11-03 14:47:21,504:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:21,504:INFO:Creating metrics dataframe
2023-11-03 14:47:21,537:INFO:Initializing Huber Regressor
2023-11-03 14:47:21,539:INFO:Total runtime is 0.9584062536557516 minutes
2023-11-03 14:47:21,557:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:21,558:INFO:Initializing create_model()
2023-11-03 14:47:21,559:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:21,559:INFO:Checking exceptions
2023-11-03 14:47:21,559:INFO:Importing libraries
2023-11-03 14:47:21,560:INFO:Copying training dataset
2023-11-03 14:47:21,601:INFO:Defining folds
2023-11-03 14:47:21,602:INFO:Declaring metric variables
2023-11-03 14:47:21,609:INFO:Importing untrained model
2023-11-03 14:47:21,622:INFO:Huber Regressor Imported successfully
2023-11-03 14:47:21,659:INFO:Starting cross validation
2023-11-03 14:47:21,664:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:29,613:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:29,677:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:29,765:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:29,945:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:30,009:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:30,030:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:30,161:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:30,193:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:33,193:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:33,251:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:33,410:INFO:Calculating mean and std
2023-11-03 14:47:33,419:INFO:Creating metrics dataframe
2023-11-03 14:47:33,440:INFO:Uploading results into container
2023-11-03 14:47:33,443:INFO:Uploading model into container now
2023-11-03 14:47:33,446:INFO:_master_model_container: 10
2023-11-03 14:47:33,446:INFO:_display_container: 2
2023-11-03 14:47:33,449:INFO:HuberRegressor()
2023-11-03 14:47:33,449:INFO:create_model() successfully completed......................................
2023-11-03 14:47:34,680:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:34,681:INFO:Creating metrics dataframe
2023-11-03 14:47:34,724:INFO:Initializing K Neighbors Regressor
2023-11-03 14:47:34,725:INFO:Total runtime is 1.178164565563202 minutes
2023-11-03 14:47:34,738:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:34,740:INFO:Initializing create_model()
2023-11-03 14:47:34,740:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:34,740:INFO:Checking exceptions
2023-11-03 14:47:34,740:INFO:Importing libraries
2023-11-03 14:47:34,740:INFO:Copying training dataset
2023-11-03 14:47:34,808:INFO:Defining folds
2023-11-03 14:47:34,808:INFO:Declaring metric variables
2023-11-03 14:47:34,829:INFO:Importing untrained model
2023-11-03 14:47:34,854:INFO:K Neighbors Regressor Imported successfully
2023-11-03 14:47:34,893:INFO:Starting cross validation
2023-11-03 14:47:34,898:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:40,477:INFO:Calculating mean and std
2023-11-03 14:47:40,481:INFO:Creating metrics dataframe
2023-11-03 14:47:40,491:INFO:Uploading results into container
2023-11-03 14:47:40,493:INFO:Uploading model into container now
2023-11-03 14:47:40,494:INFO:_master_model_container: 11
2023-11-03 14:47:40,494:INFO:_display_container: 2
2023-11-03 14:47:40,495:INFO:KNeighborsRegressor(n_jobs=-1)
2023-11-03 14:47:40,495:INFO:create_model() successfully completed......................................
2023-11-03 14:47:40,755:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:40,756:INFO:Creating metrics dataframe
2023-11-03 14:47:40,801:INFO:Initializing Decision Tree Regressor
2023-11-03 14:47:40,802:INFO:Total runtime is 1.2794479688008629 minutes
2023-11-03 14:47:40,817:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:40,819:INFO:Initializing create_model()
2023-11-03 14:47:40,819:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:40,819:INFO:Checking exceptions
2023-11-03 14:47:40,820:INFO:Importing libraries
2023-11-03 14:47:40,820:INFO:Copying training dataset
2023-11-03 14:47:40,879:INFO:Defining folds
2023-11-03 14:47:40,880:INFO:Declaring metric variables
2023-11-03 14:47:40,893:INFO:Importing untrained model
2023-11-03 14:47:40,903:INFO:Decision Tree Regressor Imported successfully
2023-11-03 14:47:40,921:INFO:Starting cross validation
2023-11-03 14:47:40,923:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:48,387:INFO:Calculating mean and std
2023-11-03 14:47:48,397:INFO:Creating metrics dataframe
2023-11-03 14:47:48,413:INFO:Uploading results into container
2023-11-03 14:47:48,421:INFO:Uploading model into container now
2023-11-03 14:47:48,422:INFO:_master_model_container: 12
2023-11-03 14:47:48,422:INFO:_display_container: 2
2023-11-03 14:47:48,424:INFO:DecisionTreeRegressor(random_state=123)
2023-11-03 14:47:48,424:INFO:create_model() successfully completed......................................
2023-11-03 14:47:48,911:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:48,912:INFO:Creating metrics dataframe
2023-11-03 14:47:49,004:INFO:Initializing Random Forest Regressor
2023-11-03 14:47:49,004:INFO:Total runtime is 1.4161543170611066 minutes
2023-11-03 14:47:49,032:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:49,033:INFO:Initializing create_model()
2023-11-03 14:47:49,034:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:49,034:INFO:Checking exceptions
2023-11-03 14:47:49,034:INFO:Importing libraries
2023-11-03 14:47:49,035:INFO:Copying training dataset
2023-11-03 14:47:49,114:INFO:Defining folds
2023-11-03 14:47:49,114:INFO:Declaring metric variables
2023-11-03 14:47:49,161:INFO:Importing untrained model
2023-11-03 14:47:49,186:INFO:Random Forest Regressor Imported successfully
2023-11-03 14:47:49,311:INFO:Starting cross validation
2023-11-03 14:47:49,319:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:51:15,375:INFO:Calculating mean and std
2023-11-03 14:51:15,401:INFO:Creating metrics dataframe
2023-11-03 14:51:15,438:INFO:Uploading results into container
2023-11-03 14:51:15,442:INFO:Uploading model into container now
2023-11-03 14:51:15,445:INFO:_master_model_container: 13
2023-11-03 14:51:15,445:INFO:_display_container: 2
2023-11-03 14:51:15,450:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:51:15,450:INFO:create_model() successfully completed......................................
2023-11-03 14:51:16,392:INFO:SubProcess create_model() end ==================================
2023-11-03 14:51:16,392:INFO:Creating metrics dataframe
2023-11-03 14:51:16,428:INFO:Initializing Extra Trees Regressor
2023-11-03 14:51:16,428:INFO:Total runtime is 4.873225502173106 minutes
2023-11-03 14:51:16,436:INFO:SubProcess create_model() called ==================================
2023-11-03 14:51:16,437:INFO:Initializing create_model()
2023-11-03 14:51:16,437:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:51:16,437:INFO:Checking exceptions
2023-11-03 14:51:16,438:INFO:Importing libraries
2023-11-03 14:51:16,438:INFO:Copying training dataset
2023-11-03 14:51:16,543:INFO:Defining folds
2023-11-03 14:51:16,544:INFO:Declaring metric variables
2023-11-03 14:51:16,553:INFO:Importing untrained model
2023-11-03 14:51:16,562:INFO:Extra Trees Regressor Imported successfully
2023-11-03 14:51:16,587:INFO:Starting cross validation
2023-11-03 14:51:16,593:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:52:37,481:INFO:Calculating mean and std
2023-11-03 14:52:37,497:INFO:Creating metrics dataframe
2023-11-03 14:52:37,522:INFO:Uploading results into container
2023-11-03 14:52:37,524:INFO:Uploading model into container now
2023-11-03 14:52:37,528:INFO:_master_model_container: 14
2023-11-03 14:52:37,528:INFO:_display_container: 2
2023-11-03 14:52:37,531:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:52:37,531:INFO:create_model() successfully completed......................................
2023-11-03 14:52:38,103:INFO:SubProcess create_model() end ==================================
2023-11-03 14:52:38,103:INFO:Creating metrics dataframe
2023-11-03 14:52:38,124:INFO:Initializing AdaBoost Regressor
2023-11-03 14:52:38,124:INFO:Total runtime is 6.234825702508291 minutes
2023-11-03 14:52:38,129:INFO:SubProcess create_model() called ==================================
2023-11-03 14:52:38,129:INFO:Initializing create_model()
2023-11-03 14:52:38,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:52:38,130:INFO:Checking exceptions
2023-11-03 14:52:38,130:INFO:Importing libraries
2023-11-03 14:52:38,130:INFO:Copying training dataset
2023-11-03 14:52:38,164:INFO:Defining folds
2023-11-03 14:52:38,165:INFO:Declaring metric variables
2023-11-03 14:52:38,172:INFO:Importing untrained model
2023-11-03 14:52:38,177:INFO:AdaBoost Regressor Imported successfully
2023-11-03 14:52:38,188:INFO:Starting cross validation
2023-11-03 14:52:38,190:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:53:07,730:INFO:Calculating mean and std
2023-11-03 14:53:07,736:INFO:Creating metrics dataframe
2023-11-03 14:53:07,743:INFO:Uploading results into container
2023-11-03 14:53:07,745:INFO:Uploading model into container now
2023-11-03 14:53:07,746:INFO:_master_model_container: 15
2023-11-03 14:53:07,746:INFO:_display_container: 2
2023-11-03 14:53:07,746:INFO:AdaBoostRegressor(random_state=123)
2023-11-03 14:53:07,746:INFO:create_model() successfully completed......................................
2023-11-03 14:53:07,912:INFO:SubProcess create_model() end ==================================
2023-11-03 14:53:07,913:INFO:Creating metrics dataframe
2023-11-03 14:53:07,930:INFO:Initializing Gradient Boosting Regressor
2023-11-03 14:53:07,930:INFO:Total runtime is 6.731589082876842 minutes
2023-11-03 14:53:07,935:INFO:SubProcess create_model() called ==================================
2023-11-03 14:53:07,935:INFO:Initializing create_model()
2023-11-03 14:53:07,935:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:53:07,935:INFO:Checking exceptions
2023-11-03 14:53:07,936:INFO:Importing libraries
2023-11-03 14:53:07,936:INFO:Copying training dataset
2023-11-03 14:53:07,958:INFO:Defining folds
2023-11-03 14:53:07,958:INFO:Declaring metric variables
2023-11-03 14:53:07,963:INFO:Importing untrained model
2023-11-03 14:53:07,967:INFO:Gradient Boosting Regressor Imported successfully
2023-11-03 14:53:07,974:INFO:Starting cross validation
2023-11-03 14:53:07,976:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:25,425:INFO:Calculating mean and std
2023-11-03 14:54:25,431:INFO:Creating metrics dataframe
2023-11-03 14:54:25,439:INFO:Uploading results into container
2023-11-03 14:54:25,440:INFO:Uploading model into container now
2023-11-03 14:54:25,442:INFO:_master_model_container: 16
2023-11-03 14:54:25,443:INFO:_display_container: 2
2023-11-03 14:54:25,444:INFO:GradientBoostingRegressor(random_state=123)
2023-11-03 14:54:25,444:INFO:create_model() successfully completed......................................
2023-11-03 14:54:25,617:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:25,618:INFO:Creating metrics dataframe
2023-11-03 14:54:25,631:INFO:Initializing Extreme Gradient Boosting
2023-11-03 14:54:25,632:INFO:Total runtime is 8.026613903045655 minutes
2023-11-03 14:54:25,636:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:25,636:INFO:Initializing create_model()
2023-11-03 14:54:25,636:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:25,636:INFO:Checking exceptions
2023-11-03 14:54:25,636:INFO:Importing libraries
2023-11-03 14:54:25,636:INFO:Copying training dataset
2023-11-03 14:54:25,659:INFO:Defining folds
2023-11-03 14:54:25,659:INFO:Declaring metric variables
2023-11-03 14:54:25,663:INFO:Importing untrained model
2023-11-03 14:54:25,670:INFO:Extreme Gradient Boosting Imported successfully
2023-11-03 14:54:25,678:INFO:Starting cross validation
2023-11-03 14:54:25,680:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:33,843:INFO:Calculating mean and std
2023-11-03 14:54:33,846:INFO:Creating metrics dataframe
2023-11-03 14:54:33,851:INFO:Uploading results into container
2023-11-03 14:54:33,852:INFO:Uploading model into container now
2023-11-03 14:54:33,853:INFO:_master_model_container: 17
2023-11-03 14:54:33,854:INFO:_display_container: 2
2023-11-03 14:54:33,856:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-03 14:54:33,857:INFO:create_model() successfully completed......................................
2023-11-03 14:54:34,061:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:34,061:INFO:Creating metrics dataframe
2023-11-03 14:54:34,081:INFO:Initializing Light Gradient Boosting Machine
2023-11-03 14:54:34,081:INFO:Total runtime is 8.167442119121553 minutes
2023-11-03 14:54:34,086:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:34,087:INFO:Initializing create_model()
2023-11-03 14:54:34,087:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:34,088:INFO:Checking exceptions
2023-11-03 14:54:34,088:INFO:Importing libraries
2023-11-03 14:54:34,088:INFO:Copying training dataset
2023-11-03 14:54:34,123:INFO:Defining folds
2023-11-03 14:54:34,123:INFO:Declaring metric variables
2023-11-03 14:54:34,129:INFO:Importing untrained model
2023-11-03 14:54:34,141:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 14:54:34,164:INFO:Starting cross validation
2023-11-03 14:54:34,166:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:53,819:INFO:Calculating mean and std
2023-11-03 14:54:53,832:INFO:Creating metrics dataframe
2023-11-03 14:54:53,858:INFO:Uploading results into container
2023-11-03 14:54:53,862:INFO:Uploading model into container now
2023-11-03 14:54:53,866:INFO:_master_model_container: 18
2023-11-03 14:54:53,866:INFO:_display_container: 2
2023-11-03 14:54:53,869:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:54:53,869:INFO:create_model() successfully completed......................................
2023-11-03 14:54:54,103:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:54,104:INFO:Creating metrics dataframe
2023-11-03 14:54:54,120:INFO:Initializing Dummy Regressor
2023-11-03 14:54:54,121:INFO:Total runtime is 8.50142823457718 minutes
2023-11-03 14:54:54,124:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:54,125:INFO:Initializing create_model()
2023-11-03 14:54:54,125:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:54,125:INFO:Checking exceptions
2023-11-03 14:54:54,125:INFO:Importing libraries
2023-11-03 14:54:54,126:INFO:Copying training dataset
2023-11-03 14:54:54,147:INFO:Defining folds
2023-11-03 14:54:54,147:INFO:Declaring metric variables
2023-11-03 14:54:54,151:INFO:Importing untrained model
2023-11-03 14:54:54,156:INFO:Dummy Regressor Imported successfully
2023-11-03 14:54:54,166:INFO:Starting cross validation
2023-11-03 14:54:54,167:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:54,899:INFO:Calculating mean and std
2023-11-03 14:54:54,903:INFO:Creating metrics dataframe
2023-11-03 14:54:54,913:INFO:Uploading results into container
2023-11-03 14:54:54,914:INFO:Uploading model into container now
2023-11-03 14:54:54,915:INFO:_master_model_container: 19
2023-11-03 14:54:54,915:INFO:_display_container: 2
2023-11-03 14:54:54,915:INFO:DummyRegressor()
2023-11-03 14:54:54,915:INFO:create_model() successfully completed......................................
2023-11-03 14:54:55,130:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:55,130:INFO:Creating metrics dataframe
2023-11-03 14:54:55,165:INFO:Initializing create_model()
2023-11-03 14:54:55,166:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:55,166:INFO:Checking exceptions
2023-11-03 14:54:55,170:INFO:Importing libraries
2023-11-03 14:54:55,171:INFO:Copying training dataset
2023-11-03 14:54:55,205:INFO:Defining folds
2023-11-03 14:54:55,205:INFO:Declaring metric variables
2023-11-03 14:54:55,205:INFO:Importing untrained model
2023-11-03 14:54:55,205:INFO:Declaring custom model
2023-11-03 14:54:55,206:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 14:54:55,208:INFO:Cross validation set to False
2023-11-03 14:54:55,208:INFO:Fitting Model
2023-11-03 14:54:55,410:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013091 seconds.
2023-11-03 14:54:55,410:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-03 14:54:55,411:INFO:[LightGBM] [Info] Total Bins 9522
2023-11-03 14:54:55,412:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 57
2023-11-03 14:54:55,413:INFO:[LightGBM] [Info] Start training from score 635.645879
2023-11-03 14:54:55,835:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:54:55,835:INFO:create_model() successfully completed......................................
2023-11-03 14:54:56,122:INFO:_master_model_container: 19
2023-11-03 14:54:56,122:INFO:_display_container: 2
2023-11-03 14:54:56,123:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:54:56,123:INFO:compare_models() successfully completed......................................
2023-11-03 14:54:56,124:INFO:Initializing compare_models()
2023-11-03 14:54:56,125:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-11-03 14:54:56,125:INFO:Checking exceptions
2023-11-03 14:54:56,153:INFO:Preparing display monitor
2023-11-03 14:54:56,217:INFO:Initializing Linear Regression
2023-11-03 14:54:56,218:INFO:Total runtime is 7.963180541992187e-06 minutes
2023-11-03 14:54:56,225:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:56,226:INFO:Initializing create_model()
2023-11-03 14:54:56,226:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:56,226:INFO:Checking exceptions
2023-11-03 14:54:56,226:INFO:Importing libraries
2023-11-03 14:54:56,227:INFO:Copying training dataset
2023-11-03 14:54:56,251:INFO:Defining folds
2023-11-03 14:54:56,252:INFO:Declaring metric variables
2023-11-03 14:54:56,259:INFO:Importing untrained model
2023-11-03 14:54:56,266:INFO:Linear Regression Imported successfully
2023-11-03 14:54:56,278:INFO:Starting cross validation
2023-11-03 14:54:56,280:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:57,274:INFO:Calculating mean and std
2023-11-03 14:54:57,279:INFO:Creating metrics dataframe
2023-11-03 14:54:57,287:INFO:Uploading results into container
2023-11-03 14:54:57,288:INFO:Uploading model into container now
2023-11-03 14:54:57,289:INFO:_master_model_container: 1
2023-11-03 14:54:57,290:INFO:_display_container: 2
2023-11-03 14:54:57,291:INFO:LinearRegression(n_jobs=-1)
2023-11-03 14:54:57,291:INFO:create_model() successfully completed......................................
2023-11-03 14:54:57,463:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:57,463:INFO:Creating metrics dataframe
2023-11-03 14:54:57,473:INFO:Initializing Lasso Regression
2023-11-03 14:54:57,473:INFO:Total runtime is 0.02092766761779785 minutes
2023-11-03 14:54:57,477:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:57,477:INFO:Initializing create_model()
2023-11-03 14:54:57,477:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:57,478:INFO:Checking exceptions
2023-11-03 14:54:57,478:INFO:Importing libraries
2023-11-03 14:54:57,479:INFO:Copying training dataset
2023-11-03 14:54:57,500:INFO:Defining folds
2023-11-03 14:54:57,500:INFO:Declaring metric variables
2023-11-03 14:54:57,505:INFO:Importing untrained model
2023-11-03 14:54:57,511:INFO:Lasso Regression Imported successfully
2023-11-03 14:54:57,520:INFO:Starting cross validation
2023-11-03 14:54:57,524:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:04,851:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.199e+07, tolerance: 7.608e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:04,926:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.297e+07, tolerance: 7.710e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,233:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.144e+07, tolerance: 7.656e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,373:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.240e+07, tolerance: 7.711e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,519:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.234e+07, tolerance: 7.715e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,615:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.276e+07, tolerance: 7.797e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,660:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.237e+07, tolerance: 7.670e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,718:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.103e+07, tolerance: 7.618e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:08,811:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.204e+07, tolerance: 7.640e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:08,856:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.288e+07, tolerance: 7.697e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:09,009:INFO:Calculating mean and std
2023-11-03 14:55:09,012:INFO:Creating metrics dataframe
2023-11-03 14:55:09,019:INFO:Uploading results into container
2023-11-03 14:55:09,021:INFO:Uploading model into container now
2023-11-03 14:55:09,022:INFO:_master_model_container: 2
2023-11-03 14:55:09,022:INFO:_display_container: 2
2023-11-03 14:55:09,023:INFO:Lasso(random_state=123)
2023-11-03 14:55:09,024:INFO:create_model() successfully completed......................................
2023-11-03 14:55:09,260:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:09,260:INFO:Creating metrics dataframe
2023-11-03 14:55:09,276:INFO:Initializing Ridge Regression
2023-11-03 14:55:09,276:INFO:Total runtime is 0.2176552136739095 minutes
2023-11-03 14:55:09,283:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:09,284:INFO:Initializing create_model()
2023-11-03 14:55:09,285:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:09,285:INFO:Checking exceptions
2023-11-03 14:55:09,286:INFO:Importing libraries
2023-11-03 14:55:09,286:INFO:Copying training dataset
2023-11-03 14:55:09,317:INFO:Defining folds
2023-11-03 14:55:09,317:INFO:Declaring metric variables
2023-11-03 14:55:09,323:INFO:Importing untrained model
2023-11-03 14:55:09,334:INFO:Ridge Regression Imported successfully
2023-11-03 14:55:09,362:INFO:Starting cross validation
2023-11-03 14:55:09,366:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:09,993:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.97211e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,025:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=2.64292e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,076:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.94757e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,103:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.93339e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,108:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.92872e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,469:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.95681e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,777:INFO:Calculating mean and std
2023-11-03 14:55:10,785:INFO:Creating metrics dataframe
2023-11-03 14:55:10,802:INFO:Uploading results into container
2023-11-03 14:55:10,805:INFO:Uploading model into container now
2023-11-03 14:55:10,807:INFO:_master_model_container: 3
2023-11-03 14:55:10,808:INFO:_display_container: 2
2023-11-03 14:55:10,809:INFO:Ridge(random_state=123)
2023-11-03 14:55:10,809:INFO:create_model() successfully completed......................................
2023-11-03 14:55:11,190:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:11,190:INFO:Creating metrics dataframe
2023-11-03 14:55:11,233:INFO:Initializing Elastic Net
2023-11-03 14:55:11,234:INFO:Total runtime is 0.2502824465433756 minutes
2023-11-03 14:55:11,246:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:11,247:INFO:Initializing create_model()
2023-11-03 14:55:11,248:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:11,248:INFO:Checking exceptions
2023-11-03 14:55:11,249:INFO:Importing libraries
2023-11-03 14:55:11,251:INFO:Copying training dataset
2023-11-03 14:55:11,318:INFO:Defining folds
2023-11-03 14:55:11,319:INFO:Declaring metric variables
2023-11-03 14:55:11,347:INFO:Importing untrained model
2023-11-03 14:55:11,366:INFO:Elastic Net Imported successfully
2023-11-03 14:55:11,404:INFO:Starting cross validation
2023-11-03 14:55:11,410:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:18,008:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.256e+07, tolerance: 7.670e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,120:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.167e+07, tolerance: 7.656e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,181:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.319e+07, tolerance: 7.710e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,186:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.255e+07, tolerance: 7.715e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,228:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.222e+07, tolerance: 7.608e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,300:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.261e+07, tolerance: 7.711e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,301:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.298e+07, tolerance: 7.797e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,359:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.121e+07, tolerance: 7.618e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:20,401:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.224e+07, tolerance: 7.640e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:20,498:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.309e+07, tolerance: 7.697e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:20,644:INFO:Calculating mean and std
2023-11-03 14:55:20,648:INFO:Creating metrics dataframe
2023-11-03 14:55:20,656:INFO:Uploading results into container
2023-11-03 14:55:20,657:INFO:Uploading model into container now
2023-11-03 14:55:20,658:INFO:_master_model_container: 4
2023-11-03 14:55:20,659:INFO:_display_container: 2
2023-11-03 14:55:20,660:INFO:ElasticNet(random_state=123)
2023-11-03 14:55:20,660:INFO:create_model() successfully completed......................................
2023-11-03 14:55:20,880:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:20,880:INFO:Creating metrics dataframe
2023-11-03 14:55:20,902:INFO:Initializing Least Angle Regression
2023-11-03 14:55:20,902:INFO:Total runtime is 0.4114136298497517 minutes
2023-11-03 14:55:20,910:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:20,911:INFO:Initializing create_model()
2023-11-03 14:55:20,911:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:20,911:INFO:Checking exceptions
2023-11-03 14:55:20,912:INFO:Importing libraries
2023-11-03 14:55:20,912:INFO:Copying training dataset
2023-11-03 14:55:20,949:INFO:Defining folds
2023-11-03 14:55:20,950:INFO:Declaring metric variables
2023-11-03 14:55:20,960:INFO:Importing untrained model
2023-11-03 14:55:20,971:INFO:Least Angle Regression Imported successfully
2023-11-03 14:55:20,989:INFO:Starting cross validation
2023-11-03 14:55:20,992:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:21,284:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,342:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,374:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,516:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,528:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.875e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,529:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.152e+04, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,530:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.974e+04, with an active set of 55 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,531:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.119e+04, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,532:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=7.482e+03, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,534:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.707e-03, with an active set of 39 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,572:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.070e+07, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,573:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.943e+07, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,574:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.465e+07, with an active set of 54 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,574:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.515e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,575:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,576:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=7.201e+06, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,577:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=6.765e+06, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,578:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=4.521e+06, with an active set of 55 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,578:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=5.263e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,578:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=6.642e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,628:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,628:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,646:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=3.269e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,668:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.215e+06, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,669:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.843e+06, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,670:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=9.718e+05, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,673:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.802e+05, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,677:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=9.032e-02, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,681:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,738:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.298e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,738:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.668e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,739:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.937e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,740:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.726e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,750:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=3.072e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,751:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.678e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,751:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.316e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,751:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.103e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,753:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.010e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,756:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.541e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,757:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=7.564e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,758:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.211e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,758:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.044e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,760:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.432e+04, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,761:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.311e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,762:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.285e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,776:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.097e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,777:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.022e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,789:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=7.609e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,791:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.894e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,792:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.364e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,795:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=6.150e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,795:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.199e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,866:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,882:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,899:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.973e-03, with an active set of 39 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,899:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.530e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,906:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=5.263e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,907:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.571e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,907:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.401e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,908:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=7.021e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,909:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=6.445e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,909:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.646e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,927:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=6.405e+04, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,927:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=6.346e+04, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:22,078:INFO:Calculating mean and std
2023-11-03 14:55:22,083:INFO:Creating metrics dataframe
2023-11-03 14:55:22,094:INFO:Uploading results into container
2023-11-03 14:55:22,096:INFO:Uploading model into container now
2023-11-03 14:55:22,098:INFO:_master_model_container: 5
2023-11-03 14:55:22,098:INFO:_display_container: 2
2023-11-03 14:55:22,100:INFO:Lars(random_state=123)
2023-11-03 14:55:22,101:INFO:create_model() successfully completed......................................
2023-11-03 14:55:22,341:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:22,341:INFO:Creating metrics dataframe
2023-11-03 14:55:22,361:INFO:Initializing Lasso Least Angle Regression
2023-11-03 14:55:22,361:INFO:Total runtime is 0.4357351620992024 minutes
2023-11-03 14:55:22,370:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:22,370:INFO:Initializing create_model()
2023-11-03 14:55:22,370:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:22,371:INFO:Checking exceptions
2023-11-03 14:55:22,371:INFO:Importing libraries
2023-11-03 14:55:22,371:INFO:Copying training dataset
2023-11-03 14:55:22,401:INFO:Defining folds
2023-11-03 14:55:22,401:INFO:Declaring metric variables
2023-11-03 14:55:22,408:INFO:Importing untrained model
2023-11-03 14:55:22,416:INFO:Lasso Least Angle Regression Imported successfully
2023-11-03 14:55:22,432:INFO:Starting cross validation
2023-11-03 14:55:22,440:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:22,602:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,640:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,700:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,719:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,783:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,808:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,859:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,895:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,922:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,952:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:23,110:INFO:Calculating mean and std
2023-11-03 14:55:23,114:INFO:Creating metrics dataframe
2023-11-03 14:55:23,120:INFO:Uploading results into container
2023-11-03 14:55:23,122:INFO:Uploading model into container now
2023-11-03 14:55:23,123:INFO:_master_model_container: 6
2023-11-03 14:55:23,123:INFO:_display_container: 2
2023-11-03 14:55:23,124:INFO:LassoLars(random_state=123)
2023-11-03 14:55:23,124:INFO:create_model() successfully completed......................................
2023-11-03 14:55:23,291:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:23,291:INFO:Creating metrics dataframe
2023-11-03 14:55:23,306:INFO:Initializing Orthogonal Matching Pursuit
2023-11-03 14:55:23,306:INFO:Total runtime is 0.45147597789764393 minutes
2023-11-03 14:55:23,309:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:23,310:INFO:Initializing create_model()
2023-11-03 14:55:23,310:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:23,311:INFO:Checking exceptions
2023-11-03 14:55:23,311:INFO:Importing libraries
2023-11-03 14:55:23,311:INFO:Copying training dataset
2023-11-03 14:55:23,334:INFO:Defining folds
2023-11-03 14:55:23,334:INFO:Declaring metric variables
2023-11-03 14:55:23,339:INFO:Importing untrained model
2023-11-03 14:55:23,345:INFO:Orthogonal Matching Pursuit Imported successfully
2023-11-03 14:55:23,356:INFO:Starting cross validation
2023-11-03 14:55:23,358:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:23,515:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,518:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,558:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,625:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,667:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,697:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,741:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,763:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,792:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,820:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,983:INFO:Calculating mean and std
2023-11-03 14:55:23,989:INFO:Creating metrics dataframe
2023-11-03 14:55:23,997:INFO:Uploading results into container
2023-11-03 14:55:23,998:INFO:Uploading model into container now
2023-11-03 14:55:23,999:INFO:_master_model_container: 7
2023-11-03 14:55:23,999:INFO:_display_container: 2
2023-11-03 14:55:24,000:INFO:OrthogonalMatchingPursuit()
2023-11-03 14:55:24,000:INFO:create_model() successfully completed......................................
2023-11-03 14:55:24,175:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:24,175:INFO:Creating metrics dataframe
2023-11-03 14:55:24,190:INFO:Initializing Bayesian Ridge
2023-11-03 14:55:24,191:INFO:Total runtime is 0.46622438033421826 minutes
2023-11-03 14:55:24,195:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:24,196:INFO:Initializing create_model()
2023-11-03 14:55:24,196:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:24,196:INFO:Checking exceptions
2023-11-03 14:55:24,197:INFO:Importing libraries
2023-11-03 14:55:24,197:INFO:Copying training dataset
2023-11-03 14:55:24,223:INFO:Defining folds
2023-11-03 14:55:24,224:INFO:Declaring metric variables
2023-11-03 14:55:24,228:INFO:Importing untrained model
2023-11-03 14:55:24,234:INFO:Bayesian Ridge Imported successfully
2023-11-03 14:55:24,244:INFO:Starting cross validation
2023-11-03 14:55:24,246:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:25,469:INFO:Calculating mean and std
2023-11-03 14:55:25,472:INFO:Creating metrics dataframe
2023-11-03 14:55:25,478:INFO:Uploading results into container
2023-11-03 14:55:25,479:INFO:Uploading model into container now
2023-11-03 14:55:25,480:INFO:_master_model_container: 8
2023-11-03 14:55:25,480:INFO:_display_container: 2
2023-11-03 14:55:25,480:INFO:BayesianRidge()
2023-11-03 14:55:25,480:INFO:create_model() successfully completed......................................
2023-11-03 14:55:25,658:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:25,658:INFO:Creating metrics dataframe
2023-11-03 14:55:25,680:INFO:Initializing Passive Aggressive Regressor
2023-11-03 14:55:25,680:INFO:Total runtime is 0.491053815682729 minutes
2023-11-03 14:55:25,688:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:25,701:INFO:Initializing create_model()
2023-11-03 14:55:25,702:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:25,702:INFO:Checking exceptions
2023-11-03 14:55:25,702:INFO:Importing libraries
2023-11-03 14:55:25,702:INFO:Copying training dataset
2023-11-03 14:55:25,730:INFO:Defining folds
2023-11-03 14:55:25,730:INFO:Declaring metric variables
2023-11-03 14:55:25,737:INFO:Importing untrained model
2023-11-03 14:55:25,742:INFO:Passive Aggressive Regressor Imported successfully
2023-11-03 14:55:25,754:INFO:Starting cross validation
2023-11-03 14:55:25,756:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:26,872:INFO:Calculating mean and std
2023-11-03 14:55:26,901:INFO:Creating metrics dataframe
2023-11-03 14:55:26,912:INFO:Uploading results into container
2023-11-03 14:55:26,914:INFO:Uploading model into container now
2023-11-03 14:55:26,916:INFO:_master_model_container: 9
2023-11-03 14:55:26,917:INFO:_display_container: 2
2023-11-03 14:55:26,918:INFO:PassiveAggressiveRegressor(random_state=123)
2023-11-03 14:55:26,918:INFO:create_model() successfully completed......................................
2023-11-03 14:55:27,131:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:27,131:INFO:Creating metrics dataframe
2023-11-03 14:55:27,155:INFO:Initializing Huber Regressor
2023-11-03 14:55:27,155:INFO:Total runtime is 0.51563903093338 minutes
2023-11-03 14:55:27,168:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:27,169:INFO:Initializing create_model()
2023-11-03 14:55:27,169:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:27,169:INFO:Checking exceptions
2023-11-03 14:55:27,170:INFO:Importing libraries
2023-11-03 14:55:27,170:INFO:Copying training dataset
2023-11-03 14:55:27,210:INFO:Defining folds
2023-11-03 14:55:27,210:INFO:Declaring metric variables
2023-11-03 14:55:27,221:INFO:Importing untrained model
2023-11-03 14:55:27,227:INFO:Huber Regressor Imported successfully
2023-11-03 14:55:27,241:INFO:Starting cross validation
2023-11-03 14:55:27,245:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:30,694:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,712:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,745:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,814:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,880:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,932:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,979:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:31,044:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:32,326:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:32,328:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:32,461:INFO:Calculating mean and std
2023-11-03 14:55:32,466:INFO:Creating metrics dataframe
2023-11-03 14:55:32,472:INFO:Uploading results into container
2023-11-03 14:55:32,473:INFO:Uploading model into container now
2023-11-03 14:55:32,475:INFO:_master_model_container: 10
2023-11-03 14:55:32,475:INFO:_display_container: 2
2023-11-03 14:55:32,476:INFO:HuberRegressor()
2023-11-03 14:55:32,476:INFO:create_model() successfully completed......................................
2023-11-03 14:55:32,651:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:32,651:INFO:Creating metrics dataframe
2023-11-03 14:55:32,669:INFO:Initializing K Neighbors Regressor
2023-11-03 14:55:32,670:INFO:Total runtime is 0.6075413505236307 minutes
2023-11-03 14:55:32,675:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:32,675:INFO:Initializing create_model()
2023-11-03 14:55:32,675:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:32,675:INFO:Checking exceptions
2023-11-03 14:55:32,675:INFO:Importing libraries
2023-11-03 14:55:32,676:INFO:Copying training dataset
2023-11-03 14:55:32,706:INFO:Defining folds
2023-11-03 14:55:32,707:INFO:Declaring metric variables
2023-11-03 14:55:32,716:INFO:Importing untrained model
2023-11-03 14:55:32,723:INFO:K Neighbors Regressor Imported successfully
2023-11-03 14:55:32,739:INFO:Starting cross validation
2023-11-03 14:55:32,742:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:36,433:INFO:Calculating mean and std
2023-11-03 14:55:36,438:INFO:Creating metrics dataframe
2023-11-03 14:55:36,454:INFO:Uploading results into container
2023-11-03 14:55:36,457:INFO:Uploading model into container now
2023-11-03 14:55:36,458:INFO:_master_model_container: 11
2023-11-03 14:55:36,458:INFO:_display_container: 2
2023-11-03 14:55:36,461:INFO:KNeighborsRegressor(n_jobs=-1)
2023-11-03 14:55:36,461:INFO:create_model() successfully completed......................................
2023-11-03 14:55:36,798:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:36,799:INFO:Creating metrics dataframe
2023-11-03 14:55:36,828:INFO:Initializing Decision Tree Regressor
2023-11-03 14:55:36,828:INFO:Total runtime is 0.6768476168314614 minutes
2023-11-03 14:55:36,837:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:36,837:INFO:Initializing create_model()
2023-11-03 14:55:36,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:36,838:INFO:Checking exceptions
2023-11-03 14:55:36,838:INFO:Importing libraries
2023-11-03 14:55:36,838:INFO:Copying training dataset
2023-11-03 14:55:36,880:INFO:Defining folds
2023-11-03 14:55:36,881:INFO:Declaring metric variables
2023-11-03 14:55:36,890:INFO:Importing untrained model
2023-11-03 14:55:36,902:INFO:Decision Tree Regressor Imported successfully
2023-11-03 14:55:36,923:INFO:Starting cross validation
2023-11-03 14:55:36,925:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:42,082:INFO:Calculating mean and std
2023-11-03 14:55:42,084:INFO:Creating metrics dataframe
2023-11-03 14:55:42,088:INFO:Uploading results into container
2023-11-03 14:55:42,090:INFO:Uploading model into container now
2023-11-03 14:55:42,091:INFO:_master_model_container: 12
2023-11-03 14:55:42,091:INFO:_display_container: 2
2023-11-03 14:55:42,092:INFO:DecisionTreeRegressor(random_state=123)
2023-11-03 14:55:42,092:INFO:create_model() successfully completed......................................
2023-11-03 14:55:42,299:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:42,300:INFO:Creating metrics dataframe
2023-11-03 14:55:42,319:INFO:Initializing Random Forest Regressor
2023-11-03 14:55:42,319:INFO:Total runtime is 0.7683629631996154 minutes
2023-11-03 14:55:42,324:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:42,324:INFO:Initializing create_model()
2023-11-03 14:55:42,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:42,325:INFO:Checking exceptions
2023-11-03 14:55:42,325:INFO:Importing libraries
2023-11-03 14:55:42,325:INFO:Copying training dataset
2023-11-03 14:55:42,352:INFO:Defining folds
2023-11-03 14:55:42,353:INFO:Declaring metric variables
2023-11-03 14:55:42,360:INFO:Importing untrained model
2023-11-03 14:55:42,369:INFO:Random Forest Regressor Imported successfully
2023-11-03 14:55:42,381:INFO:Starting cross validation
2023-11-03 14:55:42,383:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:58:16,975:INFO:Calculating mean and std
2023-11-03 14:58:16,990:INFO:Creating metrics dataframe
2023-11-03 14:58:17,009:INFO:Uploading results into container
2023-11-03 14:58:17,011:INFO:Uploading model into container now
2023-11-03 14:58:17,014:INFO:_master_model_container: 13
2023-11-03 14:58:17,014:INFO:_display_container: 2
2023-11-03 14:58:17,016:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:58:17,016:INFO:create_model() successfully completed......................................
2023-11-03 14:58:17,760:INFO:SubProcess create_model() end ==================================
2023-11-03 14:58:17,760:INFO:Creating metrics dataframe
2023-11-03 14:58:17,781:INFO:Initializing Extra Trees Regressor
2023-11-03 14:58:17,781:INFO:Total runtime is 3.3594010670979815 minutes
2023-11-03 14:58:17,787:INFO:SubProcess create_model() called ==================================
2023-11-03 14:58:17,788:INFO:Initializing create_model()
2023-11-03 14:58:17,788:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:58:17,788:INFO:Checking exceptions
2023-11-03 14:58:17,788:INFO:Importing libraries
2023-11-03 14:58:17,788:INFO:Copying training dataset
2023-11-03 14:58:17,840:INFO:Defining folds
2023-11-03 14:58:17,841:INFO:Declaring metric variables
2023-11-03 14:58:17,846:INFO:Importing untrained model
2023-11-03 14:58:17,854:INFO:Extra Trees Regressor Imported successfully
2023-11-03 14:58:17,867:INFO:Starting cross validation
2023-11-03 14:58:17,869:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:59:21,058:INFO:Calculating mean and std
2023-11-03 14:59:21,084:INFO:Creating metrics dataframe
2023-11-03 14:59:21,118:INFO:Uploading results into container
2023-11-03 14:59:21,122:INFO:Uploading model into container now
2023-11-03 14:59:21,125:INFO:_master_model_container: 14
2023-11-03 14:59:21,126:INFO:_display_container: 2
2023-11-03 14:59:21,129:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:59:21,130:INFO:create_model() successfully completed......................................
2023-11-03 14:59:21,613:INFO:SubProcess create_model() end ==================================
2023-11-03 14:59:21,613:INFO:Creating metrics dataframe
2023-11-03 14:59:21,632:INFO:Initializing AdaBoost Regressor
2023-11-03 14:59:21,632:INFO:Total runtime is 4.423589499791463 minutes
2023-11-03 14:59:21,638:INFO:SubProcess create_model() called ==================================
2023-11-03 14:59:21,638:INFO:Initializing create_model()
2023-11-03 14:59:21,638:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:59:21,639:INFO:Checking exceptions
2023-11-03 14:59:21,639:INFO:Importing libraries
2023-11-03 14:59:21,639:INFO:Copying training dataset
2023-11-03 14:59:21,685:INFO:Defining folds
2023-11-03 14:59:21,686:INFO:Declaring metric variables
2023-11-03 14:59:21,694:INFO:Importing untrained model
2023-11-03 14:59:21,708:INFO:AdaBoost Regressor Imported successfully
2023-11-03 14:59:21,731:INFO:Starting cross validation
2023-11-03 14:59:21,739:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:59:53,080:INFO:Calculating mean and std
2023-11-03 14:59:53,086:INFO:Creating metrics dataframe
2023-11-03 14:59:53,097:INFO:Uploading results into container
2023-11-03 14:59:53,099:INFO:Uploading model into container now
2023-11-03 14:59:53,101:INFO:_master_model_container: 15
2023-11-03 14:59:53,101:INFO:_display_container: 2
2023-11-03 14:59:53,102:INFO:AdaBoostRegressor(random_state=123)
2023-11-03 14:59:53,102:INFO:create_model() successfully completed......................................
2023-11-03 14:59:53,356:INFO:SubProcess create_model() end ==================================
2023-11-03 14:59:53,356:INFO:Creating metrics dataframe
2023-11-03 14:59:53,397:INFO:Initializing Gradient Boosting Regressor
2023-11-03 14:59:53,399:INFO:Total runtime is 4.953032247225443 minutes
2023-11-03 14:59:53,411:INFO:SubProcess create_model() called ==================================
2023-11-03 14:59:53,412:INFO:Initializing create_model()
2023-11-03 14:59:53,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:59:53,412:INFO:Checking exceptions
2023-11-03 14:59:53,413:INFO:Importing libraries
2023-11-03 14:59:53,413:INFO:Copying training dataset
2023-11-03 14:59:53,495:INFO:Defining folds
2023-11-03 14:59:53,497:INFO:Declaring metric variables
2023-11-03 14:59:53,510:INFO:Importing untrained model
2023-11-03 14:59:53,520:INFO:Gradient Boosting Regressor Imported successfully
2023-11-03 14:59:53,547:INFO:Starting cross validation
2023-11-03 14:59:53,554:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:01:01,425:INFO:Calculating mean and std
2023-11-03 15:01:01,447:INFO:Creating metrics dataframe
2023-11-03 15:01:01,482:INFO:Uploading results into container
2023-11-03 15:01:01,486:INFO:Uploading model into container now
2023-11-03 15:01:01,490:INFO:_master_model_container: 16
2023-11-03 15:01:01,491:INFO:_display_container: 2
2023-11-03 15:01:01,496:INFO:GradientBoostingRegressor(random_state=123)
2023-11-03 15:01:01,497:INFO:create_model() successfully completed......................................
2023-11-03 15:01:02,094:INFO:SubProcess create_model() end ==================================
2023-11-03 15:01:02,094:INFO:Creating metrics dataframe
2023-11-03 15:01:02,115:INFO:Initializing Extreme Gradient Boosting
2023-11-03 15:01:02,115:INFO:Total runtime is 6.098301831881205 minutes
2023-11-03 15:01:02,120:INFO:SubProcess create_model() called ==================================
2023-11-03 15:01:02,120:INFO:Initializing create_model()
2023-11-03 15:01:02,120:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:01:02,120:INFO:Checking exceptions
2023-11-03 15:01:02,121:INFO:Importing libraries
2023-11-03 15:01:02,121:INFO:Copying training dataset
2023-11-03 15:01:02,153:INFO:Defining folds
2023-11-03 15:01:02,154:INFO:Declaring metric variables
2023-11-03 15:01:02,161:INFO:Importing untrained model
2023-11-03 15:01:02,292:INFO:Extreme Gradient Boosting Imported successfully
2023-11-03 15:01:02,334:INFO:Starting cross validation
2023-11-03 15:01:02,338:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:01:18,486:INFO:Calculating mean and std
2023-11-03 15:01:18,493:INFO:Creating metrics dataframe
2023-11-03 15:01:18,505:INFO:Uploading results into container
2023-11-03 15:01:18,507:INFO:Uploading model into container now
2023-11-03 15:01:18,509:INFO:_master_model_container: 17
2023-11-03 15:01:18,510:INFO:_display_container: 2
2023-11-03 15:01:18,513:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-03 15:01:18,513:INFO:create_model() successfully completed......................................
2023-11-03 15:01:18,846:INFO:SubProcess create_model() end ==================================
2023-11-03 15:01:18,847:INFO:Creating metrics dataframe
2023-11-03 15:01:18,938:INFO:Initializing Light Gradient Boosting Machine
2023-11-03 15:01:18,939:INFO:Total runtime is 6.378700212637583 minutes
2023-11-03 15:01:18,965:INFO:SubProcess create_model() called ==================================
2023-11-03 15:01:18,966:INFO:Initializing create_model()
2023-11-03 15:01:18,967:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:01:18,967:INFO:Checking exceptions
2023-11-03 15:01:18,967:INFO:Importing libraries
2023-11-03 15:01:18,968:INFO:Copying training dataset
2023-11-03 15:01:19,419:INFO:Defining folds
2023-11-03 15:01:19,420:INFO:Declaring metric variables
2023-11-03 15:01:19,433:INFO:Importing untrained model
2023-11-03 15:01:19,454:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 15:01:19,705:INFO:Starting cross validation
2023-11-03 15:01:19,712:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:08,253:INFO:Calculating mean and std
2023-11-03 15:02:08,273:INFO:Creating metrics dataframe
2023-11-03 15:02:08,302:INFO:Uploading results into container
2023-11-03 15:02:08,305:INFO:Uploading model into container now
2023-11-03 15:02:08,310:INFO:_master_model_container: 18
2023-11-03 15:02:08,310:INFO:_display_container: 2
2023-11-03 15:02:08,313:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:02:08,313:INFO:create_model() successfully completed......................................
2023-11-03 15:02:09,002:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:09,002:INFO:Creating metrics dataframe
2023-11-03 15:02:09,027:INFO:Initializing Dummy Regressor
2023-11-03 15:02:09,028:INFO:Total runtime is 7.213509631156922 minutes
2023-11-03 15:02:09,032:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:09,033:INFO:Initializing create_model()
2023-11-03 15:02:09,033:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:09,033:INFO:Checking exceptions
2023-11-03 15:02:09,033:INFO:Importing libraries
2023-11-03 15:02:09,034:INFO:Copying training dataset
2023-11-03 15:02:09,147:INFO:Defining folds
2023-11-03 15:02:09,147:INFO:Declaring metric variables
2023-11-03 15:02:09,156:INFO:Importing untrained model
2023-11-03 15:02:09,165:INFO:Dummy Regressor Imported successfully
2023-11-03 15:02:09,181:INFO:Starting cross validation
2023-11-03 15:02:09,185:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:10,091:INFO:Calculating mean and std
2023-11-03 15:02:10,096:INFO:Creating metrics dataframe
2023-11-03 15:02:10,103:INFO:Uploading results into container
2023-11-03 15:02:10,105:INFO:Uploading model into container now
2023-11-03 15:02:10,106:INFO:_master_model_container: 19
2023-11-03 15:02:10,106:INFO:_display_container: 2
2023-11-03 15:02:10,107:INFO:DummyRegressor()
2023-11-03 15:02:10,107:INFO:create_model() successfully completed......................................
2023-11-03 15:02:10,275:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:10,275:INFO:Creating metrics dataframe
2023-11-03 15:02:10,308:INFO:Initializing create_model()
2023-11-03 15:02:10,308:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:10,309:INFO:Checking exceptions
2023-11-03 15:02:10,314:INFO:Importing libraries
2023-11-03 15:02:10,314:INFO:Copying training dataset
2023-11-03 15:02:10,338:INFO:Defining folds
2023-11-03 15:02:10,339:INFO:Declaring metric variables
2023-11-03 15:02:10,339:INFO:Importing untrained model
2023-11-03 15:02:10,339:INFO:Declaring custom model
2023-11-03 15:02:10,340:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 15:02:10,342:INFO:Cross validation set to False
2023-11-03 15:02:10,342:INFO:Fitting Model
2023-11-03 15:02:10,510:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011705 seconds.
2023-11-03 15:02:10,510:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-03 15:02:10,510:INFO:[LightGBM] [Info] Total Bins 9588
2023-11-03 15:02:10,512:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 57
2023-11-03 15:02:10,513:INFO:[LightGBM] [Info] Start training from score 94.273259
2023-11-03 15:02:11,069:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:02:11,069:INFO:create_model() successfully completed......................................
2023-11-03 15:02:11,348:INFO:_master_model_container: 19
2023-11-03 15:02:11,348:INFO:_display_container: 2
2023-11-03 15:02:11,349:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:02:11,349:INFO:compare_models() successfully completed......................................
2023-11-03 15:02:11,350:INFO:Initializing compare_models()
2023-11-03 15:02:11,350:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-11-03 15:02:11,350:INFO:Checking exceptions
2023-11-03 15:02:11,401:INFO:Preparing display monitor
2023-11-03 15:02:11,671:INFO:Initializing Linear Regression
2023-11-03 15:02:11,673:INFO:Total runtime is 3.701845804850261e-05 minutes
2023-11-03 15:02:11,685:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:11,689:INFO:Initializing create_model()
2023-11-03 15:02:11,690:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:11,690:INFO:Checking exceptions
2023-11-03 15:02:11,690:INFO:Importing libraries
2023-11-03 15:02:11,690:INFO:Copying training dataset
2023-11-03 15:02:11,766:INFO:Defining folds
2023-11-03 15:02:11,766:INFO:Declaring metric variables
2023-11-03 15:02:11,772:INFO:Importing untrained model
2023-11-03 15:02:11,785:INFO:Linear Regression Imported successfully
2023-11-03 15:02:11,802:INFO:Starting cross validation
2023-11-03 15:02:11,805:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:12,785:INFO:Calculating mean and std
2023-11-03 15:02:12,788:INFO:Creating metrics dataframe
2023-11-03 15:02:12,795:INFO:Uploading results into container
2023-11-03 15:02:12,796:INFO:Uploading model into container now
2023-11-03 15:02:12,797:INFO:_master_model_container: 1
2023-11-03 15:02:12,797:INFO:_display_container: 2
2023-11-03 15:02:12,798:INFO:LinearRegression(n_jobs=-1)
2023-11-03 15:02:12,798:INFO:create_model() successfully completed......................................
2023-11-03 15:02:12,972:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:12,972:INFO:Creating metrics dataframe
2023-11-03 15:02:12,990:INFO:Initializing Lasso Regression
2023-11-03 15:02:12,990:INFO:Total runtime is 0.021985952059427896 minutes
2023-11-03 15:02:12,999:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:12,999:INFO:Initializing create_model()
2023-11-03 15:02:13,000:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:13,000:INFO:Checking exceptions
2023-11-03 15:02:13,000:INFO:Importing libraries
2023-11-03 15:02:13,001:INFO:Copying training dataset
2023-11-03 15:02:13,038:INFO:Defining folds
2023-11-03 15:02:13,039:INFO:Declaring metric variables
2023-11-03 15:02:13,049:INFO:Importing untrained model
2023-11-03 15:02:13,057:INFO:Lasso Regression Imported successfully
2023-11-03 15:02:13,077:INFO:Starting cross validation
2023-11-03 15:02:13,080:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:20,517:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.202e+07, tolerance: 4.504e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,660:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.261e+07, tolerance: 4.496e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,701:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.231e+07, tolerance: 4.450e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,791:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.240e+07, tolerance: 4.512e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,844:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+07, tolerance: 4.482e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,886:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.206e+07, tolerance: 4.493e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,894:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+07, tolerance: 4.540e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,912:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+07, tolerance: 4.520e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:23,237:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+07, tolerance: 4.472e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:23,257:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.248e+07, tolerance: 4.467e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:23,317:INFO:Calculating mean and std
2023-11-03 15:02:23,323:INFO:Creating metrics dataframe
2023-11-03 15:02:23,329:INFO:Uploading results into container
2023-11-03 15:02:23,330:INFO:Uploading model into container now
2023-11-03 15:02:23,331:INFO:_master_model_container: 2
2023-11-03 15:02:23,331:INFO:_display_container: 2
2023-11-03 15:02:23,332:INFO:Lasso(random_state=123)
2023-11-03 15:02:23,333:INFO:create_model() successfully completed......................................
2023-11-03 15:02:23,512:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:23,512:INFO:Creating metrics dataframe
2023-11-03 15:02:23,531:INFO:Initializing Ridge Regression
2023-11-03 15:02:23,531:INFO:Total runtime is 0.19767549832661946 minutes
2023-11-03 15:02:23,541:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:23,542:INFO:Initializing create_model()
2023-11-03 15:02:23,542:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:23,542:INFO:Checking exceptions
2023-11-03 15:02:23,543:INFO:Importing libraries
2023-11-03 15:02:23,543:INFO:Copying training dataset
2023-11-03 15:02:23,580:INFO:Defining folds
2023-11-03 15:02:23,580:INFO:Declaring metric variables
2023-11-03 15:02:23,591:INFO:Importing untrained model
2023-11-03 15:02:23,604:INFO:Ridge Regression Imported successfully
2023-11-03 15:02:23,624:INFO:Starting cross validation
2023-11-03 15:02:23,627:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:23,984:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=6.33162e-18): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,036:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.79232e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,257:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.75036e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,372:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.71232e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,384:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.74887e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,454:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.78896e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,631:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.76594e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,661:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.28532e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,897:INFO:Calculating mean and std
2023-11-03 15:02:24,902:INFO:Creating metrics dataframe
2023-11-03 15:02:24,915:INFO:Uploading results into container
2023-11-03 15:02:24,918:INFO:Uploading model into container now
2023-11-03 15:02:24,920:INFO:_master_model_container: 3
2023-11-03 15:02:24,920:INFO:_display_container: 2
2023-11-03 15:02:24,922:INFO:Ridge(random_state=123)
2023-11-03 15:02:24,924:INFO:create_model() successfully completed......................................
2023-11-03 15:02:25,263:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:25,263:INFO:Creating metrics dataframe
2023-11-03 15:02:25,290:INFO:Initializing Elastic Net
2023-11-03 15:02:25,291:INFO:Total runtime is 0.22700021664301553 minutes
2023-11-03 15:02:25,302:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:25,303:INFO:Initializing create_model()
2023-11-03 15:02:25,303:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:25,304:INFO:Checking exceptions
2023-11-03 15:02:25,304:INFO:Importing libraries
2023-11-03 15:02:25,305:INFO:Copying training dataset
2023-11-03 15:02:25,353:INFO:Defining folds
2023-11-03 15:02:25,353:INFO:Declaring metric variables
2023-11-03 15:02:25,365:INFO:Importing untrained model
2023-11-03 15:02:25,384:INFO:Elastic Net Imported successfully
2023-11-03 15:02:25,484:INFO:Starting cross validation
2023-11-03 15:02:25,490:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:33,291:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+07, tolerance: 4.496e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,296:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.241e+07, tolerance: 4.450e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,417:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e+07, tolerance: 4.540e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,438:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+07, tolerance: 4.504e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,447:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.252e+07, tolerance: 4.512e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,455:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+07, tolerance: 4.482e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,514:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+07, tolerance: 4.520e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,516:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+07, tolerance: 4.493e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:35,947:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.204e+07, tolerance: 4.472e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:35,973:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.259e+07, tolerance: 4.467e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:43,089:INFO:Calculating mean and std
2023-11-03 15:02:43,094:INFO:Creating metrics dataframe
2023-11-03 15:02:43,101:INFO:Uploading results into container
2023-11-03 15:02:43,104:INFO:Uploading model into container now
2023-11-03 15:02:43,110:INFO:_master_model_container: 4
2023-11-03 15:02:43,110:INFO:_display_container: 2
2023-11-03 15:02:43,111:INFO:ElasticNet(random_state=123)
2023-11-03 15:02:43,111:INFO:create_model() successfully completed......................................
2023-11-03 15:02:43,350:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:43,350:INFO:Creating metrics dataframe
2023-11-03 15:02:43,376:INFO:Initializing Least Angle Regression
2023-11-03 15:02:43,377:INFO:Total runtime is 0.5284322182337443 minutes
2023-11-03 15:02:43,389:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:43,390:INFO:Initializing create_model()
2023-11-03 15:02:43,391:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:43,391:INFO:Checking exceptions
2023-11-03 15:02:43,392:INFO:Importing libraries
2023-11-03 15:02:43,392:INFO:Copying training dataset
2023-11-03 15:02:43,447:INFO:Defining folds
2023-11-03 15:02:43,448:INFO:Declaring metric variables
2023-11-03 15:02:43,479:INFO:Importing untrained model
2023-11-03 15:02:43,500:INFO:Least Angle Regression Imported successfully
2023-11-03 15:02:43,521:INFO:Starting cross validation
2023-11-03 15:02:43,523:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:43,715:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,762:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,765:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,781:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,795:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.016e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,806:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.597e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,861:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,906:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=7.905e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,907:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.041e-01, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,909:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=8.962e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,909:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.092e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,910:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.631e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,910:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=4.697e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,911:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.028e+00, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,911:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.016e+02, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,912:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.682e+00, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,912:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=6.725e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,921:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.040e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,934:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.745e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,942:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,956:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,985:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.563e+05, with an active set of 57 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,996:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.619e+04, with an active set of 57 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,018:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=6.871e-01, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,021:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.182e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,022:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=9.180e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,022:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=6.061e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,034:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:44,046:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=9.541e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,050:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.511e-01, with an active set of 58 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,071:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.001e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,075:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.391e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,097:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.950e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,098:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.777e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,098:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.041e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,098:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.495e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,136:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:44,151:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:44,170:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.313e+00, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.306e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,170:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.525e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.306e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,170:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.305e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,183:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.180e+00, with an active set of 56 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,184:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.543e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,323:INFO:Calculating mean and std
2023-11-03 15:02:44,325:INFO:Creating metrics dataframe
2023-11-03 15:02:44,332:INFO:Uploading results into container
2023-11-03 15:02:44,333:INFO:Uploading model into container now
2023-11-03 15:02:44,335:INFO:_master_model_container: 5
2023-11-03 15:02:44,335:INFO:_display_container: 2
2023-11-03 15:02:44,336:INFO:Lars(random_state=123)
2023-11-03 15:02:44,336:INFO:create_model() successfully completed......................................
2023-11-03 15:02:44,507:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:44,507:INFO:Creating metrics dataframe
2023-11-03 15:02:44,520:INFO:Initializing Lasso Least Angle Regression
2023-11-03 15:02:44,521:INFO:Total runtime is 0.5475023667017619 minutes
2023-11-03 15:02:44,525:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:44,525:INFO:Initializing create_model()
2023-11-03 15:02:44,526:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:44,526:INFO:Checking exceptions
2023-11-03 15:02:44,526:INFO:Importing libraries
2023-11-03 15:02:44,526:INFO:Copying training dataset
2023-11-03 15:02:44,550:INFO:Defining folds
2023-11-03 15:02:44,550:INFO:Declaring metric variables
2023-11-03 15:02:44,556:INFO:Importing untrained model
2023-11-03 15:02:44,562:INFO:Lasso Least Angle Regression Imported successfully
2023-11-03 15:02:44,580:INFO:Starting cross validation
2023-11-03 15:02:44,583:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:44,818:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:44,823:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:44,864:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:44,947:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:44,969:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,074:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,100:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,132:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,230:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,231:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,395:INFO:Calculating mean and std
2023-11-03 15:02:45,399:INFO:Creating metrics dataframe
2023-11-03 15:02:45,406:INFO:Uploading results into container
2023-11-03 15:02:45,407:INFO:Uploading model into container now
2023-11-03 15:02:45,408:INFO:_master_model_container: 6
2023-11-03 15:02:45,408:INFO:_display_container: 2
2023-11-03 15:02:45,409:INFO:LassoLars(random_state=123)
2023-11-03 15:02:45,409:INFO:create_model() successfully completed......................................
2023-11-03 15:02:45,576:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:45,576:INFO:Creating metrics dataframe
2023-11-03 15:02:45,594:INFO:Initializing Orthogonal Matching Pursuit
2023-11-03 15:02:45,594:INFO:Total runtime is 0.5653855164845785 minutes
2023-11-03 15:02:45,600:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:45,601:INFO:Initializing create_model()
2023-11-03 15:02:45,601:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:45,601:INFO:Checking exceptions
2023-11-03 15:02:45,601:INFO:Importing libraries
2023-11-03 15:02:45,601:INFO:Copying training dataset
2023-11-03 15:02:45,631:INFO:Defining folds
2023-11-03 15:02:45,632:INFO:Declaring metric variables
2023-11-03 15:02:45,642:INFO:Importing untrained model
2023-11-03 15:02:45,651:INFO:Orthogonal Matching Pursuit Imported successfully
2023-11-03 15:02:45,671:INFO:Starting cross validation
2023-11-03 15:02:45,675:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:45,879:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:45,887:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:45,934:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:45,979:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,045:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,064:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,131:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,152:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,237:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,278:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,477:INFO:Calculating mean and std
2023-11-03 15:02:46,479:INFO:Creating metrics dataframe
2023-11-03 15:02:46,485:INFO:Uploading results into container
2023-11-03 15:02:46,486:INFO:Uploading model into container now
2023-11-03 15:02:46,487:INFO:_master_model_container: 7
2023-11-03 15:02:46,487:INFO:_display_container: 2
2023-11-03 15:02:46,487:INFO:OrthogonalMatchingPursuit()
2023-11-03 15:02:46,487:INFO:create_model() successfully completed......................................
2023-11-03 15:02:46,644:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:46,645:INFO:Creating metrics dataframe
2023-11-03 15:02:46,659:INFO:Initializing Bayesian Ridge
2023-11-03 15:02:46,659:INFO:Total runtime is 0.5831444025039673 minutes
2023-11-03 15:02:46,664:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:46,665:INFO:Initializing create_model()
2023-11-03 15:02:46,665:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:46,665:INFO:Checking exceptions
2023-11-03 15:02:46,665:INFO:Importing libraries
2023-11-03 15:02:46,666:INFO:Copying training dataset
2023-11-03 15:02:46,701:INFO:Defining folds
2023-11-03 15:02:46,705:INFO:Declaring metric variables
2023-11-03 15:02:46,714:INFO:Importing untrained model
2023-11-03 15:02:46,722:INFO:Bayesian Ridge Imported successfully
2023-11-03 15:02:46,737:INFO:Starting cross validation
2023-11-03 15:02:46,739:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:48,021:INFO:Calculating mean and std
2023-11-03 15:02:48,026:INFO:Creating metrics dataframe
2023-11-03 15:02:48,035:INFO:Uploading results into container
2023-11-03 15:02:48,037:INFO:Uploading model into container now
2023-11-03 15:02:48,038:INFO:_master_model_container: 8
2023-11-03 15:02:48,038:INFO:_display_container: 2
2023-11-03 15:02:48,039:INFO:BayesianRidge()
2023-11-03 15:02:48,039:INFO:create_model() successfully completed......................................
2023-11-03 15:02:48,251:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:48,256:INFO:Creating metrics dataframe
2023-11-03 15:02:48,343:INFO:Initializing Passive Aggressive Regressor
2023-11-03 15:02:48,344:INFO:Total runtime is 0.6112192511558533 minutes
2023-11-03 15:02:48,359:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:48,360:INFO:Initializing create_model()
2023-11-03 15:02:48,361:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:48,361:INFO:Checking exceptions
2023-11-03 15:02:48,361:INFO:Importing libraries
2023-11-03 15:02:48,361:INFO:Copying training dataset
2023-11-03 15:02:48,423:INFO:Defining folds
2023-11-03 15:02:48,423:INFO:Declaring metric variables
2023-11-03 15:02:48,462:INFO:Importing untrained model
2023-11-03 15:02:48,478:INFO:Passive Aggressive Regressor Imported successfully
2023-11-03 15:02:48,502:INFO:Starting cross validation
2023-11-03 15:02:48,505:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:49,420:INFO:Calculating mean and std
2023-11-03 15:02:49,423:INFO:Creating metrics dataframe
2023-11-03 15:02:49,429:INFO:Uploading results into container
2023-11-03 15:02:49,430:INFO:Uploading model into container now
2023-11-03 15:02:49,432:INFO:_master_model_container: 9
2023-11-03 15:02:49,432:INFO:_display_container: 2
2023-11-03 15:02:49,433:INFO:PassiveAggressiveRegressor(random_state=123)
2023-11-03 15:02:49,433:INFO:create_model() successfully completed......................................
2023-11-03 15:02:49,594:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:49,594:INFO:Creating metrics dataframe
2023-11-03 15:02:49,605:INFO:Initializing Huber Regressor
2023-11-03 15:02:49,606:INFO:Total runtime is 0.632252283891042 minutes
2023-11-03 15:02:49,609:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:49,609:INFO:Initializing create_model()
2023-11-03 15:02:49,610:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:49,610:INFO:Checking exceptions
2023-11-03 15:02:49,610:INFO:Importing libraries
2023-11-03 15:02:49,610:INFO:Copying training dataset
2023-11-03 15:02:49,635:INFO:Defining folds
2023-11-03 15:02:49,636:INFO:Declaring metric variables
2023-11-03 15:02:49,645:INFO:Importing untrained model
2023-11-03 15:02:49,652:INFO:Huber Regressor Imported successfully
2023-11-03 15:02:49,666:INFO:Starting cross validation
2023-11-03 15:02:49,668:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:52,201:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,380:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,406:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,580:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,639:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,675:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:53,539:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:53,564:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:53,704:INFO:Calculating mean and std
2023-11-03 15:02:53,707:INFO:Creating metrics dataframe
2023-11-03 15:02:53,713:INFO:Uploading results into container
2023-11-03 15:02:53,714:INFO:Uploading model into container now
2023-11-03 15:02:53,715:INFO:_master_model_container: 10
2023-11-03 15:02:53,715:INFO:_display_container: 2
2023-11-03 15:02:53,715:INFO:HuberRegressor()
2023-11-03 15:02:53,716:INFO:create_model() successfully completed......................................
2023-11-03 15:02:53,921:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:53,922:INFO:Creating metrics dataframe
2023-11-03 15:02:53,944:INFO:Initializing K Neighbors Regressor
2023-11-03 15:02:53,944:INFO:Total runtime is 0.7045547644297282 minutes
2023-11-03 15:02:53,949:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:53,950:INFO:Initializing create_model()
2023-11-03 15:02:53,950:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:53,951:INFO:Checking exceptions
2023-11-03 15:02:53,951:INFO:Importing libraries
2023-11-03 15:02:53,952:INFO:Copying training dataset
2023-11-03 15:02:53,982:INFO:Defining folds
2023-11-03 15:02:53,982:INFO:Declaring metric variables
2023-11-03 15:02:53,993:INFO:Importing untrained model
2023-11-03 15:02:54,004:INFO:K Neighbors Regressor Imported successfully
2023-11-03 15:02:54,021:INFO:Starting cross validation
2023-11-03 15:02:54,026:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:56,461:INFO:Calculating mean and std
2023-11-03 15:02:56,468:INFO:Creating metrics dataframe
2023-11-03 15:02:56,480:INFO:Uploading results into container
2023-11-03 15:02:56,482:INFO:Uploading model into container now
2023-11-03 15:02:56,483:INFO:_master_model_container: 11
2023-11-03 15:02:56,484:INFO:_display_container: 2
2023-11-03 15:02:56,484:INFO:KNeighborsRegressor(n_jobs=-1)
2023-11-03 15:02:56,484:INFO:create_model() successfully completed......................................
2023-11-03 15:02:56,691:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:56,691:INFO:Creating metrics dataframe
2023-11-03 15:02:56,715:INFO:Initializing Decision Tree Regressor
2023-11-03 15:02:56,715:INFO:Total runtime is 0.7507464488347372 minutes
2023-11-03 15:02:56,723:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:56,724:INFO:Initializing create_model()
2023-11-03 15:02:56,724:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:56,725:INFO:Checking exceptions
2023-11-03 15:02:56,725:INFO:Importing libraries
2023-11-03 15:02:56,726:INFO:Copying training dataset
2023-11-03 15:02:56,762:INFO:Defining folds
2023-11-03 15:02:56,763:INFO:Declaring metric variables
2023-11-03 15:02:56,770:INFO:Importing untrained model
2023-11-03 15:02:56,780:INFO:Decision Tree Regressor Imported successfully
2023-11-03 15:02:56,798:INFO:Starting cross validation
2023-11-03 15:02:56,800:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:03:00,012:INFO:Calculating mean and std
2023-11-03 15:03:00,015:INFO:Creating metrics dataframe
2023-11-03 15:03:00,022:INFO:Uploading results into container
2023-11-03 15:03:00,024:INFO:Uploading model into container now
2023-11-03 15:03:00,026:INFO:_master_model_container: 12
2023-11-03 15:03:00,026:INFO:_display_container: 2
2023-11-03 15:03:00,027:INFO:DecisionTreeRegressor(random_state=123)
2023-11-03 15:03:00,027:INFO:create_model() successfully completed......................................
2023-11-03 15:03:00,232:INFO:SubProcess create_model() end ==================================
2023-11-03 15:03:00,233:INFO:Creating metrics dataframe
2023-11-03 15:03:00,261:INFO:Initializing Random Forest Regressor
2023-11-03 15:03:00,261:INFO:Total runtime is 0.8098389863967896 minutes
2023-11-03 15:03:00,271:INFO:SubProcess create_model() called ==================================
2023-11-03 15:03:00,272:INFO:Initializing create_model()
2023-11-03 15:03:00,272:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:03:00,273:INFO:Checking exceptions
2023-11-03 15:03:00,274:INFO:Importing libraries
2023-11-03 15:03:00,274:INFO:Copying training dataset
2023-11-03 15:03:00,338:INFO:Defining folds
2023-11-03 15:03:00,339:INFO:Declaring metric variables
2023-11-03 15:03:00,368:INFO:Importing untrained model
2023-11-03 15:03:00,398:INFO:Random Forest Regressor Imported successfully
2023-11-03 15:03:00,430:INFO:Starting cross validation
2023-11-03 15:03:00,434:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:05:22,689:INFO:Calculating mean and std
2023-11-03 15:05:22,706:INFO:Creating metrics dataframe
2023-11-03 15:05:22,729:INFO:Uploading results into container
2023-11-03 15:05:22,731:INFO:Uploading model into container now
2023-11-03 15:05:22,734:INFO:_master_model_container: 13
2023-11-03 15:05:22,734:INFO:_display_container: 2
2023-11-03 15:05:22,737:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:05:22,738:INFO:create_model() successfully completed......................................
2023-11-03 15:05:23,309:INFO:SubProcess create_model() end ==================================
2023-11-03 15:05:23,310:INFO:Creating metrics dataframe
2023-11-03 15:05:23,329:INFO:Initializing Extra Trees Regressor
2023-11-03 15:05:23,329:INFO:Total runtime is 3.194313100973765 minutes
2023-11-03 15:05:23,336:INFO:SubProcess create_model() called ==================================
2023-11-03 15:05:23,336:INFO:Initializing create_model()
2023-11-03 15:05:23,337:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:05:23,337:INFO:Checking exceptions
2023-11-03 15:05:23,337:INFO:Importing libraries
2023-11-03 15:05:23,337:INFO:Copying training dataset
2023-11-03 15:05:23,376:INFO:Defining folds
2023-11-03 15:05:23,376:INFO:Declaring metric variables
2023-11-03 15:05:23,382:INFO:Importing untrained model
2023-11-03 15:05:23,389:INFO:Extra Trees Regressor Imported successfully
2023-11-03 15:05:23,401:INFO:Starting cross validation
2023-11-03 15:05:23,403:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:06:03,022:INFO:Calculating mean and std
2023-11-03 15:06:03,027:INFO:Creating metrics dataframe
2023-11-03 15:06:03,033:INFO:Uploading results into container
2023-11-03 15:06:03,034:INFO:Uploading model into container now
2023-11-03 15:06:03,035:INFO:_master_model_container: 14
2023-11-03 15:06:03,035:INFO:_display_container: 2
2023-11-03 15:06:03,036:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:06:03,036:INFO:create_model() successfully completed......................................
2023-11-03 15:06:03,198:INFO:SubProcess create_model() end ==================================
2023-11-03 15:06:03,198:INFO:Creating metrics dataframe
2023-11-03 15:06:03,214:INFO:Initializing AdaBoost Regressor
2023-11-03 15:06:03,214:INFO:Total runtime is 3.8590561151504517 minutes
2023-11-03 15:06:03,218:INFO:SubProcess create_model() called ==================================
2023-11-03 15:06:03,218:INFO:Initializing create_model()
2023-11-03 15:06:03,218:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:06:03,218:INFO:Checking exceptions
2023-11-03 15:06:03,218:INFO:Importing libraries
2023-11-03 15:06:03,218:INFO:Copying training dataset
2023-11-03 15:06:03,239:INFO:Defining folds
2023-11-03 15:06:03,239:INFO:Declaring metric variables
2023-11-03 15:06:03,244:INFO:Importing untrained model
2023-11-03 15:06:03,251:INFO:AdaBoost Regressor Imported successfully
2023-11-03 15:06:03,264:INFO:Starting cross validation
2023-11-03 15:06:03,266:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:06:22,693:INFO:Calculating mean and std
2023-11-03 15:06:22,698:INFO:Creating metrics dataframe
2023-11-03 15:06:22,706:INFO:Uploading results into container
2023-11-03 15:06:22,707:INFO:Uploading model into container now
2023-11-03 15:06:22,708:INFO:_master_model_container: 15
2023-11-03 15:06:22,708:INFO:_display_container: 2
2023-11-03 15:06:22,708:INFO:AdaBoostRegressor(random_state=123)
2023-11-03 15:06:22,709:INFO:create_model() successfully completed......................................
2023-11-03 15:06:22,876:INFO:SubProcess create_model() end ==================================
2023-11-03 15:06:22,876:INFO:Creating metrics dataframe
2023-11-03 15:06:22,892:INFO:Initializing Gradient Boosting Regressor
2023-11-03 15:06:22,892:INFO:Total runtime is 4.187024199962616 minutes
2023-11-03 15:06:22,897:INFO:SubProcess create_model() called ==================================
2023-11-03 15:06:22,897:INFO:Initializing create_model()
2023-11-03 15:06:22,897:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:06:22,897:INFO:Checking exceptions
2023-11-03 15:06:22,898:INFO:Importing libraries
2023-11-03 15:06:22,898:INFO:Copying training dataset
2023-11-03 15:06:22,918:INFO:Defining folds
2023-11-03 15:06:22,918:INFO:Declaring metric variables
2023-11-03 15:06:22,923:INFO:Importing untrained model
2023-11-03 15:06:22,928:INFO:Gradient Boosting Regressor Imported successfully
2023-11-03 15:06:22,938:INFO:Starting cross validation
2023-11-03 15:06:22,940:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:07:15,296:INFO:Calculating mean and std
2023-11-03 15:07:15,302:INFO:Creating metrics dataframe
2023-11-03 15:07:15,311:INFO:Uploading results into container
2023-11-03 15:07:15,313:INFO:Uploading model into container now
2023-11-03 15:07:15,314:INFO:_master_model_container: 16
2023-11-03 15:07:15,315:INFO:_display_container: 2
2023-11-03 15:07:15,316:INFO:GradientBoostingRegressor(random_state=123)
2023-11-03 15:07:15,317:INFO:create_model() successfully completed......................................
2023-11-03 15:07:15,481:INFO:SubProcess create_model() end ==================================
2023-11-03 15:07:15,482:INFO:Creating metrics dataframe
2023-11-03 15:07:15,495:INFO:Initializing Extreme Gradient Boosting
2023-11-03 15:07:15,495:INFO:Total runtime is 5.063737984498342 minutes
2023-11-03 15:07:15,499:INFO:SubProcess create_model() called ==================================
2023-11-03 15:07:15,499:INFO:Initializing create_model()
2023-11-03 15:07:15,499:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:07:15,499:INFO:Checking exceptions
2023-11-03 15:07:15,500:INFO:Importing libraries
2023-11-03 15:07:15,500:INFO:Copying training dataset
2023-11-03 15:07:15,520:INFO:Defining folds
2023-11-03 15:07:15,520:INFO:Declaring metric variables
2023-11-03 15:07:15,524:INFO:Importing untrained model
2023-11-03 15:07:15,529:INFO:Extreme Gradient Boosting Imported successfully
2023-11-03 15:07:15,539:INFO:Starting cross validation
2023-11-03 15:07:15,541:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:07:22,484:INFO:Calculating mean and std
2023-11-03 15:07:22,489:INFO:Creating metrics dataframe
2023-11-03 15:07:22,496:INFO:Uploading results into container
2023-11-03 15:07:22,497:INFO:Uploading model into container now
2023-11-03 15:07:22,498:INFO:_master_model_container: 17
2023-11-03 15:07:22,498:INFO:_display_container: 2
2023-11-03 15:07:22,499:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-03 15:07:22,499:INFO:create_model() successfully completed......................................
2023-11-03 15:07:22,679:INFO:SubProcess create_model() end ==================================
2023-11-03 15:07:22,679:INFO:Creating metrics dataframe
2023-11-03 15:07:22,695:INFO:Initializing Light Gradient Boosting Machine
2023-11-03 15:07:22,696:INFO:Total runtime is 5.183753514289856 minutes
2023-11-03 15:07:22,702:INFO:SubProcess create_model() called ==================================
2023-11-03 15:07:22,702:INFO:Initializing create_model()
2023-11-03 15:07:22,703:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:07:22,703:INFO:Checking exceptions
2023-11-03 15:07:22,703:INFO:Importing libraries
2023-11-03 15:07:22,703:INFO:Copying training dataset
2023-11-03 15:07:22,727:INFO:Defining folds
2023-11-03 15:07:22,727:INFO:Declaring metric variables
2023-11-03 15:07:22,733:INFO:Importing untrained model
2023-11-03 15:07:22,741:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 15:07:22,752:INFO:Starting cross validation
2023-11-03 15:07:22,756:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:07:42,307:INFO:Calculating mean and std
2023-11-03 15:07:42,321:INFO:Creating metrics dataframe
2023-11-03 15:07:42,345:INFO:Uploading results into container
2023-11-03 15:07:42,347:INFO:Uploading model into container now
2023-11-03 15:07:42,351:INFO:_master_model_container: 18
2023-11-03 15:07:42,351:INFO:_display_container: 2
2023-11-03 15:07:42,356:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:07:42,356:INFO:create_model() successfully completed......................................
2023-11-03 15:07:42,622:INFO:SubProcess create_model() end ==================================
2023-11-03 15:07:42,622:INFO:Creating metrics dataframe
2023-11-03 15:07:42,636:INFO:Initializing Dummy Regressor
2023-11-03 15:07:42,636:INFO:Total runtime is 5.516091783841452 minutes
2023-11-03 15:07:42,640:INFO:SubProcess create_model() called ==================================
2023-11-03 15:07:42,640:INFO:Initializing create_model()
2023-11-03 15:07:42,640:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:07:42,640:INFO:Checking exceptions
2023-11-03 15:07:42,640:INFO:Importing libraries
2023-11-03 15:07:42,641:INFO:Copying training dataset
2023-11-03 15:07:42,659:INFO:Defining folds
2023-11-03 15:07:42,660:INFO:Declaring metric variables
2023-11-03 15:07:42,665:INFO:Importing untrained model
2023-11-03 15:07:42,670:INFO:Dummy Regressor Imported successfully
2023-11-03 15:07:42,680:INFO:Starting cross validation
2023-11-03 15:07:42,681:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:07:43,233:INFO:Calculating mean and std
2023-11-03 15:07:43,238:INFO:Creating metrics dataframe
2023-11-03 15:07:43,245:INFO:Uploading results into container
2023-11-03 15:07:43,247:INFO:Uploading model into container now
2023-11-03 15:07:43,248:INFO:_master_model_container: 19
2023-11-03 15:07:43,248:INFO:_display_container: 2
2023-11-03 15:07:43,249:INFO:DummyRegressor()
2023-11-03 15:07:43,249:INFO:create_model() successfully completed......................................
2023-11-03 15:07:43,415:INFO:SubProcess create_model() end ==================================
2023-11-03 15:07:43,415:INFO:Creating metrics dataframe
2023-11-03 15:07:43,440:INFO:Initializing create_model()
2023-11-03 15:07:43,440:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:07:43,440:INFO:Checking exceptions
2023-11-03 15:07:43,443:INFO:Importing libraries
2023-11-03 15:07:43,443:INFO:Copying training dataset
2023-11-03 15:07:43,461:INFO:Defining folds
2023-11-03 15:07:43,462:INFO:Declaring metric variables
2023-11-03 15:07:43,462:INFO:Importing untrained model
2023-11-03 15:07:43,462:INFO:Declaring custom model
2023-11-03 15:07:43,463:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 15:07:43,464:INFO:Cross validation set to False
2023-11-03 15:07:43,464:INFO:Fitting Model
2023-11-03 15:07:43,598:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008414 seconds.
2023-11-03 15:07:43,599:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-03 15:07:43,599:INFO:[LightGBM] [Info] Total Bins 9798
2023-11-03 15:07:43,601:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 58
2023-11-03 15:07:43,602:INFO:[LightGBM] [Info] Start training from score 77.068662
2023-11-03 15:07:43,941:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:07:43,941:INFO:create_model() successfully completed......................................
2023-11-03 15:07:44,210:INFO:_master_model_container: 19
2023-11-03 15:07:44,210:INFO:_display_container: 2
2023-11-03 15:07:44,211:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:07:44,211:INFO:compare_models() successfully completed......................................
2023-11-03 15:07:44,307:INFO:Initializing evaluate_model()
2023-11-03 15:07:44,308:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-03 15:07:44,382:INFO:Initializing plot_model()
2023-11-03 15:07:44,382:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, system=True)
2023-11-03 15:07:44,383:INFO:Checking exceptions
2023-11-03 15:07:44,395:INFO:Preloading libraries
2023-11-03 15:07:44,407:INFO:Copying training dataset
2023-11-03 15:07:44,407:INFO:Plot type: pipeline
2023-11-03 15:07:44,733:INFO:Visual Rendered Successfully
2023-11-03 15:07:44,917:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:44,922:INFO:Initializing evaluate_model()
2023-11-03 15:07:44,922:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-03 15:07:44,955:INFO:Initializing plot_model()
2023-11-03 15:07:44,955:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, system=True)
2023-11-03 15:07:44,955:INFO:Checking exceptions
2023-11-03 15:07:44,962:INFO:Preloading libraries
2023-11-03 15:07:44,966:INFO:Copying training dataset
2023-11-03 15:07:44,966:INFO:Plot type: pipeline
2023-11-03 15:07:45,179:INFO:Visual Rendered Successfully
2023-11-03 15:07:45,353:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:45,358:INFO:Initializing evaluate_model()
2023-11-03 15:07:45,359:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-03 15:07:45,380:INFO:Initializing plot_model()
2023-11-03 15:07:45,380:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, system=True)
2023-11-03 15:07:45,380:INFO:Checking exceptions
2023-11-03 15:07:45,386:INFO:Preloading libraries
2023-11-03 15:07:45,390:INFO:Copying training dataset
2023-11-03 15:07:45,390:INFO:Plot type: pipeline
2023-11-03 15:07:45,606:INFO:Visual Rendered Successfully
2023-11-03 15:07:45,784:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:45,788:INFO:Initializing plot_model()
2023-11-03 15:07:45,789:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, system=True)
2023-11-03 15:07:45,789:INFO:Checking exceptions
2023-11-03 15:07:45,800:INFO:Preloading libraries
2023-11-03 15:07:45,805:INFO:Copying training dataset
2023-11-03 15:07:45,805:INFO:Plot type: feature
2023-11-03 15:07:45,806:WARNING:No coef_ found. Trying feature_importances_
2023-11-03 15:07:46,136:INFO:Visual Rendered Successfully
2023-11-03 15:07:46,294:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:46,294:INFO:Initializing plot_model()
2023-11-03 15:07:46,294:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, system=True)
2023-11-03 15:07:46,294:INFO:Checking exceptions
2023-11-03 15:07:46,307:INFO:Preloading libraries
2023-11-03 15:07:46,311:INFO:Copying training dataset
2023-11-03 15:07:46,311:INFO:Plot type: feature
2023-11-03 15:07:46,312:WARNING:No coef_ found. Trying feature_importances_
2023-11-03 15:07:46,618:INFO:Visual Rendered Successfully
2023-11-03 15:07:46,775:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:46,776:INFO:Initializing plot_model()
2023-11-03 15:07:46,776:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, system=True)
2023-11-03 15:07:46,776:INFO:Checking exceptions
2023-11-03 15:07:46,789:INFO:Preloading libraries
2023-11-03 15:07:46,793:INFO:Copying training dataset
2023-11-03 15:07:46,793:INFO:Plot type: feature
2023-11-03 15:07:46,794:WARNING:No coef_ found. Trying feature_importances_
2023-11-03 15:07:47,245:INFO:Visual Rendered Successfully
2023-11-03 15:07:47,402:INFO:plot_model() successfully completed......................................
2023-11-03 15:10:32,033:INFO:Initializing predict_model()
2023-11-03 15:10:32,036:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fc9ea8bb040>)
2023-11-03 15:10:32,037:INFO:Checking exceptions
2023-11-03 15:10:32,038:INFO:Preloading libraries
2023-11-03 15:10:32,069:INFO:Set up data.
2023-11-03 15:10:32,178:INFO:Set up index.
2023-11-03 15:10:33,872:INFO:Initializing predict_model()
2023-11-03 15:10:33,892:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fc9ea1a5af0>)
2023-11-03 15:10:33,892:INFO:Checking exceptions
2023-11-03 15:10:33,892:INFO:Preloading libraries
2023-11-03 15:10:33,895:INFO:Set up data.
2023-11-03 15:10:33,942:INFO:Set up index.
2023-11-03 15:10:34,431:INFO:Initializing predict_model()
2023-11-03 15:10:34,431:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fc9ea1a5af0>)
2023-11-03 15:10:34,431:INFO:Checking exceptions
2023-11-03 15:10:34,431:INFO:Preloading libraries
2023-11-03 15:10:34,435:INFO:Set up data.
2023-11-03 15:10:34,524:INFO:Set up index.
