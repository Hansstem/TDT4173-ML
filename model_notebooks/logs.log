2023-11-02 12:48:17,339:INFO:PyCaret RegressionExperiment
2023-11-02 12:48:17,339:INFO:Logging name: reg-default-name
2023-11-02 12:48:17,340:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-02 12:48:17,340:INFO:version 3.1.0
2023-11-02 12:48:17,340:INFO:Initializing setup()
2023-11-02 12:48:17,340:INFO:self.USI: b61b
2023-11-02 12:48:17,340:INFO:self._variable_keys: {'pipeline', 'data', 'log_plots_param', 'n_jobs_param', 'memory', 'X_test', 'seed', 'X', 'X_train', '_available_plots', 'exp_name_log', 'USI', 'target_param', 'html_param', 'gpu_n_jobs_param', '_ml_usecase', 'fold_shuffle_param', 'gpu_param', 'fold_groups_param', 'y', 'y_train', 'logging_param', 'fold_generator', 'idx', 'y_test', 'transform_target_param', 'exp_id'}
2023-11-02 12:48:17,340:INFO:Checking environment
2023-11-02 12:48:17,341:INFO:python_version: 3.9.6
2023-11-02 12:48:17,341:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-02 12:48:17,341:INFO:machine: x86_64
2023-11-02 12:48:17,341:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-02 12:48:17,342:INFO:Memory: svmem(total=8589934592, available=2813247488, percent=67.2, used=5199163392, free=64573440, active=2753372160, inactive=2733961216, wired=2445791232)
2023-11-02 12:48:17,342:INFO:Physical Core: 4
2023-11-02 12:48:17,342:INFO:Logical Core: 8
2023-11-02 12:48:17,342:INFO:Checking libraries
2023-11-02 12:48:17,342:INFO:System:
2023-11-02 12:48:17,342:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-02 12:48:17,342:INFO:executable: /usr/local/bin/python3.9
2023-11-02 12:48:17,342:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-02 12:48:17,342:INFO:PyCaret required dependencies:
2023-11-02 12:48:17,343:INFO:                 pip: 23.3.1
2023-11-02 12:48:17,343:INFO:          setuptools: 56.0.0
2023-11-02 12:48:17,343:INFO:             pycaret: 3.1.0
2023-11-02 12:48:17,343:INFO:             IPython: 7.28.0
2023-11-02 12:48:17,343:INFO:          ipywidgets: 8.1.1
2023-11-02 12:48:17,343:INFO:                tqdm: 4.66.1
2023-11-02 12:48:17,343:INFO:               numpy: 1.23.5
2023-11-02 12:48:17,343:INFO:              pandas: 1.5.3
2023-11-02 12:48:17,343:INFO:              jinja2: 3.0.1
2023-11-02 12:48:17,343:INFO:               scipy: 1.10.1
2023-11-02 12:48:17,343:INFO:              joblib: 1.3.2
2023-11-02 12:48:17,343:INFO:             sklearn: 1.1.3
2023-11-02 12:48:17,343:INFO:                pyod: 1.1.1
2023-11-02 12:48:17,343:INFO:            imblearn: 0.11.0
2023-11-02 12:48:17,343:INFO:   category_encoders: 2.6.3
2023-11-02 12:48:17,343:INFO:            lightgbm: 4.1.0
2023-11-02 12:48:17,343:INFO:               numba: 0.58.1
2023-11-02 12:48:17,343:INFO:            requests: 2.31.0
2023-11-02 12:48:17,343:INFO:          matplotlib: 3.4.2
2023-11-02 12:48:17,343:INFO:          scikitplot: 0.3.7
2023-11-02 12:48:17,343:INFO:         yellowbrick: 1.5
2023-11-02 12:48:17,343:INFO:              plotly: 5.18.0
2023-11-02 12:48:17,343:INFO:    plotly-resampler: Not installed
2023-11-02 12:48:17,343:INFO:             kaleido: 0.2.1
2023-11-02 12:48:17,344:INFO:           schemdraw: 0.15
2023-11-02 12:48:17,344:INFO:         statsmodels: 0.14.0
2023-11-02 12:48:17,344:INFO:              sktime: 0.21.1
2023-11-02 12:48:17,344:INFO:               tbats: 1.1.3
2023-11-02 12:48:17,344:INFO:            pmdarima: 2.0.4
2023-11-02 12:48:17,344:INFO:              psutil: 5.9.6
2023-11-02 12:48:17,344:INFO:          markupsafe: 2.1.3
2023-11-02 12:48:17,344:INFO:             pickle5: Not installed
2023-11-02 12:48:17,344:INFO:         cloudpickle: 2.2.1
2023-11-02 12:48:17,344:INFO:         deprecation: 2.1.0
2023-11-02 12:48:17,344:INFO:              xxhash: 3.4.1
2023-11-02 12:48:17,344:INFO:           wurlitzer: 3.0.3
2023-11-02 12:48:17,344:INFO:PyCaret optional dependencies:
2023-11-02 12:48:17,344:INFO:                shap: Not installed
2023-11-02 12:48:17,344:INFO:           interpret: Not installed
2023-11-02 12:48:17,344:INFO:                umap: Not installed
2023-11-02 12:48:17,344:INFO:     ydata_profiling: Not installed
2023-11-02 12:48:17,344:INFO:  explainerdashboard: Not installed
2023-11-02 12:48:17,344:INFO:             autoviz: Not installed
2023-11-02 12:48:17,344:INFO:           fairlearn: Not installed
2023-11-02 12:48:17,344:INFO:          deepchecks: Not installed
2023-11-02 12:48:17,344:INFO:             xgboost: 2.0.0
2023-11-02 12:48:17,345:INFO:            catboost: Not installed
2023-11-02 12:48:17,345:INFO:              kmodes: Not installed
2023-11-02 12:48:17,345:INFO:             mlxtend: Not installed
2023-11-02 12:48:17,345:INFO:       statsforecast: Not installed
2023-11-02 12:48:17,345:INFO:        tune_sklearn: Not installed
2023-11-02 12:48:17,345:INFO:                 ray: Not installed
2023-11-02 12:48:17,345:INFO:            hyperopt: 0.2.7
2023-11-02 12:48:17,345:INFO:              optuna: 3.4.0
2023-11-02 12:48:17,345:INFO:               skopt: Not installed
2023-11-02 12:48:17,345:INFO:              mlflow: Not installed
2023-11-02 12:48:17,345:INFO:              gradio: Not installed
2023-11-02 12:48:17,345:INFO:             fastapi: Not installed
2023-11-02 12:48:17,345:INFO:             uvicorn: Not installed
2023-11-02 12:48:17,345:INFO:              m2cgen: Not installed
2023-11-02 12:48:17,345:INFO:           evidently: Not installed
2023-11-02 12:48:17,345:INFO:               fugue: Not installed
2023-11-02 12:48:17,345:INFO:           streamlit: Not installed
2023-11-02 12:48:17,345:INFO:             prophet: Not installed
2023-11-02 12:48:17,345:INFO:None
2023-11-02 12:48:17,345:INFO:Set up data.
2023-11-02 12:48:17,444:INFO:Set up folding strategy.
2023-11-02 12:48:17,444:INFO:Set up train/test split.
2023-11-02 12:48:17,546:INFO:Set up index.
2023-11-02 12:48:17,552:INFO:Assigning column types.
2023-11-02 12:48:17,603:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-02 12:48:17,605:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,612:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,618:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,743:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,819:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,823:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:17,829:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:17,830:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,837:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,844:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:17,959:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,018:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,019:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:18,024:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:18,024:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-02 12:48:18,031:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,037:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,156:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,236:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,238:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:18,247:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:18,301:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,373:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,624:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,708:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,709:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:18,713:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:18,714:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-02 12:48:18,731:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,876:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,944:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:18,945:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:18,949:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:18,963:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,096:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,177:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,178:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:19,182:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:19,183:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-02 12:48:19,310:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,373:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,373:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:19,377:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:19,528:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,601:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,602:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:19,606:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:19,607:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-02 12:48:19,770:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:19,845:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:19,848:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:19,970:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-02 12:48:20,039:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:20,045:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:20,046:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-02 12:48:20,258:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:20,262:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:20,485:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:20,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:20,494:INFO:Preparing preprocessing pipeline...
2023-11-02 12:48:20,494:INFO:Set up simple imputation.
2023-11-02 12:48:20,500:INFO:Set up column name cleaning.
2023-11-02 12:48:20,688:INFO:Finished creating preprocessing pipeline.
2023-11-02 12:48:20,698:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['is_day:idx', 'is_in_shadow:idx',
                                             'absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'di...
                                             'pressure_50m:hPa', 'prob_rime:p',
                                             'rain_water:kgm2',
                                             'relative_humidity_1000hPa:p',
                                             'sfc_pressure:hPa',
                                             'snow_depth:cm', 'snow_drift:idx',
                                             'snow_melt_10min:mm', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-02 12:48:20,698:INFO:Creating final display dataframe.
2023-11-02 12:48:21,357:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    pv_measurement
2                   Target type        Regression
3           Original data shape       (89684, 57)
4        Transformed data shape       (89684, 57)
5   Transformed train set shape       (62778, 57)
6    Transformed test set shape       (26906, 57)
7              Numeric features                56
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              b61b
2023-11-02 12:48:21,681:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:21,688:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:21,950:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-02 12:48:21,954:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-02 12:48:21,955:INFO:setup() successfully completed in 4.62s...............
2023-11-02 12:48:22,111:INFO:Initializing compare_models()
2023-11-02 12:48:22,111:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-11-02 12:48:22,112:INFO:Checking exceptions
2023-11-02 12:48:22,153:INFO:Preparing display monitor
2023-11-02 12:48:22,275:INFO:Initializing Linear Regression
2023-11-02 12:48:22,276:INFO:Total runtime is 9.453296661376953e-06 minutes
2023-11-02 12:48:22,289:INFO:SubProcess create_model() called ==================================
2023-11-02 12:48:22,290:INFO:Initializing create_model()
2023-11-02 12:48:22,291:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:48:22,291:INFO:Checking exceptions
2023-11-02 12:48:22,291:INFO:Importing libraries
2023-11-02 12:48:22,291:INFO:Copying training dataset
2023-11-02 12:48:22,364:INFO:Defining folds
2023-11-02 12:48:22,364:INFO:Declaring metric variables
2023-11-02 12:48:22,373:INFO:Importing untrained model
2023-11-02 12:48:22,384:INFO:Linear Regression Imported successfully
2023-11-02 12:48:22,420:INFO:Starting cross validation
2023-11-02 12:48:22,423:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:48:36,812:INFO:Calculating mean and std
2023-11-02 12:48:36,822:INFO:Creating metrics dataframe
2023-11-02 12:48:36,848:INFO:Uploading results into container
2023-11-02 12:48:36,850:INFO:Uploading model into container now
2023-11-02 12:48:36,852:INFO:_master_model_container: 1
2023-11-02 12:48:36,852:INFO:_display_container: 2
2023-11-02 12:48:36,853:INFO:LinearRegression(n_jobs=-1)
2023-11-02 12:48:36,853:INFO:create_model() successfully completed......................................
2023-11-02 12:48:37,415:INFO:SubProcess create_model() end ==================================
2023-11-02 12:48:37,415:INFO:Creating metrics dataframe
2023-11-02 12:48:37,428:INFO:Initializing Lasso Regression
2023-11-02 12:48:37,429:INFO:Total runtime is 0.25255595048268636 minutes
2023-11-02 12:48:37,433:INFO:SubProcess create_model() called ==================================
2023-11-02 12:48:37,433:INFO:Initializing create_model()
2023-11-02 12:48:37,434:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:48:37,434:INFO:Checking exceptions
2023-11-02 12:48:37,434:INFO:Importing libraries
2023-11-02 12:48:37,434:INFO:Copying training dataset
2023-11-02 12:48:37,510:INFO:Defining folds
2023-11-02 12:48:37,510:INFO:Declaring metric variables
2023-11-02 12:48:37,515:INFO:Importing untrained model
2023-11-02 12:48:37,521:INFO:Lasso Regression Imported successfully
2023-11-02 12:48:37,532:INFO:Starting cross validation
2023-11-02 12:48:37,534:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:48:46,649:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.600e+09, tolerance: 3.438e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:46,764:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.528e+09, tolerance: 3.416e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:46,799:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.607e+09, tolerance: 3.432e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:46,819:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.522e+09, tolerance: 3.397e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:46,949:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.537e+09, tolerance: 3.422e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:47,044:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.570e+09, tolerance: 3.437e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:47,062:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.577e+09, tolerance: 3.429e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:47,071:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.532e+09, tolerance: 3.415e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:49,177:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.550e+09, tolerance: 3.415e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:49,271:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.623e+09, tolerance: 3.442e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:48:49,418:INFO:Calculating mean and std
2023-11-02 12:48:49,421:INFO:Creating metrics dataframe
2023-11-02 12:48:49,428:INFO:Uploading results into container
2023-11-02 12:48:49,430:INFO:Uploading model into container now
2023-11-02 12:48:49,430:INFO:_master_model_container: 2
2023-11-02 12:48:49,431:INFO:_display_container: 2
2023-11-02 12:48:49,431:INFO:Lasso(random_state=123)
2023-11-02 12:48:49,431:INFO:create_model() successfully completed......................................
2023-11-02 12:48:49,734:INFO:SubProcess create_model() end ==================================
2023-11-02 12:48:49,734:INFO:Creating metrics dataframe
2023-11-02 12:48:49,745:INFO:Initializing Ridge Regression
2023-11-02 12:48:49,746:INFO:Total runtime is 0.4578392028808594 minutes
2023-11-02 12:48:49,750:INFO:SubProcess create_model() called ==================================
2023-11-02 12:48:49,751:INFO:Initializing create_model()
2023-11-02 12:48:49,751:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:48:49,751:INFO:Checking exceptions
2023-11-02 12:48:49,751:INFO:Importing libraries
2023-11-02 12:48:49,751:INFO:Copying training dataset
2023-11-02 12:48:49,807:INFO:Defining folds
2023-11-02 12:48:49,807:INFO:Declaring metric variables
2023-11-02 12:48:49,811:INFO:Importing untrained model
2023-11-02 12:48:49,821:INFO:Ridge Regression Imported successfully
2023-11-02 12:48:49,830:INFO:Starting cross validation
2023-11-02 12:48:49,833:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:48:50,241:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.62328e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,242:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.64158e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,305:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.62804e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,357:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.63451e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,414:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.61666e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,468:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.57377e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,517:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.63188e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,540:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.63479e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,669:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.60506e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,686:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.63581e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-02 12:48:50,830:INFO:Calculating mean and std
2023-11-02 12:48:50,836:INFO:Creating metrics dataframe
2023-11-02 12:48:50,844:INFO:Uploading results into container
2023-11-02 12:48:50,845:INFO:Uploading model into container now
2023-11-02 12:48:50,848:INFO:_master_model_container: 3
2023-11-02 12:48:50,848:INFO:_display_container: 2
2023-11-02 12:48:50,849:INFO:Ridge(random_state=123)
2023-11-02 12:48:50,850:INFO:create_model() successfully completed......................................
2023-11-02 12:48:51,203:INFO:SubProcess create_model() end ==================================
2023-11-02 12:48:51,204:INFO:Creating metrics dataframe
2023-11-02 12:48:51,235:INFO:Initializing Elastic Net
2023-11-02 12:48:51,236:INFO:Total runtime is 0.4826743364334107 minutes
2023-11-02 12:48:51,244:INFO:SubProcess create_model() called ==================================
2023-11-02 12:48:51,245:INFO:Initializing create_model()
2023-11-02 12:48:51,245:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:48:51,245:INFO:Checking exceptions
2023-11-02 12:48:51,245:INFO:Importing libraries
2023-11-02 12:48:51,245:INFO:Copying training dataset
2023-11-02 12:48:51,323:INFO:Defining folds
2023-11-02 12:48:51,324:INFO:Declaring metric variables
2023-11-02 12:48:51,329:INFO:Importing untrained model
2023-11-02 12:48:51,336:INFO:Elastic Net Imported successfully
2023-11-02 12:48:51,346:INFO:Starting cross validation
2023-11-02 12:48:51,349:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:03,192:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.431e+09, tolerance: 3.397e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,285:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.428e+09, tolerance: 3.416e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,332:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.520e+09, tolerance: 3.438e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,476:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.529e+09, tolerance: 3.432e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,694:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.448e+09, tolerance: 3.422e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,754:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.486e+09, tolerance: 3.429e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,761:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.482e+09, tolerance: 3.437e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:03,762:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.444e+09, tolerance: 3.415e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:06,310:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.545e+09, tolerance: 3.442e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:06,403:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.466e+09, tolerance: 3.415e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-02 12:49:06,548:INFO:Calculating mean and std
2023-11-02 12:49:06,552:INFO:Creating metrics dataframe
2023-11-02 12:49:06,561:INFO:Uploading results into container
2023-11-02 12:49:06,563:INFO:Uploading model into container now
2023-11-02 12:49:06,564:INFO:_master_model_container: 4
2023-11-02 12:49:06,565:INFO:_display_container: 2
2023-11-02 12:49:06,565:INFO:ElasticNet(random_state=123)
2023-11-02 12:49:06,566:INFO:create_model() successfully completed......................................
2023-11-02 12:49:06,879:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:06,880:INFO:Creating metrics dataframe
2023-11-02 12:49:06,890:INFO:Initializing Least Angle Regression
2023-11-02 12:49:06,891:INFO:Total runtime is 0.7435892184575399 minutes
2023-11-02 12:49:06,894:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:06,895:INFO:Initializing create_model()
2023-11-02 12:49:06,895:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:06,895:INFO:Checking exceptions
2023-11-02 12:49:06,895:INFO:Importing libraries
2023-11-02 12:49:06,895:INFO:Copying training dataset
2023-11-02 12:49:06,948:INFO:Defining folds
2023-11-02 12:49:06,948:INFO:Declaring metric variables
2023-11-02 12:49:06,952:INFO:Importing untrained model
2023-11-02 12:49:06,957:INFO:Least Angle Regression Imported successfully
2023-11-02 12:49:06,967:INFO:Starting cross validation
2023-11-02 12:49:06,969:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:07,345:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,345:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,556:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,626:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,642:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,827:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:07,907:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:08,050:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:08,274:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:08,288:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:08,363:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=1.107e-03, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-02 12:49:08,365:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.976e-04, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-02 12:49:08,553:INFO:Calculating mean and std
2023-11-02 12:49:08,558:INFO:Creating metrics dataframe
2023-11-02 12:49:08,565:INFO:Uploading results into container
2023-11-02 12:49:08,566:INFO:Uploading model into container now
2023-11-02 12:49:08,567:INFO:_master_model_container: 5
2023-11-02 12:49:08,567:INFO:_display_container: 2
2023-11-02 12:49:08,569:INFO:Lars(random_state=123)
2023-11-02 12:49:08,569:INFO:create_model() successfully completed......................................
2023-11-02 12:49:08,913:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:08,913:INFO:Creating metrics dataframe
2023-11-02 12:49:08,927:INFO:Initializing Lasso Least Angle Regression
2023-11-02 12:49:08,928:INFO:Total runtime is 0.7775471727053325 minutes
2023-11-02 12:49:08,934:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:08,935:INFO:Initializing create_model()
2023-11-02 12:49:08,935:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:08,935:INFO:Checking exceptions
2023-11-02 12:49:08,935:INFO:Importing libraries
2023-11-02 12:49:08,935:INFO:Copying training dataset
2023-11-02 12:49:09,015:INFO:Defining folds
2023-11-02 12:49:09,016:INFO:Declaring metric variables
2023-11-02 12:49:09,023:INFO:Importing untrained model
2023-11-02 12:49:09,035:INFO:Lasso Least Angle Regression Imported successfully
2023-11-02 12:49:09,051:INFO:Starting cross validation
2023-11-02 12:49:09,053:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:09,495:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,516:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,537:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,639:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,741:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,773:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,794:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:09,822:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:10,145:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:10,178:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-02 12:49:10,432:INFO:Calculating mean and std
2023-11-02 12:49:10,436:INFO:Creating metrics dataframe
2023-11-02 12:49:10,444:INFO:Uploading results into container
2023-11-02 12:49:10,446:INFO:Uploading model into container now
2023-11-02 12:49:10,446:INFO:_master_model_container: 6
2023-11-02 12:49:10,446:INFO:_display_container: 2
2023-11-02 12:49:10,447:INFO:LassoLars(random_state=123)
2023-11-02 12:49:10,447:INFO:create_model() successfully completed......................................
2023-11-02 12:49:10,765:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:10,765:INFO:Creating metrics dataframe
2023-11-02 12:49:10,781:INFO:Initializing Orthogonal Matching Pursuit
2023-11-02 12:49:10,782:INFO:Total runtime is 0.8084387222925822 minutes
2023-11-02 12:49:10,788:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:10,788:INFO:Initializing create_model()
2023-11-02 12:49:10,789:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:10,789:INFO:Checking exceptions
2023-11-02 12:49:10,789:INFO:Importing libraries
2023-11-02 12:49:10,790:INFO:Copying training dataset
2023-11-02 12:49:10,865:INFO:Defining folds
2023-11-02 12:49:10,865:INFO:Declaring metric variables
2023-11-02 12:49:10,872:INFO:Importing untrained model
2023-11-02 12:49:10,881:INFO:Orthogonal Matching Pursuit Imported successfully
2023-11-02 12:49:10,893:INFO:Starting cross validation
2023-11-02 12:49:10,895:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:11,248:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,350:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,365:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,373:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,466:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,523:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,644:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:11,759:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:12,200:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:12,252:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-02 12:49:12,490:INFO:Calculating mean and std
2023-11-02 12:49:12,493:INFO:Creating metrics dataframe
2023-11-02 12:49:12,498:INFO:Uploading results into container
2023-11-02 12:49:12,500:INFO:Uploading model into container now
2023-11-02 12:49:12,501:INFO:_master_model_container: 7
2023-11-02 12:49:12,501:INFO:_display_container: 2
2023-11-02 12:49:12,501:INFO:OrthogonalMatchingPursuit()
2023-11-02 12:49:12,501:INFO:create_model() successfully completed......................................
2023-11-02 12:49:12,809:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:12,810:INFO:Creating metrics dataframe
2023-11-02 12:49:12,829:INFO:Initializing Bayesian Ridge
2023-11-02 12:49:12,829:INFO:Total runtime is 0.8425685365994772 minutes
2023-11-02 12:49:12,836:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:12,836:INFO:Initializing create_model()
2023-11-02 12:49:12,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:12,837:INFO:Checking exceptions
2023-11-02 12:49:12,837:INFO:Importing libraries
2023-11-02 12:49:12,837:INFO:Copying training dataset
2023-11-02 12:49:12,904:INFO:Defining folds
2023-11-02 12:49:12,904:INFO:Declaring metric variables
2023-11-02 12:49:12,911:INFO:Importing untrained model
2023-11-02 12:49:12,915:INFO:Bayesian Ridge Imported successfully
2023-11-02 12:49:12,926:INFO:Starting cross validation
2023-11-02 12:49:12,928:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:15,753:INFO:Calculating mean and std
2023-11-02 12:49:15,757:INFO:Creating metrics dataframe
2023-11-02 12:49:15,765:INFO:Uploading results into container
2023-11-02 12:49:15,767:INFO:Uploading model into container now
2023-11-02 12:49:15,768:INFO:_master_model_container: 8
2023-11-02 12:49:15,768:INFO:_display_container: 2
2023-11-02 12:49:15,769:INFO:BayesianRidge()
2023-11-02 12:49:15,769:INFO:create_model() successfully completed......................................
2023-11-02 12:49:16,072:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:16,072:INFO:Creating metrics dataframe
2023-11-02 12:49:16,087:INFO:Initializing Passive Aggressive Regressor
2023-11-02 12:49:16,087:INFO:Total runtime is 0.8968613346417745 minutes
2023-11-02 12:49:16,091:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:16,092:INFO:Initializing create_model()
2023-11-02 12:49:16,092:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:16,092:INFO:Checking exceptions
2023-11-02 12:49:16,092:INFO:Importing libraries
2023-11-02 12:49:16,092:INFO:Copying training dataset
2023-11-02 12:49:16,153:INFO:Defining folds
2023-11-02 12:49:16,154:INFO:Declaring metric variables
2023-11-02 12:49:16,159:INFO:Importing untrained model
2023-11-02 12:49:16,170:INFO:Passive Aggressive Regressor Imported successfully
2023-11-02 12:49:16,188:INFO:Starting cross validation
2023-11-02 12:49:16,190:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:18,711:INFO:Calculating mean and std
2023-11-02 12:49:18,718:INFO:Creating metrics dataframe
2023-11-02 12:49:18,727:INFO:Uploading results into container
2023-11-02 12:49:18,728:INFO:Uploading model into container now
2023-11-02 12:49:18,730:INFO:_master_model_container: 9
2023-11-02 12:49:18,730:INFO:_display_container: 2
2023-11-02 12:49:18,731:INFO:PassiveAggressiveRegressor(random_state=123)
2023-11-02 12:49:18,731:INFO:create_model() successfully completed......................................
2023-11-02 12:49:19,050:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:19,050:INFO:Creating metrics dataframe
2023-11-02 12:49:19,065:INFO:Initializing Huber Regressor
2023-11-02 12:49:19,065:INFO:Total runtime is 0.9465027372042338 minutes
2023-11-02 12:49:19,071:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:19,071:INFO:Initializing create_model()
2023-11-02 12:49:19,072:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:19,072:INFO:Checking exceptions
2023-11-02 12:49:19,072:INFO:Importing libraries
2023-11-02 12:49:19,072:INFO:Copying training dataset
2023-11-02 12:49:19,143:INFO:Defining folds
2023-11-02 12:49:19,143:INFO:Declaring metric variables
2023-11-02 12:49:19,147:INFO:Importing untrained model
2023-11-02 12:49:19,153:INFO:Huber Regressor Imported successfully
2023-11-02 12:49:19,162:INFO:Starting cross validation
2023-11-02 12:49:19,164:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 12:49:32,289:INFO:Calculating mean and std
2023-11-02 12:49:32,295:INFO:Creating metrics dataframe
2023-11-02 12:49:32,302:INFO:Uploading results into container
2023-11-02 12:49:32,304:INFO:Uploading model into container now
2023-11-02 12:49:32,305:INFO:_master_model_container: 10
2023-11-02 12:49:32,306:INFO:_display_container: 2
2023-11-02 12:49:32,306:INFO:HuberRegressor()
2023-11-02 12:49:32,307:INFO:create_model() successfully completed......................................
2023-11-02 12:49:32,603:INFO:SubProcess create_model() end ==================================
2023-11-02 12:49:32,603:INFO:Creating metrics dataframe
2023-11-02 12:49:32,616:INFO:Initializing K Neighbors Regressor
2023-11-02 12:49:32,617:INFO:Total runtime is 1.172360916932424 minutes
2023-11-02 12:49:32,623:INFO:SubProcess create_model() called ==================================
2023-11-02 12:49:32,624:INFO:Initializing create_model()
2023-11-02 12:49:32,624:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 12:49:32,625:INFO:Checking exceptions
2023-11-02 12:49:32,625:INFO:Importing libraries
2023-11-02 12:49:32,625:INFO:Copying training dataset
2023-11-02 12:49:32,696:INFO:Defining folds
2023-11-02 12:49:32,697:INFO:Declaring metric variables
2023-11-02 12:49:32,703:INFO:Importing untrained model
2023-11-02 12:49:32,723:INFO:K Neighbors Regressor Imported successfully
2023-11-02 12:49:32,733:INFO:Starting cross validation
2023-11-02 12:49:32,735:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:12:44,193:INFO:Calculating mean and std
2023-11-02 13:12:44,543:INFO:Creating metrics dataframe
2023-11-02 13:12:44,767:INFO:Uploading results into container
2023-11-02 13:12:44,778:INFO:Uploading model into container now
2023-11-02 13:12:44,803:INFO:_master_model_container: 11
2023-11-02 13:12:44,804:INFO:_display_container: 2
2023-11-02 13:12:44,834:INFO:KNeighborsRegressor(n_jobs=-1)
2023-11-02 13:12:44,834:INFO:create_model() successfully completed......................................
2023-11-02 13:12:49,199:INFO:SubProcess create_model() end ==================================
2023-11-02 13:12:49,199:INFO:Creating metrics dataframe
2023-11-02 13:12:49,236:INFO:Initializing Decision Tree Regressor
2023-11-02 13:12:49,236:INFO:Total runtime is 24.4493457198143 minutes
2023-11-02 13:12:49,241:INFO:SubProcess create_model() called ==================================
2023-11-02 13:12:49,242:INFO:Initializing create_model()
2023-11-02 13:12:49,242:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:12:49,242:INFO:Checking exceptions
2023-11-02 13:12:49,242:INFO:Importing libraries
2023-11-02 13:12:49,248:INFO:Copying training dataset
2023-11-02 13:12:49,431:INFO:Defining folds
2023-11-02 13:12:49,432:INFO:Declaring metric variables
2023-11-02 13:12:49,438:INFO:Importing untrained model
2023-11-02 13:12:49,442:INFO:Decision Tree Regressor Imported successfully
2023-11-02 13:12:49,452:INFO:Starting cross validation
2023-11-02 13:12:49,454:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:13:00,198:INFO:Calculating mean and std
2023-11-02 13:13:00,202:INFO:Creating metrics dataframe
2023-11-02 13:13:00,209:INFO:Uploading results into container
2023-11-02 13:13:00,211:INFO:Uploading model into container now
2023-11-02 13:13:00,213:INFO:_master_model_container: 12
2023-11-02 13:13:00,213:INFO:_display_container: 2
2023-11-02 13:13:00,215:INFO:DecisionTreeRegressor(random_state=123)
2023-11-02 13:13:00,215:INFO:create_model() successfully completed......................................
2023-11-02 13:13:00,539:INFO:SubProcess create_model() end ==================================
2023-11-02 13:13:00,539:INFO:Creating metrics dataframe
2023-11-02 13:13:00,555:INFO:Initializing Random Forest Regressor
2023-11-02 13:13:00,555:INFO:Total runtime is 24.637998783588408 minutes
2023-11-02 13:13:00,559:INFO:SubProcess create_model() called ==================================
2023-11-02 13:13:00,560:INFO:Initializing create_model()
2023-11-02 13:13:00,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:13:00,560:INFO:Checking exceptions
2023-11-02 13:13:00,560:INFO:Importing libraries
2023-11-02 13:13:00,560:INFO:Copying training dataset
2023-11-02 13:13:00,634:INFO:Defining folds
2023-11-02 13:13:00,634:INFO:Declaring metric variables
2023-11-02 13:13:00,640:INFO:Importing untrained model
2023-11-02 13:13:00,647:INFO:Random Forest Regressor Imported successfully
2023-11-02 13:13:00,660:INFO:Starting cross validation
2023-11-02 13:13:00,663:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:20:55,295:INFO:Calculating mean and std
2023-11-02 13:20:55,312:INFO:Creating metrics dataframe
2023-11-02 13:20:55,335:INFO:Uploading results into container
2023-11-02 13:20:55,339:INFO:Uploading model into container now
2023-11-02 13:20:55,342:INFO:_master_model_container: 13
2023-11-02 13:20:55,342:INFO:_display_container: 2
2023-11-02 13:20:55,345:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:20:55,345:INFO:create_model() successfully completed......................................
2023-11-02 13:20:56,260:INFO:SubProcess create_model() end ==================================
2023-11-02 13:20:56,261:INFO:Creating metrics dataframe
2023-11-02 13:20:56,278:INFO:Initializing Extra Trees Regressor
2023-11-02 13:20:56,278:INFO:Total runtime is 32.566712435086565 minutes
2023-11-02 13:20:56,282:INFO:SubProcess create_model() called ==================================
2023-11-02 13:20:56,283:INFO:Initializing create_model()
2023-11-02 13:20:56,283:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:20:56,283:INFO:Checking exceptions
2023-11-02 13:20:56,283:INFO:Importing libraries
2023-11-02 13:20:56,283:INFO:Copying training dataset
2023-11-02 13:20:56,383:INFO:Defining folds
2023-11-02 13:20:56,383:INFO:Declaring metric variables
2023-11-02 13:20:56,387:INFO:Importing untrained model
2023-11-02 13:20:56,392:INFO:Extra Trees Regressor Imported successfully
2023-11-02 13:20:56,399:INFO:Starting cross validation
2023-11-02 13:20:56,401:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:23:46,594:INFO:Calculating mean and std
2023-11-02 13:23:46,612:INFO:Creating metrics dataframe
2023-11-02 13:23:46,636:INFO:Uploading results into container
2023-11-02 13:23:46,638:INFO:Uploading model into container now
2023-11-02 13:23:46,641:INFO:_master_model_container: 14
2023-11-02 13:23:46,641:INFO:_display_container: 2
2023-11-02 13:23:46,643:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:23:46,643:INFO:create_model() successfully completed......................................
2023-11-02 13:23:47,544:INFO:SubProcess create_model() end ==================================
2023-11-02 13:23:47,544:INFO:Creating metrics dataframe
2023-11-02 13:23:47,564:INFO:Initializing AdaBoost Regressor
2023-11-02 13:23:47,564:INFO:Total runtime is 35.42147845427195 minutes
2023-11-02 13:23:47,568:INFO:SubProcess create_model() called ==================================
2023-11-02 13:23:47,569:INFO:Initializing create_model()
2023-11-02 13:23:47,569:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:23:47,569:INFO:Checking exceptions
2023-11-02 13:23:47,569:INFO:Importing libraries
2023-11-02 13:23:47,569:INFO:Copying training dataset
2023-11-02 13:23:47,681:INFO:Defining folds
2023-11-02 13:23:47,682:INFO:Declaring metric variables
2023-11-02 13:23:47,686:INFO:Importing untrained model
2023-11-02 13:23:47,694:INFO:AdaBoost Regressor Imported successfully
2023-11-02 13:23:47,704:INFO:Starting cross validation
2023-11-02 13:23:47,706:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:24:22,261:INFO:Calculating mean and std
2023-11-02 13:24:22,264:INFO:Creating metrics dataframe
2023-11-02 13:24:22,270:INFO:Uploading results into container
2023-11-02 13:24:22,271:INFO:Uploading model into container now
2023-11-02 13:24:22,272:INFO:_master_model_container: 15
2023-11-02 13:24:22,272:INFO:_display_container: 2
2023-11-02 13:24:22,273:INFO:AdaBoostRegressor(random_state=123)
2023-11-02 13:24:22,273:INFO:create_model() successfully completed......................................
2023-11-02 13:24:22,552:INFO:SubProcess create_model() end ==================================
2023-11-02 13:24:22,552:INFO:Creating metrics dataframe
2023-11-02 13:24:22,567:INFO:Initializing Gradient Boosting Regressor
2023-11-02 13:24:22,568:INFO:Total runtime is 36.00487538576126 minutes
2023-11-02 13:24:22,571:INFO:SubProcess create_model() called ==================================
2023-11-02 13:24:22,572:INFO:Initializing create_model()
2023-11-02 13:24:22,572:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:24:22,572:INFO:Checking exceptions
2023-11-02 13:24:22,573:INFO:Importing libraries
2023-11-02 13:24:22,573:INFO:Copying training dataset
2023-11-02 13:24:22,625:INFO:Defining folds
2023-11-02 13:24:22,626:INFO:Declaring metric variables
2023-11-02 13:24:22,630:INFO:Importing untrained model
2023-11-02 13:24:22,638:INFO:Gradient Boosting Regressor Imported successfully
2023-11-02 13:24:22,648:INFO:Starting cross validation
2023-11-02 13:24:22,650:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:26:55,883:INFO:Calculating mean and std
2023-11-02 13:26:55,889:INFO:Creating metrics dataframe
2023-11-02 13:26:55,898:INFO:Uploading results into container
2023-11-02 13:26:55,899:INFO:Uploading model into container now
2023-11-02 13:26:55,901:INFO:_master_model_container: 16
2023-11-02 13:26:55,901:INFO:_display_container: 2
2023-11-02 13:26:55,902:INFO:GradientBoostingRegressor(random_state=123)
2023-11-02 13:26:55,903:INFO:create_model() successfully completed......................................
2023-11-02 13:26:56,186:INFO:SubProcess create_model() end ==================================
2023-11-02 13:26:56,186:INFO:Creating metrics dataframe
2023-11-02 13:26:56,199:INFO:Initializing Extreme Gradient Boosting
2023-11-02 13:26:56,199:INFO:Total runtime is 38.56540245612462 minutes
2023-11-02 13:26:56,203:INFO:SubProcess create_model() called ==================================
2023-11-02 13:26:56,203:INFO:Initializing create_model()
2023-11-02 13:26:56,203:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:26:56,203:INFO:Checking exceptions
2023-11-02 13:26:56,203:INFO:Importing libraries
2023-11-02 13:26:56,204:INFO:Copying training dataset
2023-11-02 13:26:56,252:INFO:Defining folds
2023-11-02 13:26:56,253:INFO:Declaring metric variables
2023-11-02 13:26:56,259:INFO:Importing untrained model
2023-11-02 13:26:56,263:INFO:Extreme Gradient Boosting Imported successfully
2023-11-02 13:26:56,271:INFO:Starting cross validation
2023-11-02 13:26:56,273:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:27:06,939:INFO:Calculating mean and std
2023-11-02 13:27:06,944:INFO:Creating metrics dataframe
2023-11-02 13:27:06,953:INFO:Uploading results into container
2023-11-02 13:27:06,955:INFO:Uploading model into container now
2023-11-02 13:27:06,955:INFO:_master_model_container: 17
2023-11-02 13:27:06,955:INFO:_display_container: 2
2023-11-02 13:27:06,957:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-02 13:27:06,957:INFO:create_model() successfully completed......................................
2023-11-02 13:27:07,275:INFO:SubProcess create_model() end ==================================
2023-11-02 13:27:07,275:INFO:Creating metrics dataframe
2023-11-02 13:27:07,292:INFO:Initializing Light Gradient Boosting Machine
2023-11-02 13:27:07,292:INFO:Total runtime is 38.750277403990424 minutes
2023-11-02 13:27:07,296:INFO:SubProcess create_model() called ==================================
2023-11-02 13:27:07,296:INFO:Initializing create_model()
2023-11-02 13:27:07,296:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:27:07,297:INFO:Checking exceptions
2023-11-02 13:27:07,297:INFO:Importing libraries
2023-11-02 13:27:07,297:INFO:Copying training dataset
2023-11-02 13:27:07,346:INFO:Defining folds
2023-11-02 13:27:07,346:INFO:Declaring metric variables
2023-11-02 13:27:07,350:INFO:Importing untrained model
2023-11-02 13:27:07,355:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-02 13:27:07,362:INFO:Starting cross validation
2023-11-02 13:27:07,364:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:27:28,213:INFO:Calculating mean and std
2023-11-02 13:27:28,216:INFO:Creating metrics dataframe
2023-11-02 13:27:28,224:INFO:Uploading results into container
2023-11-02 13:27:28,226:INFO:Uploading model into container now
2023-11-02 13:27:28,227:INFO:_master_model_container: 18
2023-11-02 13:27:28,227:INFO:_display_container: 2
2023-11-02 13:27:28,228:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:27:28,228:INFO:create_model() successfully completed......................................
2023-11-02 13:27:28,547:INFO:SubProcess create_model() end ==================================
2023-11-02 13:27:28,547:INFO:Creating metrics dataframe
2023-11-02 13:27:28,566:INFO:Initializing Dummy Regressor
2023-11-02 13:27:28,566:INFO:Total runtime is 39.10485244989395 minutes
2023-11-02 13:27:28,571:INFO:SubProcess create_model() called ==================================
2023-11-02 13:27:28,572:INFO:Initializing create_model()
2023-11-02 13:27:28,572:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fda929ab040>, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:27:28,572:INFO:Checking exceptions
2023-11-02 13:27:28,572:INFO:Importing libraries
2023-11-02 13:27:28,573:INFO:Copying training dataset
2023-11-02 13:27:28,760:INFO:Defining folds
2023-11-02 13:27:28,760:INFO:Declaring metric variables
2023-11-02 13:27:28,768:INFO:Importing untrained model
2023-11-02 13:27:28,778:INFO:Dummy Regressor Imported successfully
2023-11-02 13:27:28,789:INFO:Starting cross validation
2023-11-02 13:27:28,792:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-02 13:27:29,608:INFO:Calculating mean and std
2023-11-02 13:27:29,610:INFO:Creating metrics dataframe
2023-11-02 13:27:29,614:INFO:Uploading results into container
2023-11-02 13:27:29,615:INFO:Uploading model into container now
2023-11-02 13:27:29,616:INFO:_master_model_container: 19
2023-11-02 13:27:29,616:INFO:_display_container: 2
2023-11-02 13:27:29,616:INFO:DummyRegressor()
2023-11-02 13:27:29,617:INFO:create_model() successfully completed......................................
2023-11-02 13:27:29,935:INFO:SubProcess create_model() end ==================================
2023-11-02 13:27:29,935:INFO:Creating metrics dataframe
2023-11-02 13:27:29,984:INFO:Initializing create_model()
2023-11-02 13:27:29,984:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-02 13:27:29,984:INFO:Checking exceptions
2023-11-02 13:27:29,994:INFO:Importing libraries
2023-11-02 13:27:29,995:INFO:Copying training dataset
2023-11-02 13:27:30,066:INFO:Defining folds
2023-11-02 13:27:30,066:INFO:Declaring metric variables
2023-11-02 13:27:30,067:INFO:Importing untrained model
2023-11-02 13:27:30,067:INFO:Declaring custom model
2023-11-02 13:27:30,069:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-02 13:27:30,070:INFO:Cross validation set to False
2023-11-02 13:27:30,070:INFO:Fitting Model
2023-11-02 13:27:30,454:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030541 seconds.
2023-11-02 13:27:30,454:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-02 13:27:30,455:INFO:[LightGBM] [Info] Total Bins 9347
2023-11-02 13:27:30,457:INFO:[LightGBM] [Info] Number of data points in the train set: 62778, number of used features: 54
2023-11-02 13:27:30,460:INFO:[LightGBM] [Info] Start training from score 292.766212
2023-11-02 13:27:31,122:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:27:31,122:INFO:create_model() successfully completed......................................
2023-11-02 13:27:31,527:INFO:_master_model_container: 19
2023-11-02 13:27:31,528:INFO:_display_container: 2
2023-11-02 13:27:31,528:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-02 13:27:31,529:INFO:compare_models() successfully completed......................................
2023-11-02 13:27:31,668:INFO:Initializing evaluate_model()
2023-11-02 13:27:31,668:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-02 13:27:31,712:INFO:Initializing plot_model()
2023-11-02 13:27:31,712:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, system=True)
2023-11-02 13:27:31,712:INFO:Checking exceptions
2023-11-02 13:27:31,736:INFO:Preloading libraries
2023-11-02 13:27:31,754:INFO:Copying training dataset
2023-11-02 13:27:31,754:INFO:Plot type: pipeline
2023-11-02 13:27:32,028:INFO:Visual Rendered Successfully
2023-11-02 13:27:32,341:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:32,346:INFO:Initializing evaluate_model()
2023-11-02 13:27:32,346:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cd460>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-02 13:27:32,414:INFO:Initializing plot_model()
2023-11-02 13:27:32,414:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cd460>, system=True)
2023-11-02 13:27:32,414:INFO:Checking exceptions
2023-11-02 13:27:32,433:INFO:Preloading libraries
2023-11-02 13:27:32,441:INFO:Copying training dataset
2023-11-02 13:27:32,441:INFO:Plot type: pipeline
2023-11-02 13:27:32,656:INFO:Visual Rendered Successfully
2023-11-02 13:27:33,078:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:33,105:INFO:Initializing evaluate_model()
2023-11-02 13:27:33,106:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cdee0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-02 13:27:33,178:INFO:Initializing plot_model()
2023-11-02 13:27:33,178:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cdee0>, system=True)
2023-11-02 13:27:33,179:INFO:Checking exceptions
2023-11-02 13:27:33,192:INFO:Preloading libraries
2023-11-02 13:27:33,198:INFO:Copying training dataset
2023-11-02 13:27:33,198:INFO:Plot type: pipeline
2023-11-02 13:27:33,431:INFO:Visual Rendered Successfully
2023-11-02 13:27:33,748:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:33,752:INFO:Initializing plot_model()
2023-11-02 13:27:33,753:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, system=True)
2023-11-02 13:27:33,753:INFO:Checking exceptions
2023-11-02 13:27:33,786:INFO:Preloading libraries
2023-11-02 13:27:33,791:INFO:Copying training dataset
2023-11-02 13:27:33,791:INFO:Plot type: feature
2023-11-02 13:27:33,792:WARNING:No coef_ found. Trying feature_importances_
2023-11-02 13:27:34,139:INFO:Visual Rendered Successfully
2023-11-02 13:27:34,419:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:34,420:INFO:Initializing plot_model()
2023-11-02 13:27:34,420:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cd460>, system=True)
2023-11-02 13:27:34,420:INFO:Checking exceptions
2023-11-02 13:27:34,433:INFO:Preloading libraries
2023-11-02 13:27:34,437:INFO:Copying training dataset
2023-11-02 13:27:34,437:INFO:Plot type: feature
2023-11-02 13:27:34,438:WARNING:No coef_ found. Trying feature_importances_
2023-11-02 13:27:34,755:INFO:Visual Rendered Successfully
2023-11-02 13:27:35,034:INFO:plot_model() successfully completed......................................
2023-11-02 13:27:35,035:INFO:Initializing plot_model()
2023-11-02 13:27:35,035:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fdabf2cdee0>, system=True)
2023-11-02 13:27:35,035:INFO:Checking exceptions
2023-11-02 13:27:35,044:INFO:Preloading libraries
2023-11-02 13:27:35,049:INFO:Copying training dataset
2023-11-02 13:27:35,049:INFO:Plot type: feature
2023-11-02 13:27:35,050:WARNING:No coef_ found. Trying feature_importances_
2023-11-02 13:27:35,384:INFO:Visual Rendered Successfully
2023-11-02 13:27:35,673:INFO:plot_model() successfully completed......................................
2023-11-02 13:30:12,936:INFO:Initializing predict_model()
2023-11-02 13:30:12,939:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fda995eb700>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fda80f4e820>)
2023-11-02 13:30:12,939:INFO:Checking exceptions
2023-11-02 13:30:12,939:INFO:Preloading libraries
2023-11-02 13:30:12,957:INFO:Set up data.
2023-11-02 13:30:13,067:INFO:Set up index.
2023-11-02 13:37:45,977:WARNING:<ipython-input-127-c47b07307b6d>:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  y_pred[i] = 0

2023-11-02 13:38:03,324:WARNING:<ipython-input-128-d8c14def8f49>:6: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  y_pred[i] = 0

2023-11-03 14:09:42,873:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-03 14:09:42,875:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-03 14:09:42,875:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-03 14:09:42,875:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-03 14:43:45,077:WARNING:<ipython-input-30-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:43:45,197:WARNING:<ipython-input-30-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:43:45,286:WARNING:<ipython-input-30-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:45:58,325:WARNING:<ipython-input-44-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:45:58,528:WARNING:<ipython-input-44-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:45:58,671:WARNING:<ipython-input-44-10a81996483f>:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  temp = train.append(test, ignore_index = True)

2023-11-03 14:45:59,123:INFO:PyCaret RegressionExperiment
2023-11-03 14:45:59,123:INFO:Logging name: reg-default-name
2023-11-03 14:45:59,124:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-03 14:45:59,124:INFO:version 3.1.0
2023-11-03 14:45:59,124:INFO:Initializing setup()
2023-11-03 14:45:59,124:INFO:self.USI: 43bf
2023-11-03 14:45:59,124:INFO:self._variable_keys: {'n_jobs_param', 'X_train', 'idx', 'X', 'exp_id', 'gpu_param', 'gpu_n_jobs_param', 'fold_generator', 'y_train', 'target_param', 'fold_groups_param', 'data', 'X_test', '_available_plots', 'log_plots_param', 'fold_shuffle_param', 'USI', '_ml_usecase', 'y', 'pipeline', 'y_test', 'exp_name_log', 'transform_target_param', 'seed', 'logging_param', 'memory', 'html_param'}
2023-11-03 14:45:59,124:INFO:Checking environment
2023-11-03 14:45:59,124:INFO:python_version: 3.9.6
2023-11-03 14:45:59,125:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-03 14:45:59,125:INFO:machine: x86_64
2023-11-03 14:45:59,371:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:45:59,372:INFO:Memory: svmem(total=8589934592, available=2331639808, percent=72.9, used=4797435904, free=16560128, active=2314715136, inactive=2313048064, wired=2482720768)
2023-11-03 14:45:59,372:INFO:Physical Core: 4
2023-11-03 14:45:59,373:INFO:Logical Core: 8
2023-11-03 14:45:59,373:INFO:Checking libraries
2023-11-03 14:45:59,377:INFO:System:
2023-11-03 14:45:59,380:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-03 14:45:59,380:INFO:executable: /usr/local/bin/python3.9
2023-11-03 14:45:59,380:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:45:59,381:INFO:PyCaret required dependencies:
2023-11-03 14:46:00,709:INFO:                 pip: 23.3.1
2023-11-03 14:46:00,713:INFO:          setuptools: 56.0.0
2023-11-03 14:46:00,713:INFO:             pycaret: 3.1.0
2023-11-03 14:46:00,713:INFO:             IPython: 7.28.0
2023-11-03 14:46:00,713:INFO:          ipywidgets: 8.1.1
2023-11-03 14:46:00,714:INFO:                tqdm: 4.66.1
2023-11-03 14:46:00,714:INFO:               numpy: 1.23.5
2023-11-03 14:46:00,714:INFO:              pandas: 1.5.3
2023-11-03 14:46:00,714:INFO:              jinja2: 3.0.1
2023-11-03 14:46:00,714:INFO:               scipy: 1.10.1
2023-11-03 14:46:00,714:INFO:              joblib: 1.3.2
2023-11-03 14:46:00,714:INFO:             sklearn: 1.1.3
2023-11-03 14:46:00,714:INFO:                pyod: 1.1.1
2023-11-03 14:46:00,714:INFO:            imblearn: 0.11.0
2023-11-03 14:46:00,714:INFO:   category_encoders: 2.6.3
2023-11-03 14:46:00,714:INFO:            lightgbm: 4.1.0
2023-11-03 14:46:00,714:INFO:               numba: 0.58.1
2023-11-03 14:46:00,714:INFO:            requests: 2.31.0
2023-11-03 14:46:00,714:INFO:          matplotlib: 3.4.2
2023-11-03 14:46:00,714:INFO:          scikitplot: 0.3.7
2023-11-03 14:46:00,714:INFO:         yellowbrick: 1.5
2023-11-03 14:46:00,714:INFO:              plotly: 5.18.0
2023-11-03 14:46:00,714:INFO:    plotly-resampler: Not installed
2023-11-03 14:46:00,714:INFO:             kaleido: 0.2.1
2023-11-03 14:46:00,714:INFO:           schemdraw: 0.15
2023-11-03 14:46:00,715:INFO:         statsmodels: 0.14.0
2023-11-03 14:46:00,715:INFO:              sktime: 0.21.1
2023-11-03 14:46:00,715:INFO:               tbats: 1.1.3
2023-11-03 14:46:00,715:INFO:            pmdarima: 2.0.4
2023-11-03 14:46:00,715:INFO:              psutil: 5.9.6
2023-11-03 14:46:00,715:INFO:          markupsafe: 2.1.3
2023-11-03 14:46:00,715:INFO:             pickle5: Not installed
2023-11-03 14:46:00,715:INFO:         cloudpickle: 2.2.1
2023-11-03 14:46:00,715:INFO:         deprecation: 2.1.0
2023-11-03 14:46:00,715:INFO:              xxhash: 3.4.1
2023-11-03 14:46:00,715:INFO:           wurlitzer: 3.0.3
2023-11-03 14:46:00,715:INFO:PyCaret optional dependencies:
2023-11-03 14:46:01,168:INFO:                shap: Not installed
2023-11-03 14:46:01,169:INFO:           interpret: Not installed
2023-11-03 14:46:01,169:INFO:                umap: Not installed
2023-11-03 14:46:01,169:INFO:     ydata_profiling: Not installed
2023-11-03 14:46:01,169:INFO:  explainerdashboard: Not installed
2023-11-03 14:46:01,169:INFO:             autoviz: Not installed
2023-11-03 14:46:01,169:INFO:           fairlearn: Not installed
2023-11-03 14:46:01,169:INFO:          deepchecks: Not installed
2023-11-03 14:46:01,169:INFO:             xgboost: 2.0.0
2023-11-03 14:46:01,169:INFO:            catboost: Not installed
2023-11-03 14:46:01,169:INFO:              kmodes: Not installed
2023-11-03 14:46:01,169:INFO:             mlxtend: Not installed
2023-11-03 14:46:01,169:INFO:       statsforecast: Not installed
2023-11-03 14:46:01,169:INFO:        tune_sklearn: Not installed
2023-11-03 14:46:01,169:INFO:                 ray: Not installed
2023-11-03 14:46:01,169:INFO:            hyperopt: 0.2.7
2023-11-03 14:46:01,169:INFO:              optuna: 3.4.0
2023-11-03 14:46:01,169:INFO:               skopt: Not installed
2023-11-03 14:46:01,170:INFO:              mlflow: Not installed
2023-11-03 14:46:01,170:INFO:              gradio: Not installed
2023-11-03 14:46:01,170:INFO:             fastapi: Not installed
2023-11-03 14:46:01,170:INFO:             uvicorn: Not installed
2023-11-03 14:46:01,170:INFO:              m2cgen: Not installed
2023-11-03 14:46:01,170:INFO:           evidently: Not installed
2023-11-03 14:46:01,170:INFO:               fugue: Not installed
2023-11-03 14:46:01,170:INFO:           streamlit: Not installed
2023-11-03 14:46:01,170:INFO:             prophet: Not installed
2023-11-03 14:46:01,170:INFO:None
2023-11-03 14:46:01,170:INFO:Set up data.
2023-11-03 14:46:01,251:INFO:Set up folding strategy.
2023-11-03 14:46:01,251:INFO:Set up train/test split.
2023-11-03 14:46:01,314:INFO:Set up index.
2023-11-03 14:46:01,317:INFO:Assigning column types.
2023-11-03 14:46:01,352:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-03 14:46:01,352:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,360:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,371:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,511:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,598:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,599:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:01,605:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:01,606:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,614:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,622:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,729:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,797:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,798:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:01,802:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:01,802:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-03 14:46:01,810:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,817:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,912:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,996:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:01,997:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:02,005:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:02,016:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,028:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,231:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,344:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,346:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:02,355:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:02,357:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-03 14:46:02,405:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,601:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,707:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:02,708:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:02,713:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:02,730:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,033:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,264:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,265:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:03,274:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:03,276:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-03 14:46:03,501:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,614:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:03,616:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:03,622:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:03,883:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:04,056:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:04,058:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:04,066:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:04,067:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-03 14:46:04,507:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:04,626:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:04,632:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:04,794:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:04,950:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:04,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:04,968:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-03 14:46:06,127:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:06,148:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:06,753:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:06,764:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:06,775:INFO:Preparing preprocessing pipeline...
2023-11-03 14:46:06,775:INFO:Set up simple imputation.
2023-11-03 14:46:06,783:INFO:Set up column name cleaning.
2023-11-03 14:46:07,034:INFO:Finished creating preprocessing pipeline.
2023-11-03 14:46:07,058:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['is_day:idx', 'is_in_shadow:idx',
                                             'absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'di...
                                             'pressure_50m:hPa', 'prob_rime:p',
                                             'rain_water:kgm2',
                                             'relative_humidity_1000hPa:p',
                                             'sfc_pressure:hPa',
                                             'snow_depth:cm',
                                             'snow_melt_10min:mm',
                                             'snow_water:kgm2', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-03 14:46:07,059:INFO:Creating final display dataframe.
2023-11-03 14:46:08,333:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    pv_measurement
2                   Target type        Regression
3           Original data shape       (34060, 60)
4        Transformed data shape       (34060, 60)
5   Transformed train set shape       (23842, 60)
6    Transformed test set shape       (10218, 60)
7              Numeric features                59
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              43bf
2023-11-03 14:46:08,876:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:08,885:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:09,531:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:09,550:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:09,554:INFO:setup() successfully completed in 10.45s...............
2023-11-03 14:46:09,563:INFO:PyCaret RegressionExperiment
2023-11-03 14:46:09,564:INFO:Logging name: reg-default-name
2023-11-03 14:46:09,566:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-03 14:46:09,566:INFO:version 3.1.0
2023-11-03 14:46:09,566:INFO:Initializing setup()
2023-11-03 14:46:09,566:INFO:self.USI: b087
2023-11-03 14:46:09,567:INFO:self._variable_keys: {'n_jobs_param', 'X_train', 'idx', 'X', 'exp_id', 'gpu_param', 'gpu_n_jobs_param', 'fold_generator', 'y_train', 'target_param', 'fold_groups_param', 'data', 'X_test', '_available_plots', 'log_plots_param', 'fold_shuffle_param', 'USI', '_ml_usecase', 'y', 'pipeline', 'y_test', 'exp_name_log', 'transform_target_param', 'seed', 'logging_param', 'memory', 'html_param'}
2023-11-03 14:46:09,567:INFO:Checking environment
2023-11-03 14:46:09,567:INFO:python_version: 3.9.6
2023-11-03 14:46:09,567:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-03 14:46:09,568:INFO:machine: x86_64
2023-11-03 14:46:09,568:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:46:09,568:INFO:Memory: svmem(total=8589934592, available=2770939904, percent=67.7, used=4941959168, free=392626176, active=2383290368, inactive=2355232768, wired=2558668800)
2023-11-03 14:46:09,568:INFO:Physical Core: 4
2023-11-03 14:46:09,568:INFO:Logical Core: 8
2023-11-03 14:46:09,569:INFO:Checking libraries
2023-11-03 14:46:09,569:INFO:System:
2023-11-03 14:46:09,569:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-03 14:46:09,569:INFO:executable: /usr/local/bin/python3.9
2023-11-03 14:46:09,569:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:46:09,570:INFO:PyCaret required dependencies:
2023-11-03 14:46:09,570:INFO:                 pip: 23.3.1
2023-11-03 14:46:09,571:INFO:          setuptools: 56.0.0
2023-11-03 14:46:09,574:INFO:             pycaret: 3.1.0
2023-11-03 14:46:09,574:INFO:             IPython: 7.28.0
2023-11-03 14:46:09,574:INFO:          ipywidgets: 8.1.1
2023-11-03 14:46:09,574:INFO:                tqdm: 4.66.1
2023-11-03 14:46:09,574:INFO:               numpy: 1.23.5
2023-11-03 14:46:09,575:INFO:              pandas: 1.5.3
2023-11-03 14:46:09,575:INFO:              jinja2: 3.0.1
2023-11-03 14:46:09,575:INFO:               scipy: 1.10.1
2023-11-03 14:46:09,575:INFO:              joblib: 1.3.2
2023-11-03 14:46:09,575:INFO:             sklearn: 1.1.3
2023-11-03 14:46:09,575:INFO:                pyod: 1.1.1
2023-11-03 14:46:09,576:INFO:            imblearn: 0.11.0
2023-11-03 14:46:09,576:INFO:   category_encoders: 2.6.3
2023-11-03 14:46:09,576:INFO:            lightgbm: 4.1.0
2023-11-03 14:46:09,576:INFO:               numba: 0.58.1
2023-11-03 14:46:09,576:INFO:            requests: 2.31.0
2023-11-03 14:46:09,577:INFO:          matplotlib: 3.4.2
2023-11-03 14:46:09,577:INFO:          scikitplot: 0.3.7
2023-11-03 14:46:09,577:INFO:         yellowbrick: 1.5
2023-11-03 14:46:09,577:INFO:              plotly: 5.18.0
2023-11-03 14:46:09,577:INFO:    plotly-resampler: Not installed
2023-11-03 14:46:09,578:INFO:             kaleido: 0.2.1
2023-11-03 14:46:09,578:INFO:           schemdraw: 0.15
2023-11-03 14:46:09,578:INFO:         statsmodels: 0.14.0
2023-11-03 14:46:09,578:INFO:              sktime: 0.21.1
2023-11-03 14:46:09,582:INFO:               tbats: 1.1.3
2023-11-03 14:46:09,582:INFO:            pmdarima: 2.0.4
2023-11-03 14:46:09,582:INFO:              psutil: 5.9.6
2023-11-03 14:46:09,583:INFO:          markupsafe: 2.1.3
2023-11-03 14:46:09,583:INFO:             pickle5: Not installed
2023-11-03 14:46:09,583:INFO:         cloudpickle: 2.2.1
2023-11-03 14:46:09,584:INFO:         deprecation: 2.1.0
2023-11-03 14:46:09,584:INFO:              xxhash: 3.4.1
2023-11-03 14:46:09,584:INFO:           wurlitzer: 3.0.3
2023-11-03 14:46:09,584:INFO:PyCaret optional dependencies:
2023-11-03 14:46:09,585:INFO:                shap: Not installed
2023-11-03 14:46:09,585:INFO:           interpret: Not installed
2023-11-03 14:46:09,585:INFO:                umap: Not installed
2023-11-03 14:46:09,585:INFO:     ydata_profiling: Not installed
2023-11-03 14:46:09,585:INFO:  explainerdashboard: Not installed
2023-11-03 14:46:09,585:INFO:             autoviz: Not installed
2023-11-03 14:46:09,586:INFO:           fairlearn: Not installed
2023-11-03 14:46:09,586:INFO:          deepchecks: Not installed
2023-11-03 14:46:09,586:INFO:             xgboost: 2.0.0
2023-11-03 14:46:09,586:INFO:            catboost: Not installed
2023-11-03 14:46:09,586:INFO:              kmodes: Not installed
2023-11-03 14:46:09,586:INFO:             mlxtend: Not installed
2023-11-03 14:46:09,586:INFO:       statsforecast: Not installed
2023-11-03 14:46:09,587:INFO:        tune_sklearn: Not installed
2023-11-03 14:46:09,587:INFO:                 ray: Not installed
2023-11-03 14:46:09,587:INFO:            hyperopt: 0.2.7
2023-11-03 14:46:09,588:INFO:              optuna: 3.4.0
2023-11-03 14:46:09,588:INFO:               skopt: Not installed
2023-11-03 14:46:09,588:INFO:              mlflow: Not installed
2023-11-03 14:46:09,588:INFO:              gradio: Not installed
2023-11-03 14:46:09,589:INFO:             fastapi: Not installed
2023-11-03 14:46:09,589:INFO:             uvicorn: Not installed
2023-11-03 14:46:09,589:INFO:              m2cgen: Not installed
2023-11-03 14:46:09,589:INFO:           evidently: Not installed
2023-11-03 14:46:09,590:INFO:               fugue: Not installed
2023-11-03 14:46:09,590:INFO:           streamlit: Not installed
2023-11-03 14:46:09,590:INFO:             prophet: Not installed
2023-11-03 14:46:09,590:INFO:None
2023-11-03 14:46:09,590:INFO:Set up data.
2023-11-03 14:46:09,907:INFO:Set up folding strategy.
2023-11-03 14:46:09,908:INFO:Set up train/test split.
2023-11-03 14:46:10,100:INFO:Set up index.
2023-11-03 14:46:10,106:INFO:Assigning column types.
2023-11-03 14:46:10,205:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-03 14:46:10,206:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,246:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,274:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,568:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,738:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,740:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:10,754:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:10,755:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,770:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:10,782:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,067:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,255:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,258:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:11,269:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:11,270:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-03 14:46:11,290:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,311:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,559:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,747:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,750:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:11,758:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:11,774:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:11,795:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:12,103:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:12,347:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:12,351:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:12,359:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:12,361:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-03 14:46:12,406:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,045:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,195:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,199:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:13,208:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:13,234:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,460:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,549:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,550:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:13,554:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:13,555:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-03 14:46:13,682:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,789:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:13,790:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:13,797:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:13,960:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:14,063:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:14,064:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:14,068:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:14,069:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-03 14:46:14,213:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:14,312:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:14,317:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:14,508:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:14,626:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:14,638:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:14,640:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-03 14:46:15,192:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:15,206:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:15,975:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:15,988:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:15,991:INFO:Preparing preprocessing pipeline...
2023-11-03 14:46:15,991:INFO:Set up simple imputation.
2023-11-03 14:46:15,997:INFO:Set up column name cleaning.
2023-11-03 14:46:16,209:INFO:Finished creating preprocessing pipeline.
2023-11-03 14:46:16,230:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['is_day:idx', 'is_in_shadow:idx',
                                             'absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'di...
                                             'pressure_50m:hPa', 'prob_rime:p',
                                             'rain_water:kgm2',
                                             'relative_humidity_1000hPa:p',
                                             'sfc_pressure:hPa',
                                             'snow_depth:cm',
                                             'snow_melt_10min:mm',
                                             'snow_water:kgm2', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-03 14:46:16,230:INFO:Creating final display dataframe.
2023-11-03 14:46:17,622:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    pv_measurement
2                   Target type        Regression
3           Original data shape       (29596, 60)
4        Transformed data shape       (29596, 60)
5   Transformed train set shape       (20717, 60)
6    Transformed test set shape        (8879, 60)
7              Numeric features                59
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              b087
2023-11-03 14:46:18,001:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:18,008:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:18,362:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:18,370:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:18,372:INFO:setup() successfully completed in 8.82s...............
2023-11-03 14:46:18,377:INFO:PyCaret RegressionExperiment
2023-11-03 14:46:18,378:INFO:Logging name: reg-default-name
2023-11-03 14:46:18,378:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-03 14:46:18,378:INFO:version 3.1.0
2023-11-03 14:46:18,378:INFO:Initializing setup()
2023-11-03 14:46:18,378:INFO:self.USI: 9015
2023-11-03 14:46:18,378:INFO:self._variable_keys: {'n_jobs_param', 'X_train', 'idx', 'X', 'exp_id', 'gpu_param', 'gpu_n_jobs_param', 'fold_generator', 'y_train', 'target_param', 'fold_groups_param', 'data', 'X_test', '_available_plots', 'log_plots_param', 'fold_shuffle_param', 'USI', '_ml_usecase', 'y', 'pipeline', 'y_test', 'exp_name_log', 'transform_target_param', 'seed', 'logging_param', 'memory', 'html_param'}
2023-11-03 14:46:18,378:INFO:Checking environment
2023-11-03 14:46:18,378:INFO:python_version: 3.9.6
2023-11-03 14:46:18,379:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-03 14:46:18,379:INFO:machine: x86_64
2023-11-03 14:46:18,379:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:46:18,379:INFO:Memory: svmem(total=8589934592, available=2559008768, percent=70.2, used=5082755072, free=82395136, active=2479779840, inactive=2437382144, wired=2602975232)
2023-11-03 14:46:18,379:INFO:Physical Core: 4
2023-11-03 14:46:18,379:INFO:Logical Core: 8
2023-11-03 14:46:18,379:INFO:Checking libraries
2023-11-03 14:46:18,379:INFO:System:
2023-11-03 14:46:18,379:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-03 14:46:18,380:INFO:executable: /usr/local/bin/python3.9
2023-11-03 14:46:18,380:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-03 14:46:18,380:INFO:PyCaret required dependencies:
2023-11-03 14:46:18,380:INFO:                 pip: 23.3.1
2023-11-03 14:46:18,380:INFO:          setuptools: 56.0.0
2023-11-03 14:46:18,380:INFO:             pycaret: 3.1.0
2023-11-03 14:46:18,380:INFO:             IPython: 7.28.0
2023-11-03 14:46:18,380:INFO:          ipywidgets: 8.1.1
2023-11-03 14:46:18,381:INFO:                tqdm: 4.66.1
2023-11-03 14:46:18,381:INFO:               numpy: 1.23.5
2023-11-03 14:46:18,381:INFO:              pandas: 1.5.3
2023-11-03 14:46:18,381:INFO:              jinja2: 3.0.1
2023-11-03 14:46:18,381:INFO:               scipy: 1.10.1
2023-11-03 14:46:18,381:INFO:              joblib: 1.3.2
2023-11-03 14:46:18,381:INFO:             sklearn: 1.1.3
2023-11-03 14:46:18,382:INFO:                pyod: 1.1.1
2023-11-03 14:46:18,382:INFO:            imblearn: 0.11.0
2023-11-03 14:46:18,382:INFO:   category_encoders: 2.6.3
2023-11-03 14:46:18,382:INFO:            lightgbm: 4.1.0
2023-11-03 14:46:18,382:INFO:               numba: 0.58.1
2023-11-03 14:46:18,382:INFO:            requests: 2.31.0
2023-11-03 14:46:18,383:INFO:          matplotlib: 3.4.2
2023-11-03 14:46:18,383:INFO:          scikitplot: 0.3.7
2023-11-03 14:46:18,383:INFO:         yellowbrick: 1.5
2023-11-03 14:46:18,383:INFO:              plotly: 5.18.0
2023-11-03 14:46:18,383:INFO:    plotly-resampler: Not installed
2023-11-03 14:46:18,383:INFO:             kaleido: 0.2.1
2023-11-03 14:46:18,383:INFO:           schemdraw: 0.15
2023-11-03 14:46:18,383:INFO:         statsmodels: 0.14.0
2023-11-03 14:46:18,383:INFO:              sktime: 0.21.1
2023-11-03 14:46:18,383:INFO:               tbats: 1.1.3
2023-11-03 14:46:18,383:INFO:            pmdarima: 2.0.4
2023-11-03 14:46:18,384:INFO:              psutil: 5.9.6
2023-11-03 14:46:18,384:INFO:          markupsafe: 2.1.3
2023-11-03 14:46:18,384:INFO:             pickle5: Not installed
2023-11-03 14:46:18,384:INFO:         cloudpickle: 2.2.1
2023-11-03 14:46:18,384:INFO:         deprecation: 2.1.0
2023-11-03 14:46:18,384:INFO:              xxhash: 3.4.1
2023-11-03 14:46:18,385:INFO:           wurlitzer: 3.0.3
2023-11-03 14:46:18,385:INFO:PyCaret optional dependencies:
2023-11-03 14:46:18,385:INFO:                shap: Not installed
2023-11-03 14:46:18,385:INFO:           interpret: Not installed
2023-11-03 14:46:18,386:INFO:                umap: Not installed
2023-11-03 14:46:18,386:INFO:     ydata_profiling: Not installed
2023-11-03 14:46:18,386:INFO:  explainerdashboard: Not installed
2023-11-03 14:46:18,386:INFO:             autoviz: Not installed
2023-11-03 14:46:18,386:INFO:           fairlearn: Not installed
2023-11-03 14:46:18,386:INFO:          deepchecks: Not installed
2023-11-03 14:46:18,387:INFO:             xgboost: 2.0.0
2023-11-03 14:46:18,387:INFO:            catboost: Not installed
2023-11-03 14:46:18,387:INFO:              kmodes: Not installed
2023-11-03 14:46:18,387:INFO:             mlxtend: Not installed
2023-11-03 14:46:18,387:INFO:       statsforecast: Not installed
2023-11-03 14:46:18,388:INFO:        tune_sklearn: Not installed
2023-11-03 14:46:18,388:INFO:                 ray: Not installed
2023-11-03 14:46:18,388:INFO:            hyperopt: 0.2.7
2023-11-03 14:46:18,388:INFO:              optuna: 3.4.0
2023-11-03 14:46:18,388:INFO:               skopt: Not installed
2023-11-03 14:46:18,389:INFO:              mlflow: Not installed
2023-11-03 14:46:18,389:INFO:              gradio: Not installed
2023-11-03 14:46:18,389:INFO:             fastapi: Not installed
2023-11-03 14:46:18,390:INFO:             uvicorn: Not installed
2023-11-03 14:46:18,390:INFO:              m2cgen: Not installed
2023-11-03 14:46:18,390:INFO:           evidently: Not installed
2023-11-03 14:46:18,390:INFO:               fugue: Not installed
2023-11-03 14:46:18,390:INFO:           streamlit: Not installed
2023-11-03 14:46:18,390:INFO:             prophet: Not installed
2023-11-03 14:46:18,391:INFO:None
2023-11-03 14:46:18,391:INFO:Set up data.
2023-11-03 14:46:18,494:INFO:Set up folding strategy.
2023-11-03 14:46:18,495:INFO:Set up train/test split.
2023-11-03 14:46:18,547:INFO:Set up index.
2023-11-03 14:46:18,560:INFO:Assigning column types.
2023-11-03 14:46:18,583:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-03 14:46:18,583:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,601:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,612:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,818:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,972:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:18,973:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:18,983:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:18,983:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,002:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,016:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,291:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,457:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,460:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:19,468:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:19,469:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-03 14:46:19,484:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,501:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:19,803:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,035:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,037:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:20,049:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:20,063:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,078:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,268:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,373:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,374:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:20,380:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:20,380:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-03 14:46:20,404:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,536:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,623:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,625:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:20,632:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:20,660:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,825:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,930:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:20,931:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:20,936:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:20,937:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-03 14:46:21,137:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:21,301:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:21,303:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:21,315:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:21,883:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:22,063:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-03 14:46:22,068:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:22,086:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:22,087:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-03 14:46:22,276:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:22,354:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:22,358:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:22,496:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-03 14:46:22,588:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:22,604:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:22,605:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-03 14:46:22,860:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:22,864:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:23,079:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:23,083:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:23,084:INFO:Preparing preprocessing pipeline...
2023-11-03 14:46:23,084:INFO:Set up simple imputation.
2023-11-03 14:46:23,087:INFO:Set up column name cleaning.
2023-11-03 14:46:23,176:INFO:Finished creating preprocessing pipeline.
2023-11-03 14:46:23,184:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['is_day:idx', 'is_in_shadow:idx',
                                             'absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'di...
                                             'pressure_50m:hPa', 'prob_rime:p',
                                             'rain_water:kgm2',
                                             'relative_humidity_1000hPa:p',
                                             'sfc_pressure:hPa',
                                             'snow_depth:cm',
                                             'snow_melt_10min:mm',
                                             'snow_water:kgm2', ...],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-03 14:46:23,184:INFO:Creating final display dataframe.
2023-11-03 14:46:23,524:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target    pv_measurement
2                   Target type        Regression
3           Original data shape       (26028, 60)
4        Transformed data shape       (26028, 60)
5   Transformed train set shape       (18219, 60)
6    Transformed test set shape        (7809, 60)
7              Numeric features                59
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator             KFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  reg-default-name
18                          USI              9015
2023-11-03 14:46:23,712:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:23,716:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:23,882:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-03 14:46:23,885:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-03 14:46:23,886:INFO:setup() successfully completed in 5.51s...............
2023-11-03 14:46:23,899:INFO:Initializing compare_models()
2023-11-03 14:46:23,900:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-11-03 14:46:23,900:INFO:Checking exceptions
2023-11-03 14:46:23,913:INFO:Preparing display monitor
2023-11-03 14:46:24,035:INFO:Initializing Linear Regression
2023-11-03 14:46:24,035:INFO:Total runtime is 9.26653544108073e-06 minutes
2023-11-03 14:46:24,044:INFO:SubProcess create_model() called ==================================
2023-11-03 14:46:24,045:INFO:Initializing create_model()
2023-11-03 14:46:24,046:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:46:24,046:INFO:Checking exceptions
2023-11-03 14:46:24,047:INFO:Importing libraries
2023-11-03 14:46:24,047:INFO:Copying training dataset
2023-11-03 14:46:24,090:INFO:Defining folds
2023-11-03 14:46:24,090:INFO:Declaring metric variables
2023-11-03 14:46:24,095:INFO:Importing untrained model
2023-11-03 14:46:24,101:INFO:Linear Regression Imported successfully
2023-11-03 14:46:24,113:INFO:Starting cross validation
2023-11-03 14:46:24,126:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:46:38,474:INFO:Calculating mean and std
2023-11-03 14:46:38,481:INFO:Creating metrics dataframe
2023-11-03 14:46:38,492:INFO:Uploading results into container
2023-11-03 14:46:38,494:INFO:Uploading model into container now
2023-11-03 14:46:38,495:INFO:_master_model_container: 1
2023-11-03 14:46:38,496:INFO:_display_container: 2
2023-11-03 14:46:38,497:INFO:LinearRegression(n_jobs=-1)
2023-11-03 14:46:38,498:INFO:create_model() successfully completed......................................
2023-11-03 14:46:39,130:INFO:SubProcess create_model() end ==================================
2023-11-03 14:46:39,131:INFO:Creating metrics dataframe
2023-11-03 14:46:39,147:INFO:Initializing Lasso Regression
2023-11-03 14:46:39,147:INFO:Total runtime is 0.25186730225880943 minutes
2023-11-03 14:46:39,151:INFO:SubProcess create_model() called ==================================
2023-11-03 14:46:39,152:INFO:Initializing create_model()
2023-11-03 14:46:39,152:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:46:39,152:INFO:Checking exceptions
2023-11-03 14:46:39,153:INFO:Importing libraries
2023-11-03 14:46:39,153:INFO:Copying training dataset
2023-11-03 14:46:39,186:INFO:Defining folds
2023-11-03 14:46:39,186:INFO:Declaring metric variables
2023-11-03 14:46:39,192:INFO:Importing untrained model
2023-11-03 14:46:39,201:INFO:Lasso Regression Imported successfully
2023-11-03 14:46:39,212:INFO:Starting cross validation
2023-11-03 14:46:39,214:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:46:48,377:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.471e+09, tolerance: 2.932e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:48,603:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.473e+09, tolerance: 2.949e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:48,707:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.499e+09, tolerance: 2.910e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:48,727:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.496e+09, tolerance: 2.944e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:48,967:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.468e+09, tolerance: 2.936e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:49,135:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.447e+09, tolerance: 2.941e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:49,194:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.432e+09, tolerance: 2.932e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:49,508:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.490e+09, tolerance: 2.936e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:52,871:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.482e+09, tolerance: 2.925e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:52,896:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.441e+09, tolerance: 2.935e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:46:53,053:INFO:Calculating mean and std
2023-11-03 14:46:53,058:INFO:Creating metrics dataframe
2023-11-03 14:46:53,070:INFO:Uploading results into container
2023-11-03 14:46:53,072:INFO:Uploading model into container now
2023-11-03 14:46:53,074:INFO:_master_model_container: 2
2023-11-03 14:46:53,074:INFO:_display_container: 2
2023-11-03 14:46:53,075:INFO:Lasso(random_state=123)
2023-11-03 14:46:53,075:INFO:create_model() successfully completed......................................
2023-11-03 14:46:53,362:INFO:SubProcess create_model() end ==================================
2023-11-03 14:46:53,362:INFO:Creating metrics dataframe
2023-11-03 14:46:53,390:INFO:Initializing Ridge Regression
2023-11-03 14:46:53,390:INFO:Total runtime is 0.4892539699872335 minutes
2023-11-03 14:46:53,399:INFO:SubProcess create_model() called ==================================
2023-11-03 14:46:53,400:INFO:Initializing create_model()
2023-11-03 14:46:53,401:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:46:53,402:INFO:Checking exceptions
2023-11-03 14:46:53,402:INFO:Importing libraries
2023-11-03 14:46:53,402:INFO:Copying training dataset
2023-11-03 14:46:53,455:INFO:Defining folds
2023-11-03 14:46:53,456:INFO:Declaring metric variables
2023-11-03 14:46:53,464:INFO:Importing untrained model
2023-11-03 14:46:53,478:INFO:Ridge Regression Imported successfully
2023-11-03 14:46:53,499:INFO:Starting cross validation
2023-11-03 14:46:53,505:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:46:54,085:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.14122e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,169:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.15068e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,209:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.13958e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,358:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.15498e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,457:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.15274e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:54,743:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.1538e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:46:55,185:INFO:Calculating mean and std
2023-11-03 14:46:55,193:INFO:Creating metrics dataframe
2023-11-03 14:46:55,215:INFO:Uploading results into container
2023-11-03 14:46:55,217:INFO:Uploading model into container now
2023-11-03 14:46:55,218:INFO:_master_model_container: 3
2023-11-03 14:46:55,219:INFO:_display_container: 2
2023-11-03 14:46:55,220:INFO:Ridge(random_state=123)
2023-11-03 14:46:55,221:INFO:create_model() successfully completed......................................
2023-11-03 14:46:55,569:INFO:SubProcess create_model() end ==================================
2023-11-03 14:46:55,570:INFO:Creating metrics dataframe
2023-11-03 14:46:55,626:INFO:Initializing Elastic Net
2023-11-03 14:46:55,627:INFO:Total runtime is 0.5265276869138081 minutes
2023-11-03 14:46:55,652:INFO:SubProcess create_model() called ==================================
2023-11-03 14:46:55,654:INFO:Initializing create_model()
2023-11-03 14:46:55,654:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:46:55,655:INFO:Checking exceptions
2023-11-03 14:46:55,656:INFO:Importing libraries
2023-11-03 14:46:55,656:INFO:Copying training dataset
2023-11-03 14:46:55,744:INFO:Defining folds
2023-11-03 14:46:55,745:INFO:Declaring metric variables
2023-11-03 14:46:55,765:INFO:Importing untrained model
2023-11-03 14:46:55,784:INFO:Elastic Net Imported successfully
2023-11-03 14:46:55,814:INFO:Starting cross validation
2023-11-03 14:46:55,821:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:08,420:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.509e+09, tolerance: 2.932e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,495:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.510e+09, tolerance: 2.949e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,587:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.505e+09, tolerance: 2.936e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,587:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.533e+09, tolerance: 2.944e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,641:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.535e+09, tolerance: 2.910e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,669:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.483e+09, tolerance: 2.941e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,715:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.467e+09, tolerance: 2.932e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:08,802:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.527e+09, tolerance: 2.936e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:12,584:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.478e+09, tolerance: 2.935e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:12,619:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.520e+09, tolerance: 2.925e+06
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:47:12,773:INFO:Calculating mean and std
2023-11-03 14:47:12,776:INFO:Creating metrics dataframe
2023-11-03 14:47:12,781:INFO:Uploading results into container
2023-11-03 14:47:12,783:INFO:Uploading model into container now
2023-11-03 14:47:12,784:INFO:_master_model_container: 4
2023-11-03 14:47:12,784:INFO:_display_container: 2
2023-11-03 14:47:12,785:INFO:ElasticNet(random_state=123)
2023-11-03 14:47:12,785:INFO:create_model() successfully completed......................................
2023-11-03 14:47:13,001:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:13,001:INFO:Creating metrics dataframe
2023-11-03 14:47:13,018:INFO:Initializing Least Angle Regression
2023-11-03 14:47:13,018:INFO:Total runtime is 0.8163832346598308 minutes
2023-11-03 14:47:13,024:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:13,025:INFO:Initializing create_model()
2023-11-03 14:47:13,025:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:13,025:INFO:Checking exceptions
2023-11-03 14:47:13,025:INFO:Importing libraries
2023-11-03 14:47:13,026:INFO:Copying training dataset
2023-11-03 14:47:13,062:INFO:Defining folds
2023-11-03 14:47:13,062:INFO:Declaring metric variables
2023-11-03 14:47:13,068:INFO:Importing untrained model
2023-11-03 14:47:13,076:INFO:Least Angle Regression Imported successfully
2023-11-03 14:47:13,091:INFO:Starting cross validation
2023-11-03 14:47:13,093:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:13,343:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,370:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,389:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,528:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=4.614e-03, with an active set of 49 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,528:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=5.469e-03, with an active set of 45 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,537:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.421e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,538:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=2.307e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,542:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=2.080e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,543:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.894e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,543:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.829e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,544:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=3.910e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,546:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=1.776e-01, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,551:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,633:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=4.008e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,634:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=3.844e-03, with an active set of 51 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,660:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=2.424e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,661:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.821e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,661:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.730e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,666:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.863e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,667:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.683e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.599e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,668:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=9.076e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,784:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,805:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,805:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=7.014e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,832:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.111e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,833:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=6.100e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,851:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=4.316e+02, with an active set of 52 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,858:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.071e+11, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,858:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.068e+11, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,859:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=3.372e+10, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,860:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 76 iterations, i.e. alpha=1.654e+10, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,899:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,913:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=6.602e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,914:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=3.155e+03, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,914:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.715e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,921:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=2.277e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,924:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=5.724e-01, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,936:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:13,981:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.400e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,982:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=1.363e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,988:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.100e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,989:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=6.063e-04, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:13,991:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=2.996e-04, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,030:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.741e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,030:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=1.247e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,034:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=3.716e+01, with an active set of 58 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,060:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:14,076:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:14,106:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.921e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,108:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.835e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,108:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=5.098e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,109:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=2.887e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,122:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.604e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:47:14,288:INFO:Calculating mean and std
2023-11-03 14:47:14,293:INFO:Creating metrics dataframe
2023-11-03 14:47:14,301:INFO:Uploading results into container
2023-11-03 14:47:14,302:INFO:Uploading model into container now
2023-11-03 14:47:14,303:INFO:_master_model_container: 5
2023-11-03 14:47:14,303:INFO:_display_container: 2
2023-11-03 14:47:14,305:INFO:Lars(random_state=123)
2023-11-03 14:47:14,305:INFO:create_model() successfully completed......................................
2023-11-03 14:47:14,538:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:14,538:INFO:Creating metrics dataframe
2023-11-03 14:47:14,566:INFO:Initializing Lasso Least Angle Regression
2023-11-03 14:47:14,566:INFO:Total runtime is 0.8421889146169027 minutes
2023-11-03 14:47:14,576:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:14,577:INFO:Initializing create_model()
2023-11-03 14:47:14,577:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:14,577:INFO:Checking exceptions
2023-11-03 14:47:14,578:INFO:Importing libraries
2023-11-03 14:47:14,579:INFO:Copying training dataset
2023-11-03 14:47:14,650:INFO:Defining folds
2023-11-03 14:47:14,650:INFO:Declaring metric variables
2023-11-03 14:47:14,663:INFO:Importing untrained model
2023-11-03 14:47:14,675:INFO:Lasso Least Angle Regression Imported successfully
2023-11-03 14:47:14,702:INFO:Starting cross validation
2023-11-03 14:47:14,707:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:15,063:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,138:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,161:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,299:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,327:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,347:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,362:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,372:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,593:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,612:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:47:15,775:INFO:Calculating mean and std
2023-11-03 14:47:15,779:INFO:Creating metrics dataframe
2023-11-03 14:47:15,792:INFO:Uploading results into container
2023-11-03 14:47:15,794:INFO:Uploading model into container now
2023-11-03 14:47:15,795:INFO:_master_model_container: 6
2023-11-03 14:47:15,795:INFO:_display_container: 2
2023-11-03 14:47:15,796:INFO:LassoLars(random_state=123)
2023-11-03 14:47:15,797:INFO:create_model() successfully completed......................................
2023-11-03 14:47:16,006:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:16,007:INFO:Creating metrics dataframe
2023-11-03 14:47:16,023:INFO:Initializing Orthogonal Matching Pursuit
2023-11-03 14:47:16,023:INFO:Total runtime is 0.8664672176043193 minutes
2023-11-03 14:47:16,028:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:16,028:INFO:Initializing create_model()
2023-11-03 14:47:16,029:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:16,029:INFO:Checking exceptions
2023-11-03 14:47:16,029:INFO:Importing libraries
2023-11-03 14:47:16,029:INFO:Copying training dataset
2023-11-03 14:47:16,061:INFO:Defining folds
2023-11-03 14:47:16,062:INFO:Declaring metric variables
2023-11-03 14:47:16,067:INFO:Importing untrained model
2023-11-03 14:47:16,075:INFO:Orthogonal Matching Pursuit Imported successfully
2023-11-03 14:47:16,087:INFO:Starting cross validation
2023-11-03 14:47:16,091:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:16,274:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,279:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,353:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,406:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,457:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,503:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,563:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,589:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,685:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,702:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:47:16,884:INFO:Calculating mean and std
2023-11-03 14:47:16,890:INFO:Creating metrics dataframe
2023-11-03 14:47:16,898:INFO:Uploading results into container
2023-11-03 14:47:16,900:INFO:Uploading model into container now
2023-11-03 14:47:16,901:INFO:_master_model_container: 7
2023-11-03 14:47:16,902:INFO:_display_container: 2
2023-11-03 14:47:16,902:INFO:OrthogonalMatchingPursuit()
2023-11-03 14:47:16,903:INFO:create_model() successfully completed......................................
2023-11-03 14:47:17,094:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:17,095:INFO:Creating metrics dataframe
2023-11-03 14:47:17,108:INFO:Initializing Bayesian Ridge
2023-11-03 14:47:17,108:INFO:Total runtime is 0.8845586021741232 minutes
2023-11-03 14:47:17,112:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:17,113:INFO:Initializing create_model()
2023-11-03 14:47:17,113:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:17,113:INFO:Checking exceptions
2023-11-03 14:47:17,113:INFO:Importing libraries
2023-11-03 14:47:17,113:INFO:Copying training dataset
2023-11-03 14:47:17,144:INFO:Defining folds
2023-11-03 14:47:17,144:INFO:Declaring metric variables
2023-11-03 14:47:17,150:INFO:Importing untrained model
2023-11-03 14:47:17,157:INFO:Bayesian Ridge Imported successfully
2023-11-03 14:47:17,169:INFO:Starting cross validation
2023-11-03 14:47:17,171:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:19,002:INFO:Calculating mean and std
2023-11-03 14:47:19,008:INFO:Creating metrics dataframe
2023-11-03 14:47:19,016:INFO:Uploading results into container
2023-11-03 14:47:19,018:INFO:Uploading model into container now
2023-11-03 14:47:19,020:INFO:_master_model_container: 8
2023-11-03 14:47:19,020:INFO:_display_container: 2
2023-11-03 14:47:19,021:INFO:BayesianRidge()
2023-11-03 14:47:19,022:INFO:create_model() successfully completed......................................
2023-11-03 14:47:19,206:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:19,206:INFO:Creating metrics dataframe
2023-11-03 14:47:19,221:INFO:Initializing Passive Aggressive Regressor
2023-11-03 14:47:19,221:INFO:Total runtime is 0.9197753985722861 minutes
2023-11-03 14:47:19,226:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:19,227:INFO:Initializing create_model()
2023-11-03 14:47:19,228:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:19,228:INFO:Checking exceptions
2023-11-03 14:47:19,228:INFO:Importing libraries
2023-11-03 14:47:19,229:INFO:Copying training dataset
2023-11-03 14:47:19,324:INFO:Defining folds
2023-11-03 14:47:19,325:INFO:Declaring metric variables
2023-11-03 14:47:19,349:INFO:Importing untrained model
2023-11-03 14:47:19,358:INFO:Passive Aggressive Regressor Imported successfully
2023-11-03 14:47:19,397:INFO:Starting cross validation
2023-11-03 14:47:19,399:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:21,235:INFO:Calculating mean and std
2023-11-03 14:47:21,239:INFO:Creating metrics dataframe
2023-11-03 14:47:21,246:INFO:Uploading results into container
2023-11-03 14:47:21,247:INFO:Uploading model into container now
2023-11-03 14:47:21,249:INFO:_master_model_container: 9
2023-11-03 14:47:21,249:INFO:_display_container: 2
2023-11-03 14:47:21,250:INFO:PassiveAggressiveRegressor(random_state=123)
2023-11-03 14:47:21,251:INFO:create_model() successfully completed......................................
2023-11-03 14:47:21,504:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:21,504:INFO:Creating metrics dataframe
2023-11-03 14:47:21,537:INFO:Initializing Huber Regressor
2023-11-03 14:47:21,539:INFO:Total runtime is 0.9584062536557516 minutes
2023-11-03 14:47:21,557:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:21,558:INFO:Initializing create_model()
2023-11-03 14:47:21,559:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:21,559:INFO:Checking exceptions
2023-11-03 14:47:21,559:INFO:Importing libraries
2023-11-03 14:47:21,560:INFO:Copying training dataset
2023-11-03 14:47:21,601:INFO:Defining folds
2023-11-03 14:47:21,602:INFO:Declaring metric variables
2023-11-03 14:47:21,609:INFO:Importing untrained model
2023-11-03 14:47:21,622:INFO:Huber Regressor Imported successfully
2023-11-03 14:47:21,659:INFO:Starting cross validation
2023-11-03 14:47:21,664:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:29,613:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:29,677:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:29,765:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:29,945:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:30,009:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:30,030:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:30,161:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:30,193:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:33,193:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:33,251:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:47:33,410:INFO:Calculating mean and std
2023-11-03 14:47:33,419:INFO:Creating metrics dataframe
2023-11-03 14:47:33,440:INFO:Uploading results into container
2023-11-03 14:47:33,443:INFO:Uploading model into container now
2023-11-03 14:47:33,446:INFO:_master_model_container: 10
2023-11-03 14:47:33,446:INFO:_display_container: 2
2023-11-03 14:47:33,449:INFO:HuberRegressor()
2023-11-03 14:47:33,449:INFO:create_model() successfully completed......................................
2023-11-03 14:47:34,680:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:34,681:INFO:Creating metrics dataframe
2023-11-03 14:47:34,724:INFO:Initializing K Neighbors Regressor
2023-11-03 14:47:34,725:INFO:Total runtime is 1.178164565563202 minutes
2023-11-03 14:47:34,738:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:34,740:INFO:Initializing create_model()
2023-11-03 14:47:34,740:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:34,740:INFO:Checking exceptions
2023-11-03 14:47:34,740:INFO:Importing libraries
2023-11-03 14:47:34,740:INFO:Copying training dataset
2023-11-03 14:47:34,808:INFO:Defining folds
2023-11-03 14:47:34,808:INFO:Declaring metric variables
2023-11-03 14:47:34,829:INFO:Importing untrained model
2023-11-03 14:47:34,854:INFO:K Neighbors Regressor Imported successfully
2023-11-03 14:47:34,893:INFO:Starting cross validation
2023-11-03 14:47:34,898:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:40,477:INFO:Calculating mean and std
2023-11-03 14:47:40,481:INFO:Creating metrics dataframe
2023-11-03 14:47:40,491:INFO:Uploading results into container
2023-11-03 14:47:40,493:INFO:Uploading model into container now
2023-11-03 14:47:40,494:INFO:_master_model_container: 11
2023-11-03 14:47:40,494:INFO:_display_container: 2
2023-11-03 14:47:40,495:INFO:KNeighborsRegressor(n_jobs=-1)
2023-11-03 14:47:40,495:INFO:create_model() successfully completed......................................
2023-11-03 14:47:40,755:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:40,756:INFO:Creating metrics dataframe
2023-11-03 14:47:40,801:INFO:Initializing Decision Tree Regressor
2023-11-03 14:47:40,802:INFO:Total runtime is 1.2794479688008629 minutes
2023-11-03 14:47:40,817:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:40,819:INFO:Initializing create_model()
2023-11-03 14:47:40,819:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:40,819:INFO:Checking exceptions
2023-11-03 14:47:40,820:INFO:Importing libraries
2023-11-03 14:47:40,820:INFO:Copying training dataset
2023-11-03 14:47:40,879:INFO:Defining folds
2023-11-03 14:47:40,880:INFO:Declaring metric variables
2023-11-03 14:47:40,893:INFO:Importing untrained model
2023-11-03 14:47:40,903:INFO:Decision Tree Regressor Imported successfully
2023-11-03 14:47:40,921:INFO:Starting cross validation
2023-11-03 14:47:40,923:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:47:48,387:INFO:Calculating mean and std
2023-11-03 14:47:48,397:INFO:Creating metrics dataframe
2023-11-03 14:47:48,413:INFO:Uploading results into container
2023-11-03 14:47:48,421:INFO:Uploading model into container now
2023-11-03 14:47:48,422:INFO:_master_model_container: 12
2023-11-03 14:47:48,422:INFO:_display_container: 2
2023-11-03 14:47:48,424:INFO:DecisionTreeRegressor(random_state=123)
2023-11-03 14:47:48,424:INFO:create_model() successfully completed......................................
2023-11-03 14:47:48,911:INFO:SubProcess create_model() end ==================================
2023-11-03 14:47:48,912:INFO:Creating metrics dataframe
2023-11-03 14:47:49,004:INFO:Initializing Random Forest Regressor
2023-11-03 14:47:49,004:INFO:Total runtime is 1.4161543170611066 minutes
2023-11-03 14:47:49,032:INFO:SubProcess create_model() called ==================================
2023-11-03 14:47:49,033:INFO:Initializing create_model()
2023-11-03 14:47:49,034:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:47:49,034:INFO:Checking exceptions
2023-11-03 14:47:49,034:INFO:Importing libraries
2023-11-03 14:47:49,035:INFO:Copying training dataset
2023-11-03 14:47:49,114:INFO:Defining folds
2023-11-03 14:47:49,114:INFO:Declaring metric variables
2023-11-03 14:47:49,161:INFO:Importing untrained model
2023-11-03 14:47:49,186:INFO:Random Forest Regressor Imported successfully
2023-11-03 14:47:49,311:INFO:Starting cross validation
2023-11-03 14:47:49,319:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:51:15,375:INFO:Calculating mean and std
2023-11-03 14:51:15,401:INFO:Creating metrics dataframe
2023-11-03 14:51:15,438:INFO:Uploading results into container
2023-11-03 14:51:15,442:INFO:Uploading model into container now
2023-11-03 14:51:15,445:INFO:_master_model_container: 13
2023-11-03 14:51:15,445:INFO:_display_container: 2
2023-11-03 14:51:15,450:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:51:15,450:INFO:create_model() successfully completed......................................
2023-11-03 14:51:16,392:INFO:SubProcess create_model() end ==================================
2023-11-03 14:51:16,392:INFO:Creating metrics dataframe
2023-11-03 14:51:16,428:INFO:Initializing Extra Trees Regressor
2023-11-03 14:51:16,428:INFO:Total runtime is 4.873225502173106 minutes
2023-11-03 14:51:16,436:INFO:SubProcess create_model() called ==================================
2023-11-03 14:51:16,437:INFO:Initializing create_model()
2023-11-03 14:51:16,437:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:51:16,437:INFO:Checking exceptions
2023-11-03 14:51:16,438:INFO:Importing libraries
2023-11-03 14:51:16,438:INFO:Copying training dataset
2023-11-03 14:51:16,543:INFO:Defining folds
2023-11-03 14:51:16,544:INFO:Declaring metric variables
2023-11-03 14:51:16,553:INFO:Importing untrained model
2023-11-03 14:51:16,562:INFO:Extra Trees Regressor Imported successfully
2023-11-03 14:51:16,587:INFO:Starting cross validation
2023-11-03 14:51:16,593:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:52:37,481:INFO:Calculating mean and std
2023-11-03 14:52:37,497:INFO:Creating metrics dataframe
2023-11-03 14:52:37,522:INFO:Uploading results into container
2023-11-03 14:52:37,524:INFO:Uploading model into container now
2023-11-03 14:52:37,528:INFO:_master_model_container: 14
2023-11-03 14:52:37,528:INFO:_display_container: 2
2023-11-03 14:52:37,531:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:52:37,531:INFO:create_model() successfully completed......................................
2023-11-03 14:52:38,103:INFO:SubProcess create_model() end ==================================
2023-11-03 14:52:38,103:INFO:Creating metrics dataframe
2023-11-03 14:52:38,124:INFO:Initializing AdaBoost Regressor
2023-11-03 14:52:38,124:INFO:Total runtime is 6.234825702508291 minutes
2023-11-03 14:52:38,129:INFO:SubProcess create_model() called ==================================
2023-11-03 14:52:38,129:INFO:Initializing create_model()
2023-11-03 14:52:38,130:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:52:38,130:INFO:Checking exceptions
2023-11-03 14:52:38,130:INFO:Importing libraries
2023-11-03 14:52:38,130:INFO:Copying training dataset
2023-11-03 14:52:38,164:INFO:Defining folds
2023-11-03 14:52:38,165:INFO:Declaring metric variables
2023-11-03 14:52:38,172:INFO:Importing untrained model
2023-11-03 14:52:38,177:INFO:AdaBoost Regressor Imported successfully
2023-11-03 14:52:38,188:INFO:Starting cross validation
2023-11-03 14:52:38,190:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:53:07,730:INFO:Calculating mean and std
2023-11-03 14:53:07,736:INFO:Creating metrics dataframe
2023-11-03 14:53:07,743:INFO:Uploading results into container
2023-11-03 14:53:07,745:INFO:Uploading model into container now
2023-11-03 14:53:07,746:INFO:_master_model_container: 15
2023-11-03 14:53:07,746:INFO:_display_container: 2
2023-11-03 14:53:07,746:INFO:AdaBoostRegressor(random_state=123)
2023-11-03 14:53:07,746:INFO:create_model() successfully completed......................................
2023-11-03 14:53:07,912:INFO:SubProcess create_model() end ==================================
2023-11-03 14:53:07,913:INFO:Creating metrics dataframe
2023-11-03 14:53:07,930:INFO:Initializing Gradient Boosting Regressor
2023-11-03 14:53:07,930:INFO:Total runtime is 6.731589082876842 minutes
2023-11-03 14:53:07,935:INFO:SubProcess create_model() called ==================================
2023-11-03 14:53:07,935:INFO:Initializing create_model()
2023-11-03 14:53:07,935:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:53:07,935:INFO:Checking exceptions
2023-11-03 14:53:07,936:INFO:Importing libraries
2023-11-03 14:53:07,936:INFO:Copying training dataset
2023-11-03 14:53:07,958:INFO:Defining folds
2023-11-03 14:53:07,958:INFO:Declaring metric variables
2023-11-03 14:53:07,963:INFO:Importing untrained model
2023-11-03 14:53:07,967:INFO:Gradient Boosting Regressor Imported successfully
2023-11-03 14:53:07,974:INFO:Starting cross validation
2023-11-03 14:53:07,976:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:25,425:INFO:Calculating mean and std
2023-11-03 14:54:25,431:INFO:Creating metrics dataframe
2023-11-03 14:54:25,439:INFO:Uploading results into container
2023-11-03 14:54:25,440:INFO:Uploading model into container now
2023-11-03 14:54:25,442:INFO:_master_model_container: 16
2023-11-03 14:54:25,443:INFO:_display_container: 2
2023-11-03 14:54:25,444:INFO:GradientBoostingRegressor(random_state=123)
2023-11-03 14:54:25,444:INFO:create_model() successfully completed......................................
2023-11-03 14:54:25,617:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:25,618:INFO:Creating metrics dataframe
2023-11-03 14:54:25,631:INFO:Initializing Extreme Gradient Boosting
2023-11-03 14:54:25,632:INFO:Total runtime is 8.026613903045655 minutes
2023-11-03 14:54:25,636:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:25,636:INFO:Initializing create_model()
2023-11-03 14:54:25,636:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:25,636:INFO:Checking exceptions
2023-11-03 14:54:25,636:INFO:Importing libraries
2023-11-03 14:54:25,636:INFO:Copying training dataset
2023-11-03 14:54:25,659:INFO:Defining folds
2023-11-03 14:54:25,659:INFO:Declaring metric variables
2023-11-03 14:54:25,663:INFO:Importing untrained model
2023-11-03 14:54:25,670:INFO:Extreme Gradient Boosting Imported successfully
2023-11-03 14:54:25,678:INFO:Starting cross validation
2023-11-03 14:54:25,680:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:33,843:INFO:Calculating mean and std
2023-11-03 14:54:33,846:INFO:Creating metrics dataframe
2023-11-03 14:54:33,851:INFO:Uploading results into container
2023-11-03 14:54:33,852:INFO:Uploading model into container now
2023-11-03 14:54:33,853:INFO:_master_model_container: 17
2023-11-03 14:54:33,854:INFO:_display_container: 2
2023-11-03 14:54:33,856:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-03 14:54:33,857:INFO:create_model() successfully completed......................................
2023-11-03 14:54:34,061:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:34,061:INFO:Creating metrics dataframe
2023-11-03 14:54:34,081:INFO:Initializing Light Gradient Boosting Machine
2023-11-03 14:54:34,081:INFO:Total runtime is 8.167442119121553 minutes
2023-11-03 14:54:34,086:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:34,087:INFO:Initializing create_model()
2023-11-03 14:54:34,087:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:34,088:INFO:Checking exceptions
2023-11-03 14:54:34,088:INFO:Importing libraries
2023-11-03 14:54:34,088:INFO:Copying training dataset
2023-11-03 14:54:34,123:INFO:Defining folds
2023-11-03 14:54:34,123:INFO:Declaring metric variables
2023-11-03 14:54:34,129:INFO:Importing untrained model
2023-11-03 14:54:34,141:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 14:54:34,164:INFO:Starting cross validation
2023-11-03 14:54:34,166:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:53,819:INFO:Calculating mean and std
2023-11-03 14:54:53,832:INFO:Creating metrics dataframe
2023-11-03 14:54:53,858:INFO:Uploading results into container
2023-11-03 14:54:53,862:INFO:Uploading model into container now
2023-11-03 14:54:53,866:INFO:_master_model_container: 18
2023-11-03 14:54:53,866:INFO:_display_container: 2
2023-11-03 14:54:53,869:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:54:53,869:INFO:create_model() successfully completed......................................
2023-11-03 14:54:54,103:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:54,104:INFO:Creating metrics dataframe
2023-11-03 14:54:54,120:INFO:Initializing Dummy Regressor
2023-11-03 14:54:54,121:INFO:Total runtime is 8.50142823457718 minutes
2023-11-03 14:54:54,124:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:54,125:INFO:Initializing create_model()
2023-11-03 14:54:54,125:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9e3bd7760>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:54,125:INFO:Checking exceptions
2023-11-03 14:54:54,125:INFO:Importing libraries
2023-11-03 14:54:54,126:INFO:Copying training dataset
2023-11-03 14:54:54,147:INFO:Defining folds
2023-11-03 14:54:54,147:INFO:Declaring metric variables
2023-11-03 14:54:54,151:INFO:Importing untrained model
2023-11-03 14:54:54,156:INFO:Dummy Regressor Imported successfully
2023-11-03 14:54:54,166:INFO:Starting cross validation
2023-11-03 14:54:54,167:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:54,899:INFO:Calculating mean and std
2023-11-03 14:54:54,903:INFO:Creating metrics dataframe
2023-11-03 14:54:54,913:INFO:Uploading results into container
2023-11-03 14:54:54,914:INFO:Uploading model into container now
2023-11-03 14:54:54,915:INFO:_master_model_container: 19
2023-11-03 14:54:54,915:INFO:_display_container: 2
2023-11-03 14:54:54,915:INFO:DummyRegressor()
2023-11-03 14:54:54,915:INFO:create_model() successfully completed......................................
2023-11-03 14:54:55,130:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:55,130:INFO:Creating metrics dataframe
2023-11-03 14:54:55,165:INFO:Initializing create_model()
2023-11-03 14:54:55,166:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:55,166:INFO:Checking exceptions
2023-11-03 14:54:55,170:INFO:Importing libraries
2023-11-03 14:54:55,171:INFO:Copying training dataset
2023-11-03 14:54:55,205:INFO:Defining folds
2023-11-03 14:54:55,205:INFO:Declaring metric variables
2023-11-03 14:54:55,205:INFO:Importing untrained model
2023-11-03 14:54:55,205:INFO:Declaring custom model
2023-11-03 14:54:55,206:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 14:54:55,208:INFO:Cross validation set to False
2023-11-03 14:54:55,208:INFO:Fitting Model
2023-11-03 14:54:55,410:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013091 seconds.
2023-11-03 14:54:55,410:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-03 14:54:55,411:INFO:[LightGBM] [Info] Total Bins 9522
2023-11-03 14:54:55,412:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 57
2023-11-03 14:54:55,413:INFO:[LightGBM] [Info] Start training from score 635.645879
2023-11-03 14:54:55,835:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:54:55,835:INFO:create_model() successfully completed......................................
2023-11-03 14:54:56,122:INFO:_master_model_container: 19
2023-11-03 14:54:56,122:INFO:_display_container: 2
2023-11-03 14:54:56,123:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:54:56,123:INFO:compare_models() successfully completed......................................
2023-11-03 14:54:56,124:INFO:Initializing compare_models()
2023-11-03 14:54:56,125:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-11-03 14:54:56,125:INFO:Checking exceptions
2023-11-03 14:54:56,153:INFO:Preparing display monitor
2023-11-03 14:54:56,217:INFO:Initializing Linear Regression
2023-11-03 14:54:56,218:INFO:Total runtime is 7.963180541992187e-06 minutes
2023-11-03 14:54:56,225:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:56,226:INFO:Initializing create_model()
2023-11-03 14:54:56,226:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:56,226:INFO:Checking exceptions
2023-11-03 14:54:56,226:INFO:Importing libraries
2023-11-03 14:54:56,227:INFO:Copying training dataset
2023-11-03 14:54:56,251:INFO:Defining folds
2023-11-03 14:54:56,252:INFO:Declaring metric variables
2023-11-03 14:54:56,259:INFO:Importing untrained model
2023-11-03 14:54:56,266:INFO:Linear Regression Imported successfully
2023-11-03 14:54:56,278:INFO:Starting cross validation
2023-11-03 14:54:56,280:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:54:57,274:INFO:Calculating mean and std
2023-11-03 14:54:57,279:INFO:Creating metrics dataframe
2023-11-03 14:54:57,287:INFO:Uploading results into container
2023-11-03 14:54:57,288:INFO:Uploading model into container now
2023-11-03 14:54:57,289:INFO:_master_model_container: 1
2023-11-03 14:54:57,290:INFO:_display_container: 2
2023-11-03 14:54:57,291:INFO:LinearRegression(n_jobs=-1)
2023-11-03 14:54:57,291:INFO:create_model() successfully completed......................................
2023-11-03 14:54:57,463:INFO:SubProcess create_model() end ==================================
2023-11-03 14:54:57,463:INFO:Creating metrics dataframe
2023-11-03 14:54:57,473:INFO:Initializing Lasso Regression
2023-11-03 14:54:57,473:INFO:Total runtime is 0.02092766761779785 minutes
2023-11-03 14:54:57,477:INFO:SubProcess create_model() called ==================================
2023-11-03 14:54:57,477:INFO:Initializing create_model()
2023-11-03 14:54:57,477:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:54:57,478:INFO:Checking exceptions
2023-11-03 14:54:57,478:INFO:Importing libraries
2023-11-03 14:54:57,479:INFO:Copying training dataset
2023-11-03 14:54:57,500:INFO:Defining folds
2023-11-03 14:54:57,500:INFO:Declaring metric variables
2023-11-03 14:54:57,505:INFO:Importing untrained model
2023-11-03 14:54:57,511:INFO:Lasso Regression Imported successfully
2023-11-03 14:54:57,520:INFO:Starting cross validation
2023-11-03 14:54:57,524:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:04,851:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.199e+07, tolerance: 7.608e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:04,926:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.297e+07, tolerance: 7.710e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,233:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.144e+07, tolerance: 7.656e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,373:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.240e+07, tolerance: 7.711e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,519:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.234e+07, tolerance: 7.715e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,615:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.276e+07, tolerance: 7.797e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,660:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.237e+07, tolerance: 7.670e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:05,718:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.103e+07, tolerance: 7.618e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:08,811:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.204e+07, tolerance: 7.640e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:08,856:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.288e+07, tolerance: 7.697e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:09,009:INFO:Calculating mean and std
2023-11-03 14:55:09,012:INFO:Creating metrics dataframe
2023-11-03 14:55:09,019:INFO:Uploading results into container
2023-11-03 14:55:09,021:INFO:Uploading model into container now
2023-11-03 14:55:09,022:INFO:_master_model_container: 2
2023-11-03 14:55:09,022:INFO:_display_container: 2
2023-11-03 14:55:09,023:INFO:Lasso(random_state=123)
2023-11-03 14:55:09,024:INFO:create_model() successfully completed......................................
2023-11-03 14:55:09,260:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:09,260:INFO:Creating metrics dataframe
2023-11-03 14:55:09,276:INFO:Initializing Ridge Regression
2023-11-03 14:55:09,276:INFO:Total runtime is 0.2176552136739095 minutes
2023-11-03 14:55:09,283:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:09,284:INFO:Initializing create_model()
2023-11-03 14:55:09,285:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:09,285:INFO:Checking exceptions
2023-11-03 14:55:09,286:INFO:Importing libraries
2023-11-03 14:55:09,286:INFO:Copying training dataset
2023-11-03 14:55:09,317:INFO:Defining folds
2023-11-03 14:55:09,317:INFO:Declaring metric variables
2023-11-03 14:55:09,323:INFO:Importing untrained model
2023-11-03 14:55:09,334:INFO:Ridge Regression Imported successfully
2023-11-03 14:55:09,362:INFO:Starting cross validation
2023-11-03 14:55:09,366:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:09,993:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.97211e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,025:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=2.64292e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,076:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.94757e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,103:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.93339e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,108:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.92872e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,469:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=3.95681e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 14:55:10,777:INFO:Calculating mean and std
2023-11-03 14:55:10,785:INFO:Creating metrics dataframe
2023-11-03 14:55:10,802:INFO:Uploading results into container
2023-11-03 14:55:10,805:INFO:Uploading model into container now
2023-11-03 14:55:10,807:INFO:_master_model_container: 3
2023-11-03 14:55:10,808:INFO:_display_container: 2
2023-11-03 14:55:10,809:INFO:Ridge(random_state=123)
2023-11-03 14:55:10,809:INFO:create_model() successfully completed......................................
2023-11-03 14:55:11,190:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:11,190:INFO:Creating metrics dataframe
2023-11-03 14:55:11,233:INFO:Initializing Elastic Net
2023-11-03 14:55:11,234:INFO:Total runtime is 0.2502824465433756 minutes
2023-11-03 14:55:11,246:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:11,247:INFO:Initializing create_model()
2023-11-03 14:55:11,248:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:11,248:INFO:Checking exceptions
2023-11-03 14:55:11,249:INFO:Importing libraries
2023-11-03 14:55:11,251:INFO:Copying training dataset
2023-11-03 14:55:11,318:INFO:Defining folds
2023-11-03 14:55:11,319:INFO:Declaring metric variables
2023-11-03 14:55:11,347:INFO:Importing untrained model
2023-11-03 14:55:11,366:INFO:Elastic Net Imported successfully
2023-11-03 14:55:11,404:INFO:Starting cross validation
2023-11-03 14:55:11,410:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:18,008:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.256e+07, tolerance: 7.670e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,120:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.167e+07, tolerance: 7.656e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,181:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.319e+07, tolerance: 7.710e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,186:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.255e+07, tolerance: 7.715e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,228:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.222e+07, tolerance: 7.608e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,300:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.261e+07, tolerance: 7.711e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,301:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.298e+07, tolerance: 7.797e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:18,359:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.121e+07, tolerance: 7.618e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:20,401:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.224e+07, tolerance: 7.640e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:20,498:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.309e+07, tolerance: 7.697e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 14:55:20,644:INFO:Calculating mean and std
2023-11-03 14:55:20,648:INFO:Creating metrics dataframe
2023-11-03 14:55:20,656:INFO:Uploading results into container
2023-11-03 14:55:20,657:INFO:Uploading model into container now
2023-11-03 14:55:20,658:INFO:_master_model_container: 4
2023-11-03 14:55:20,659:INFO:_display_container: 2
2023-11-03 14:55:20,660:INFO:ElasticNet(random_state=123)
2023-11-03 14:55:20,660:INFO:create_model() successfully completed......................................
2023-11-03 14:55:20,880:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:20,880:INFO:Creating metrics dataframe
2023-11-03 14:55:20,902:INFO:Initializing Least Angle Regression
2023-11-03 14:55:20,902:INFO:Total runtime is 0.4114136298497517 minutes
2023-11-03 14:55:20,910:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:20,911:INFO:Initializing create_model()
2023-11-03 14:55:20,911:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:20,911:INFO:Checking exceptions
2023-11-03 14:55:20,912:INFO:Importing libraries
2023-11-03 14:55:20,912:INFO:Copying training dataset
2023-11-03 14:55:20,949:INFO:Defining folds
2023-11-03 14:55:20,950:INFO:Declaring metric variables
2023-11-03 14:55:20,960:INFO:Importing untrained model
2023-11-03 14:55:20,971:INFO:Least Angle Regression Imported successfully
2023-11-03 14:55:20,989:INFO:Starting cross validation
2023-11-03 14:55:20,992:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:21,284:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,342:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,374:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,516:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,528:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.875e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,529:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=3.152e+04, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,530:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.974e+04, with an active set of 55 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,531:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=2.119e+04, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,532:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=7.482e+03, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.996e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,534:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=2.707e-03, with an active set of 39 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,572:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=2.070e+07, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,573:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.943e+07, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,574:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=1.465e+07, with an active set of 54 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,574:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=2.515e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,575:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,576:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=7.201e+06, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,577:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=6.765e+06, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,578:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=4.521e+06, with an active set of 55 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,578:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=5.263e+02, with an active set of 56 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,578:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=6.642e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,628:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,628:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,646:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=3.269e+01, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,668:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=2.215e+06, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,669:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=1.843e+06, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,670:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=9.718e+05, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,673:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=4.802e+05, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,677:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=9.032e-02, with an active set of 57 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,681:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,738:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=3.298e-03, with an active set of 34 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,738:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.668e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,739:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.937e-03, with an active set of 35 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,740:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.726e-03, with an active set of 36 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,750:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=3.072e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,751:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.678e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,751:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=2.316e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,751:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=2.103e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,753:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=2.010e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,756:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.541e-02, with an active set of 52 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,757:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=7.564e-03, with an active set of 53 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,758:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=2.211e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,758:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 88 iterations, i.e. alpha=1.044e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,760:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.432e+04, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,761:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=4.311e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,762:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 74 iterations, i.e. alpha=1.285e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,776:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.097e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,777:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.022e-03, with an active set of 43 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,789:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 64 iterations, i.e. alpha=7.609e+01, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,791:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.894e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,792:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.364e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,795:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=6.150e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,795:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.199e+01, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,866:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,882:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:21,899:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.973e-03, with an active set of 39 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,899:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 43 iterations, i.e. alpha=1.530e-03, with an active set of 40 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,906:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=5.263e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,907:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=3.571e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,907:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.401e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,908:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=7.021e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,909:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=6.445e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 7.068e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,909:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 73 iterations, i.e. alpha=1.646e-03, with an active set of 55 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,927:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=6.405e+04, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:21,927:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=6.346e+04, with an active set of 56 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 14:55:22,078:INFO:Calculating mean and std
2023-11-03 14:55:22,083:INFO:Creating metrics dataframe
2023-11-03 14:55:22,094:INFO:Uploading results into container
2023-11-03 14:55:22,096:INFO:Uploading model into container now
2023-11-03 14:55:22,098:INFO:_master_model_container: 5
2023-11-03 14:55:22,098:INFO:_display_container: 2
2023-11-03 14:55:22,100:INFO:Lars(random_state=123)
2023-11-03 14:55:22,101:INFO:create_model() successfully completed......................................
2023-11-03 14:55:22,341:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:22,341:INFO:Creating metrics dataframe
2023-11-03 14:55:22,361:INFO:Initializing Lasso Least Angle Regression
2023-11-03 14:55:22,361:INFO:Total runtime is 0.4357351620992024 minutes
2023-11-03 14:55:22,370:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:22,370:INFO:Initializing create_model()
2023-11-03 14:55:22,370:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:22,371:INFO:Checking exceptions
2023-11-03 14:55:22,371:INFO:Importing libraries
2023-11-03 14:55:22,371:INFO:Copying training dataset
2023-11-03 14:55:22,401:INFO:Defining folds
2023-11-03 14:55:22,401:INFO:Declaring metric variables
2023-11-03 14:55:22,408:INFO:Importing untrained model
2023-11-03 14:55:22,416:INFO:Lasso Least Angle Regression Imported successfully
2023-11-03 14:55:22,432:INFO:Starting cross validation
2023-11-03 14:55:22,440:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:22,602:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,640:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,700:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,719:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,783:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,808:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,859:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,895:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,922:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:22,952:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 14:55:23,110:INFO:Calculating mean and std
2023-11-03 14:55:23,114:INFO:Creating metrics dataframe
2023-11-03 14:55:23,120:INFO:Uploading results into container
2023-11-03 14:55:23,122:INFO:Uploading model into container now
2023-11-03 14:55:23,123:INFO:_master_model_container: 6
2023-11-03 14:55:23,123:INFO:_display_container: 2
2023-11-03 14:55:23,124:INFO:LassoLars(random_state=123)
2023-11-03 14:55:23,124:INFO:create_model() successfully completed......................................
2023-11-03 14:55:23,291:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:23,291:INFO:Creating metrics dataframe
2023-11-03 14:55:23,306:INFO:Initializing Orthogonal Matching Pursuit
2023-11-03 14:55:23,306:INFO:Total runtime is 0.45147597789764393 minutes
2023-11-03 14:55:23,309:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:23,310:INFO:Initializing create_model()
2023-11-03 14:55:23,310:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:23,311:INFO:Checking exceptions
2023-11-03 14:55:23,311:INFO:Importing libraries
2023-11-03 14:55:23,311:INFO:Copying training dataset
2023-11-03 14:55:23,334:INFO:Defining folds
2023-11-03 14:55:23,334:INFO:Declaring metric variables
2023-11-03 14:55:23,339:INFO:Importing untrained model
2023-11-03 14:55:23,345:INFO:Orthogonal Matching Pursuit Imported successfully
2023-11-03 14:55:23,356:INFO:Starting cross validation
2023-11-03 14:55:23,358:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:23,515:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,518:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,558:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,625:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,667:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,697:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,741:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,763:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,792:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,820:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 14:55:23,983:INFO:Calculating mean and std
2023-11-03 14:55:23,989:INFO:Creating metrics dataframe
2023-11-03 14:55:23,997:INFO:Uploading results into container
2023-11-03 14:55:23,998:INFO:Uploading model into container now
2023-11-03 14:55:23,999:INFO:_master_model_container: 7
2023-11-03 14:55:23,999:INFO:_display_container: 2
2023-11-03 14:55:24,000:INFO:OrthogonalMatchingPursuit()
2023-11-03 14:55:24,000:INFO:create_model() successfully completed......................................
2023-11-03 14:55:24,175:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:24,175:INFO:Creating metrics dataframe
2023-11-03 14:55:24,190:INFO:Initializing Bayesian Ridge
2023-11-03 14:55:24,191:INFO:Total runtime is 0.46622438033421826 minutes
2023-11-03 14:55:24,195:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:24,196:INFO:Initializing create_model()
2023-11-03 14:55:24,196:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:24,196:INFO:Checking exceptions
2023-11-03 14:55:24,197:INFO:Importing libraries
2023-11-03 14:55:24,197:INFO:Copying training dataset
2023-11-03 14:55:24,223:INFO:Defining folds
2023-11-03 14:55:24,224:INFO:Declaring metric variables
2023-11-03 14:55:24,228:INFO:Importing untrained model
2023-11-03 14:55:24,234:INFO:Bayesian Ridge Imported successfully
2023-11-03 14:55:24,244:INFO:Starting cross validation
2023-11-03 14:55:24,246:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:25,469:INFO:Calculating mean and std
2023-11-03 14:55:25,472:INFO:Creating metrics dataframe
2023-11-03 14:55:25,478:INFO:Uploading results into container
2023-11-03 14:55:25,479:INFO:Uploading model into container now
2023-11-03 14:55:25,480:INFO:_master_model_container: 8
2023-11-03 14:55:25,480:INFO:_display_container: 2
2023-11-03 14:55:25,480:INFO:BayesianRidge()
2023-11-03 14:55:25,480:INFO:create_model() successfully completed......................................
2023-11-03 14:55:25,658:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:25,658:INFO:Creating metrics dataframe
2023-11-03 14:55:25,680:INFO:Initializing Passive Aggressive Regressor
2023-11-03 14:55:25,680:INFO:Total runtime is 0.491053815682729 minutes
2023-11-03 14:55:25,688:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:25,701:INFO:Initializing create_model()
2023-11-03 14:55:25,702:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:25,702:INFO:Checking exceptions
2023-11-03 14:55:25,702:INFO:Importing libraries
2023-11-03 14:55:25,702:INFO:Copying training dataset
2023-11-03 14:55:25,730:INFO:Defining folds
2023-11-03 14:55:25,730:INFO:Declaring metric variables
2023-11-03 14:55:25,737:INFO:Importing untrained model
2023-11-03 14:55:25,742:INFO:Passive Aggressive Regressor Imported successfully
2023-11-03 14:55:25,754:INFO:Starting cross validation
2023-11-03 14:55:25,756:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:26,872:INFO:Calculating mean and std
2023-11-03 14:55:26,901:INFO:Creating metrics dataframe
2023-11-03 14:55:26,912:INFO:Uploading results into container
2023-11-03 14:55:26,914:INFO:Uploading model into container now
2023-11-03 14:55:26,916:INFO:_master_model_container: 9
2023-11-03 14:55:26,917:INFO:_display_container: 2
2023-11-03 14:55:26,918:INFO:PassiveAggressiveRegressor(random_state=123)
2023-11-03 14:55:26,918:INFO:create_model() successfully completed......................................
2023-11-03 14:55:27,131:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:27,131:INFO:Creating metrics dataframe
2023-11-03 14:55:27,155:INFO:Initializing Huber Regressor
2023-11-03 14:55:27,155:INFO:Total runtime is 0.51563903093338 minutes
2023-11-03 14:55:27,168:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:27,169:INFO:Initializing create_model()
2023-11-03 14:55:27,169:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:27,169:INFO:Checking exceptions
2023-11-03 14:55:27,170:INFO:Importing libraries
2023-11-03 14:55:27,170:INFO:Copying training dataset
2023-11-03 14:55:27,210:INFO:Defining folds
2023-11-03 14:55:27,210:INFO:Declaring metric variables
2023-11-03 14:55:27,221:INFO:Importing untrained model
2023-11-03 14:55:27,227:INFO:Huber Regressor Imported successfully
2023-11-03 14:55:27,241:INFO:Starting cross validation
2023-11-03 14:55:27,245:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:30,694:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,712:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,745:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,814:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,880:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,932:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:30,979:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:31,044:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:32,326:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:32,328:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 14:55:32,461:INFO:Calculating mean and std
2023-11-03 14:55:32,466:INFO:Creating metrics dataframe
2023-11-03 14:55:32,472:INFO:Uploading results into container
2023-11-03 14:55:32,473:INFO:Uploading model into container now
2023-11-03 14:55:32,475:INFO:_master_model_container: 10
2023-11-03 14:55:32,475:INFO:_display_container: 2
2023-11-03 14:55:32,476:INFO:HuberRegressor()
2023-11-03 14:55:32,476:INFO:create_model() successfully completed......................................
2023-11-03 14:55:32,651:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:32,651:INFO:Creating metrics dataframe
2023-11-03 14:55:32,669:INFO:Initializing K Neighbors Regressor
2023-11-03 14:55:32,670:INFO:Total runtime is 0.6075413505236307 minutes
2023-11-03 14:55:32,675:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:32,675:INFO:Initializing create_model()
2023-11-03 14:55:32,675:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:32,675:INFO:Checking exceptions
2023-11-03 14:55:32,675:INFO:Importing libraries
2023-11-03 14:55:32,676:INFO:Copying training dataset
2023-11-03 14:55:32,706:INFO:Defining folds
2023-11-03 14:55:32,707:INFO:Declaring metric variables
2023-11-03 14:55:32,716:INFO:Importing untrained model
2023-11-03 14:55:32,723:INFO:K Neighbors Regressor Imported successfully
2023-11-03 14:55:32,739:INFO:Starting cross validation
2023-11-03 14:55:32,742:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:36,433:INFO:Calculating mean and std
2023-11-03 14:55:36,438:INFO:Creating metrics dataframe
2023-11-03 14:55:36,454:INFO:Uploading results into container
2023-11-03 14:55:36,457:INFO:Uploading model into container now
2023-11-03 14:55:36,458:INFO:_master_model_container: 11
2023-11-03 14:55:36,458:INFO:_display_container: 2
2023-11-03 14:55:36,461:INFO:KNeighborsRegressor(n_jobs=-1)
2023-11-03 14:55:36,461:INFO:create_model() successfully completed......................................
2023-11-03 14:55:36,798:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:36,799:INFO:Creating metrics dataframe
2023-11-03 14:55:36,828:INFO:Initializing Decision Tree Regressor
2023-11-03 14:55:36,828:INFO:Total runtime is 0.6768476168314614 minutes
2023-11-03 14:55:36,837:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:36,837:INFO:Initializing create_model()
2023-11-03 14:55:36,837:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:36,838:INFO:Checking exceptions
2023-11-03 14:55:36,838:INFO:Importing libraries
2023-11-03 14:55:36,838:INFO:Copying training dataset
2023-11-03 14:55:36,880:INFO:Defining folds
2023-11-03 14:55:36,881:INFO:Declaring metric variables
2023-11-03 14:55:36,890:INFO:Importing untrained model
2023-11-03 14:55:36,902:INFO:Decision Tree Regressor Imported successfully
2023-11-03 14:55:36,923:INFO:Starting cross validation
2023-11-03 14:55:36,925:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:55:42,082:INFO:Calculating mean and std
2023-11-03 14:55:42,084:INFO:Creating metrics dataframe
2023-11-03 14:55:42,088:INFO:Uploading results into container
2023-11-03 14:55:42,090:INFO:Uploading model into container now
2023-11-03 14:55:42,091:INFO:_master_model_container: 12
2023-11-03 14:55:42,091:INFO:_display_container: 2
2023-11-03 14:55:42,092:INFO:DecisionTreeRegressor(random_state=123)
2023-11-03 14:55:42,092:INFO:create_model() successfully completed......................................
2023-11-03 14:55:42,299:INFO:SubProcess create_model() end ==================================
2023-11-03 14:55:42,300:INFO:Creating metrics dataframe
2023-11-03 14:55:42,319:INFO:Initializing Random Forest Regressor
2023-11-03 14:55:42,319:INFO:Total runtime is 0.7683629631996154 minutes
2023-11-03 14:55:42,324:INFO:SubProcess create_model() called ==================================
2023-11-03 14:55:42,324:INFO:Initializing create_model()
2023-11-03 14:55:42,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:55:42,325:INFO:Checking exceptions
2023-11-03 14:55:42,325:INFO:Importing libraries
2023-11-03 14:55:42,325:INFO:Copying training dataset
2023-11-03 14:55:42,352:INFO:Defining folds
2023-11-03 14:55:42,353:INFO:Declaring metric variables
2023-11-03 14:55:42,360:INFO:Importing untrained model
2023-11-03 14:55:42,369:INFO:Random Forest Regressor Imported successfully
2023-11-03 14:55:42,381:INFO:Starting cross validation
2023-11-03 14:55:42,383:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:58:16,975:INFO:Calculating mean and std
2023-11-03 14:58:16,990:INFO:Creating metrics dataframe
2023-11-03 14:58:17,009:INFO:Uploading results into container
2023-11-03 14:58:17,011:INFO:Uploading model into container now
2023-11-03 14:58:17,014:INFO:_master_model_container: 13
2023-11-03 14:58:17,014:INFO:_display_container: 2
2023-11-03 14:58:17,016:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:58:17,016:INFO:create_model() successfully completed......................................
2023-11-03 14:58:17,760:INFO:SubProcess create_model() end ==================================
2023-11-03 14:58:17,760:INFO:Creating metrics dataframe
2023-11-03 14:58:17,781:INFO:Initializing Extra Trees Regressor
2023-11-03 14:58:17,781:INFO:Total runtime is 3.3594010670979815 minutes
2023-11-03 14:58:17,787:INFO:SubProcess create_model() called ==================================
2023-11-03 14:58:17,788:INFO:Initializing create_model()
2023-11-03 14:58:17,788:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:58:17,788:INFO:Checking exceptions
2023-11-03 14:58:17,788:INFO:Importing libraries
2023-11-03 14:58:17,788:INFO:Copying training dataset
2023-11-03 14:58:17,840:INFO:Defining folds
2023-11-03 14:58:17,841:INFO:Declaring metric variables
2023-11-03 14:58:17,846:INFO:Importing untrained model
2023-11-03 14:58:17,854:INFO:Extra Trees Regressor Imported successfully
2023-11-03 14:58:17,867:INFO:Starting cross validation
2023-11-03 14:58:17,869:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:59:21,058:INFO:Calculating mean and std
2023-11-03 14:59:21,084:INFO:Creating metrics dataframe
2023-11-03 14:59:21,118:INFO:Uploading results into container
2023-11-03 14:59:21,122:INFO:Uploading model into container now
2023-11-03 14:59:21,125:INFO:_master_model_container: 14
2023-11-03 14:59:21,126:INFO:_display_container: 2
2023-11-03 14:59:21,129:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-11-03 14:59:21,130:INFO:create_model() successfully completed......................................
2023-11-03 14:59:21,613:INFO:SubProcess create_model() end ==================================
2023-11-03 14:59:21,613:INFO:Creating metrics dataframe
2023-11-03 14:59:21,632:INFO:Initializing AdaBoost Regressor
2023-11-03 14:59:21,632:INFO:Total runtime is 4.423589499791463 minutes
2023-11-03 14:59:21,638:INFO:SubProcess create_model() called ==================================
2023-11-03 14:59:21,638:INFO:Initializing create_model()
2023-11-03 14:59:21,638:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:59:21,639:INFO:Checking exceptions
2023-11-03 14:59:21,639:INFO:Importing libraries
2023-11-03 14:59:21,639:INFO:Copying training dataset
2023-11-03 14:59:21,685:INFO:Defining folds
2023-11-03 14:59:21,686:INFO:Declaring metric variables
2023-11-03 14:59:21,694:INFO:Importing untrained model
2023-11-03 14:59:21,708:INFO:AdaBoost Regressor Imported successfully
2023-11-03 14:59:21,731:INFO:Starting cross validation
2023-11-03 14:59:21,739:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 14:59:53,080:INFO:Calculating mean and std
2023-11-03 14:59:53,086:INFO:Creating metrics dataframe
2023-11-03 14:59:53,097:INFO:Uploading results into container
2023-11-03 14:59:53,099:INFO:Uploading model into container now
2023-11-03 14:59:53,101:INFO:_master_model_container: 15
2023-11-03 14:59:53,101:INFO:_display_container: 2
2023-11-03 14:59:53,102:INFO:AdaBoostRegressor(random_state=123)
2023-11-03 14:59:53,102:INFO:create_model() successfully completed......................................
2023-11-03 14:59:53,356:INFO:SubProcess create_model() end ==================================
2023-11-03 14:59:53,356:INFO:Creating metrics dataframe
2023-11-03 14:59:53,397:INFO:Initializing Gradient Boosting Regressor
2023-11-03 14:59:53,399:INFO:Total runtime is 4.953032247225443 minutes
2023-11-03 14:59:53,411:INFO:SubProcess create_model() called ==================================
2023-11-03 14:59:53,412:INFO:Initializing create_model()
2023-11-03 14:59:53,412:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 14:59:53,412:INFO:Checking exceptions
2023-11-03 14:59:53,413:INFO:Importing libraries
2023-11-03 14:59:53,413:INFO:Copying training dataset
2023-11-03 14:59:53,495:INFO:Defining folds
2023-11-03 14:59:53,497:INFO:Declaring metric variables
2023-11-03 14:59:53,510:INFO:Importing untrained model
2023-11-03 14:59:53,520:INFO:Gradient Boosting Regressor Imported successfully
2023-11-03 14:59:53,547:INFO:Starting cross validation
2023-11-03 14:59:53,554:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:01:01,425:INFO:Calculating mean and std
2023-11-03 15:01:01,447:INFO:Creating metrics dataframe
2023-11-03 15:01:01,482:INFO:Uploading results into container
2023-11-03 15:01:01,486:INFO:Uploading model into container now
2023-11-03 15:01:01,490:INFO:_master_model_container: 16
2023-11-03 15:01:01,491:INFO:_display_container: 2
2023-11-03 15:01:01,496:INFO:GradientBoostingRegressor(random_state=123)
2023-11-03 15:01:01,497:INFO:create_model() successfully completed......................................
2023-11-03 15:01:02,094:INFO:SubProcess create_model() end ==================================
2023-11-03 15:01:02,094:INFO:Creating metrics dataframe
2023-11-03 15:01:02,115:INFO:Initializing Extreme Gradient Boosting
2023-11-03 15:01:02,115:INFO:Total runtime is 6.098301831881205 minutes
2023-11-03 15:01:02,120:INFO:SubProcess create_model() called ==================================
2023-11-03 15:01:02,120:INFO:Initializing create_model()
2023-11-03 15:01:02,120:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:01:02,120:INFO:Checking exceptions
2023-11-03 15:01:02,121:INFO:Importing libraries
2023-11-03 15:01:02,121:INFO:Copying training dataset
2023-11-03 15:01:02,153:INFO:Defining folds
2023-11-03 15:01:02,154:INFO:Declaring metric variables
2023-11-03 15:01:02,161:INFO:Importing untrained model
2023-11-03 15:01:02,292:INFO:Extreme Gradient Boosting Imported successfully
2023-11-03 15:01:02,334:INFO:Starting cross validation
2023-11-03 15:01:02,338:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:01:18,486:INFO:Calculating mean and std
2023-11-03 15:01:18,493:INFO:Creating metrics dataframe
2023-11-03 15:01:18,505:INFO:Uploading results into container
2023-11-03 15:01:18,507:INFO:Uploading model into container now
2023-11-03 15:01:18,509:INFO:_master_model_container: 17
2023-11-03 15:01:18,510:INFO:_display_container: 2
2023-11-03 15:01:18,513:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-03 15:01:18,513:INFO:create_model() successfully completed......................................
2023-11-03 15:01:18,846:INFO:SubProcess create_model() end ==================================
2023-11-03 15:01:18,847:INFO:Creating metrics dataframe
2023-11-03 15:01:18,938:INFO:Initializing Light Gradient Boosting Machine
2023-11-03 15:01:18,939:INFO:Total runtime is 6.378700212637583 minutes
2023-11-03 15:01:18,965:INFO:SubProcess create_model() called ==================================
2023-11-03 15:01:18,966:INFO:Initializing create_model()
2023-11-03 15:01:18,967:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:01:18,967:INFO:Checking exceptions
2023-11-03 15:01:18,967:INFO:Importing libraries
2023-11-03 15:01:18,968:INFO:Copying training dataset
2023-11-03 15:01:19,419:INFO:Defining folds
2023-11-03 15:01:19,420:INFO:Declaring metric variables
2023-11-03 15:01:19,433:INFO:Importing untrained model
2023-11-03 15:01:19,454:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 15:01:19,705:INFO:Starting cross validation
2023-11-03 15:01:19,712:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:08,253:INFO:Calculating mean and std
2023-11-03 15:02:08,273:INFO:Creating metrics dataframe
2023-11-03 15:02:08,302:INFO:Uploading results into container
2023-11-03 15:02:08,305:INFO:Uploading model into container now
2023-11-03 15:02:08,310:INFO:_master_model_container: 18
2023-11-03 15:02:08,310:INFO:_display_container: 2
2023-11-03 15:02:08,313:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:02:08,313:INFO:create_model() successfully completed......................................
2023-11-03 15:02:09,002:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:09,002:INFO:Creating metrics dataframe
2023-11-03 15:02:09,027:INFO:Initializing Dummy Regressor
2023-11-03 15:02:09,028:INFO:Total runtime is 7.213509631156922 minutes
2023-11-03 15:02:09,032:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:09,033:INFO:Initializing create_model()
2023-11-03 15:02:09,033:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fc9de24fd30>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:09,033:INFO:Checking exceptions
2023-11-03 15:02:09,033:INFO:Importing libraries
2023-11-03 15:02:09,034:INFO:Copying training dataset
2023-11-03 15:02:09,147:INFO:Defining folds
2023-11-03 15:02:09,147:INFO:Declaring metric variables
2023-11-03 15:02:09,156:INFO:Importing untrained model
2023-11-03 15:02:09,165:INFO:Dummy Regressor Imported successfully
2023-11-03 15:02:09,181:INFO:Starting cross validation
2023-11-03 15:02:09,185:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:10,091:INFO:Calculating mean and std
2023-11-03 15:02:10,096:INFO:Creating metrics dataframe
2023-11-03 15:02:10,103:INFO:Uploading results into container
2023-11-03 15:02:10,105:INFO:Uploading model into container now
2023-11-03 15:02:10,106:INFO:_master_model_container: 19
2023-11-03 15:02:10,106:INFO:_display_container: 2
2023-11-03 15:02:10,107:INFO:DummyRegressor()
2023-11-03 15:02:10,107:INFO:create_model() successfully completed......................................
2023-11-03 15:02:10,275:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:10,275:INFO:Creating metrics dataframe
2023-11-03 15:02:10,308:INFO:Initializing create_model()
2023-11-03 15:02:10,308:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:10,309:INFO:Checking exceptions
2023-11-03 15:02:10,314:INFO:Importing libraries
2023-11-03 15:02:10,314:INFO:Copying training dataset
2023-11-03 15:02:10,338:INFO:Defining folds
2023-11-03 15:02:10,339:INFO:Declaring metric variables
2023-11-03 15:02:10,339:INFO:Importing untrained model
2023-11-03 15:02:10,339:INFO:Declaring custom model
2023-11-03 15:02:10,340:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 15:02:10,342:INFO:Cross validation set to False
2023-11-03 15:02:10,342:INFO:Fitting Model
2023-11-03 15:02:10,510:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011705 seconds.
2023-11-03 15:02:10,510:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-03 15:02:10,510:INFO:[LightGBM] [Info] Total Bins 9588
2023-11-03 15:02:10,512:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 57
2023-11-03 15:02:10,513:INFO:[LightGBM] [Info] Start training from score 94.273259
2023-11-03 15:02:11,069:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:02:11,069:INFO:create_model() successfully completed......................................
2023-11-03 15:02:11,348:INFO:_master_model_container: 19
2023-11-03 15:02:11,348:INFO:_display_container: 2
2023-11-03 15:02:11,349:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:02:11,349:INFO:compare_models() successfully completed......................................
2023-11-03 15:02:11,350:INFO:Initializing compare_models()
2023-11-03 15:02:11,350:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2023-11-03 15:02:11,350:INFO:Checking exceptions
2023-11-03 15:02:11,401:INFO:Preparing display monitor
2023-11-03 15:02:11,671:INFO:Initializing Linear Regression
2023-11-03 15:02:11,673:INFO:Total runtime is 3.701845804850261e-05 minutes
2023-11-03 15:02:11,685:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:11,689:INFO:Initializing create_model()
2023-11-03 15:02:11,690:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:11,690:INFO:Checking exceptions
2023-11-03 15:02:11,690:INFO:Importing libraries
2023-11-03 15:02:11,690:INFO:Copying training dataset
2023-11-03 15:02:11,766:INFO:Defining folds
2023-11-03 15:02:11,766:INFO:Declaring metric variables
2023-11-03 15:02:11,772:INFO:Importing untrained model
2023-11-03 15:02:11,785:INFO:Linear Regression Imported successfully
2023-11-03 15:02:11,802:INFO:Starting cross validation
2023-11-03 15:02:11,805:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:12,785:INFO:Calculating mean and std
2023-11-03 15:02:12,788:INFO:Creating metrics dataframe
2023-11-03 15:02:12,795:INFO:Uploading results into container
2023-11-03 15:02:12,796:INFO:Uploading model into container now
2023-11-03 15:02:12,797:INFO:_master_model_container: 1
2023-11-03 15:02:12,797:INFO:_display_container: 2
2023-11-03 15:02:12,798:INFO:LinearRegression(n_jobs=-1)
2023-11-03 15:02:12,798:INFO:create_model() successfully completed......................................
2023-11-03 15:02:12,972:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:12,972:INFO:Creating metrics dataframe
2023-11-03 15:02:12,990:INFO:Initializing Lasso Regression
2023-11-03 15:02:12,990:INFO:Total runtime is 0.021985952059427896 minutes
2023-11-03 15:02:12,999:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:12,999:INFO:Initializing create_model()
2023-11-03 15:02:13,000:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:13,000:INFO:Checking exceptions
2023-11-03 15:02:13,000:INFO:Importing libraries
2023-11-03 15:02:13,001:INFO:Copying training dataset
2023-11-03 15:02:13,038:INFO:Defining folds
2023-11-03 15:02:13,039:INFO:Declaring metric variables
2023-11-03 15:02:13,049:INFO:Importing untrained model
2023-11-03 15:02:13,057:INFO:Lasso Regression Imported successfully
2023-11-03 15:02:13,077:INFO:Starting cross validation
2023-11-03 15:02:13,080:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:20,517:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.202e+07, tolerance: 4.504e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,660:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.261e+07, tolerance: 4.496e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,701:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.231e+07, tolerance: 4.450e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,791:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.240e+07, tolerance: 4.512e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,844:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.208e+07, tolerance: 4.482e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,886:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.206e+07, tolerance: 4.493e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,894:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.242e+07, tolerance: 4.540e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:20,912:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.214e+07, tolerance: 4.520e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:23,237:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.192e+07, tolerance: 4.472e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:23,257:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.248e+07, tolerance: 4.467e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:23,317:INFO:Calculating mean and std
2023-11-03 15:02:23,323:INFO:Creating metrics dataframe
2023-11-03 15:02:23,329:INFO:Uploading results into container
2023-11-03 15:02:23,330:INFO:Uploading model into container now
2023-11-03 15:02:23,331:INFO:_master_model_container: 2
2023-11-03 15:02:23,331:INFO:_display_container: 2
2023-11-03 15:02:23,332:INFO:Lasso(random_state=123)
2023-11-03 15:02:23,333:INFO:create_model() successfully completed......................................
2023-11-03 15:02:23,512:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:23,512:INFO:Creating metrics dataframe
2023-11-03 15:02:23,531:INFO:Initializing Ridge Regression
2023-11-03 15:02:23,531:INFO:Total runtime is 0.19767549832661946 minutes
2023-11-03 15:02:23,541:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:23,542:INFO:Initializing create_model()
2023-11-03 15:02:23,542:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:23,542:INFO:Checking exceptions
2023-11-03 15:02:23,543:INFO:Importing libraries
2023-11-03 15:02:23,543:INFO:Copying training dataset
2023-11-03 15:02:23,580:INFO:Defining folds
2023-11-03 15:02:23,580:INFO:Declaring metric variables
2023-11-03 15:02:23,591:INFO:Importing untrained model
2023-11-03 15:02:23,604:INFO:Ridge Regression Imported successfully
2023-11-03 15:02:23,624:INFO:Starting cross validation
2023-11-03 15:02:23,627:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:23,984:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=6.33162e-18): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,036:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.79232e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,257:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.75036e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,372:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.71232e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,384:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.74887e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,454:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.78896e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,631:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=4.76594e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,661:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.28532e-17): result may not be accurate.
  return linalg.solve(A, Xy, assume_a="pos", overwrite_a=True).T

2023-11-03 15:02:24,897:INFO:Calculating mean and std
2023-11-03 15:02:24,902:INFO:Creating metrics dataframe
2023-11-03 15:02:24,915:INFO:Uploading results into container
2023-11-03 15:02:24,918:INFO:Uploading model into container now
2023-11-03 15:02:24,920:INFO:_master_model_container: 3
2023-11-03 15:02:24,920:INFO:_display_container: 2
2023-11-03 15:02:24,922:INFO:Ridge(random_state=123)
2023-11-03 15:02:24,924:INFO:create_model() successfully completed......................................
2023-11-03 15:02:25,263:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:25,263:INFO:Creating metrics dataframe
2023-11-03 15:02:25,290:INFO:Initializing Elastic Net
2023-11-03 15:02:25,291:INFO:Total runtime is 0.22700021664301553 minutes
2023-11-03 15:02:25,302:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:25,303:INFO:Initializing create_model()
2023-11-03 15:02:25,303:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:25,304:INFO:Checking exceptions
2023-11-03 15:02:25,304:INFO:Importing libraries
2023-11-03 15:02:25,305:INFO:Copying training dataset
2023-11-03 15:02:25,353:INFO:Defining folds
2023-11-03 15:02:25,353:INFO:Declaring metric variables
2023-11-03 15:02:25,365:INFO:Importing untrained model
2023-11-03 15:02:25,384:INFO:Elastic Net Imported successfully
2023-11-03 15:02:25,484:INFO:Starting cross validation
2023-11-03 15:02:25,490:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:33,291:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.271e+07, tolerance: 4.496e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,296:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.241e+07, tolerance: 4.450e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,417:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.254e+07, tolerance: 4.540e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,438:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.213e+07, tolerance: 4.504e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,447:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.252e+07, tolerance: 4.512e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,455:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.220e+07, tolerance: 4.482e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,514:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.226e+07, tolerance: 4.520e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:33,516:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+07, tolerance: 4.493e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:35,947:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.204e+07, tolerance: 4.472e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:35,973:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.259e+07, tolerance: 4.467e+04
  model = cd_fast.enet_coordinate_descent(

2023-11-03 15:02:43,089:INFO:Calculating mean and std
2023-11-03 15:02:43,094:INFO:Creating metrics dataframe
2023-11-03 15:02:43,101:INFO:Uploading results into container
2023-11-03 15:02:43,104:INFO:Uploading model into container now
2023-11-03 15:02:43,110:INFO:_master_model_container: 4
2023-11-03 15:02:43,110:INFO:_display_container: 2
2023-11-03 15:02:43,111:INFO:ElasticNet(random_state=123)
2023-11-03 15:02:43,111:INFO:create_model() successfully completed......................................
2023-11-03 15:02:43,350:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:43,350:INFO:Creating metrics dataframe
2023-11-03 15:02:43,376:INFO:Initializing Least Angle Regression
2023-11-03 15:02:43,377:INFO:Total runtime is 0.5284322182337443 minutes
2023-11-03 15:02:43,389:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:43,390:INFO:Initializing create_model()
2023-11-03 15:02:43,391:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:43,391:INFO:Checking exceptions
2023-11-03 15:02:43,392:INFO:Importing libraries
2023-11-03 15:02:43,392:INFO:Copying training dataset
2023-11-03 15:02:43,447:INFO:Defining folds
2023-11-03 15:02:43,448:INFO:Declaring metric variables
2023-11-03 15:02:43,479:INFO:Importing untrained model
2023-11-03 15:02:43,500:INFO:Least Angle Regression Imported successfully
2023-11-03 15:02:43,521:INFO:Starting cross validation
2023-11-03 15:02:43,523:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:43,715:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,762:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,765:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,781:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,795:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.016e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,806:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.597e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,861:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,906:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=7.905e+01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,907:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.041e-01, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,909:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=8.962e+01, with an active set of 57 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,909:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.092e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,910:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=5.631e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,910:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=4.697e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,911:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.028e+00, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,911:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.016e+02, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,912:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.682e+00, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,912:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=6.725e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,921:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.040e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,934:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.745e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,942:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,956:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:43,985:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.563e+05, with an active set of 57 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:43,996:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=1.619e+04, with an active set of 57 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,018:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=6.871e-01, with an active set of 55 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,021:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=1.182e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,022:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=9.180e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,022:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=6.061e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 7.671e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,034:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:44,046:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=9.541e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,050:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=2.511e-01, with an active set of 58 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,071:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.001e-01, with an active set of 7 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,075:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.391e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,097:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.950e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,098:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.777e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,098:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=1.041e+03, with an active set of 55 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,098:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=4.495e+02, with an active set of 55 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,136:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:44,151:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:44,170:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.313e+00, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.306e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,170:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.525e-01, with an active set of 56 regressors, and the smallest cholesky pivot element being 9.306e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,170:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=3.305e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,183:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 70 iterations, i.e. alpha=1.180e+00, with an active set of 56 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,184:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_least_angle.py:649: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=1.543e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.
  warnings.warn(

2023-11-03 15:02:44,323:INFO:Calculating mean and std
2023-11-03 15:02:44,325:INFO:Creating metrics dataframe
2023-11-03 15:02:44,332:INFO:Uploading results into container
2023-11-03 15:02:44,333:INFO:Uploading model into container now
2023-11-03 15:02:44,335:INFO:_master_model_container: 5
2023-11-03 15:02:44,335:INFO:_display_container: 2
2023-11-03 15:02:44,336:INFO:Lars(random_state=123)
2023-11-03 15:02:44,336:INFO:create_model() successfully completed......................................
2023-11-03 15:02:44,507:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:44,507:INFO:Creating metrics dataframe
2023-11-03 15:02:44,520:INFO:Initializing Lasso Least Angle Regression
2023-11-03 15:02:44,521:INFO:Total runtime is 0.5475023667017619 minutes
2023-11-03 15:02:44,525:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:44,525:INFO:Initializing create_model()
2023-11-03 15:02:44,526:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:44,526:INFO:Checking exceptions
2023-11-03 15:02:44,526:INFO:Importing libraries
2023-11-03 15:02:44,526:INFO:Copying training dataset
2023-11-03 15:02:44,550:INFO:Defining folds
2023-11-03 15:02:44,550:INFO:Declaring metric variables
2023-11-03 15:02:44,556:INFO:Importing untrained model
2023-11-03 15:02:44,562:INFO:Lasso Least Angle Regression Imported successfully
2023-11-03 15:02:44,580:INFO:Starting cross validation
2023-11-03 15:02:44,583:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:44,818:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:44,823:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:44,864:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:44,947:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:44,969:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,074:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,100:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,132:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,230:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,231:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  warnings.warn(

2023-11-03 15:02:45,395:INFO:Calculating mean and std
2023-11-03 15:02:45,399:INFO:Creating metrics dataframe
2023-11-03 15:02:45,406:INFO:Uploading results into container
2023-11-03 15:02:45,407:INFO:Uploading model into container now
2023-11-03 15:02:45,408:INFO:_master_model_container: 6
2023-11-03 15:02:45,408:INFO:_display_container: 2
2023-11-03 15:02:45,409:INFO:LassoLars(random_state=123)
2023-11-03 15:02:45,409:INFO:create_model() successfully completed......................................
2023-11-03 15:02:45,576:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:45,576:INFO:Creating metrics dataframe
2023-11-03 15:02:45,594:INFO:Initializing Orthogonal Matching Pursuit
2023-11-03 15:02:45,594:INFO:Total runtime is 0.5653855164845785 minutes
2023-11-03 15:02:45,600:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:45,601:INFO:Initializing create_model()
2023-11-03 15:02:45,601:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:45,601:INFO:Checking exceptions
2023-11-03 15:02:45,601:INFO:Importing libraries
2023-11-03 15:02:45,601:INFO:Copying training dataset
2023-11-03 15:02:45,631:INFO:Defining folds
2023-11-03 15:02:45,632:INFO:Declaring metric variables
2023-11-03 15:02:45,642:INFO:Importing untrained model
2023-11-03 15:02:45,651:INFO:Orthogonal Matching Pursuit Imported successfully
2023-11-03 15:02:45,671:INFO:Starting cross validation
2023-11-03 15:02:45,675:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:45,879:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:45,887:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:45,934:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:45,979:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,045:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,064:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,131:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,152:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,237:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,278:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_base.py:133: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  warnings.warn(

2023-11-03 15:02:46,477:INFO:Calculating mean and std
2023-11-03 15:02:46,479:INFO:Creating metrics dataframe
2023-11-03 15:02:46,485:INFO:Uploading results into container
2023-11-03 15:02:46,486:INFO:Uploading model into container now
2023-11-03 15:02:46,487:INFO:_master_model_container: 7
2023-11-03 15:02:46,487:INFO:_display_container: 2
2023-11-03 15:02:46,487:INFO:OrthogonalMatchingPursuit()
2023-11-03 15:02:46,487:INFO:create_model() successfully completed......................................
2023-11-03 15:02:46,644:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:46,645:INFO:Creating metrics dataframe
2023-11-03 15:02:46,659:INFO:Initializing Bayesian Ridge
2023-11-03 15:02:46,659:INFO:Total runtime is 0.5831444025039673 minutes
2023-11-03 15:02:46,664:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:46,665:INFO:Initializing create_model()
2023-11-03 15:02:46,665:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:46,665:INFO:Checking exceptions
2023-11-03 15:02:46,665:INFO:Importing libraries
2023-11-03 15:02:46,666:INFO:Copying training dataset
2023-11-03 15:02:46,701:INFO:Defining folds
2023-11-03 15:02:46,705:INFO:Declaring metric variables
2023-11-03 15:02:46,714:INFO:Importing untrained model
2023-11-03 15:02:46,722:INFO:Bayesian Ridge Imported successfully
2023-11-03 15:02:46,737:INFO:Starting cross validation
2023-11-03 15:02:46,739:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:48,021:INFO:Calculating mean and std
2023-11-03 15:02:48,026:INFO:Creating metrics dataframe
2023-11-03 15:02:48,035:INFO:Uploading results into container
2023-11-03 15:02:48,037:INFO:Uploading model into container now
2023-11-03 15:02:48,038:INFO:_master_model_container: 8
2023-11-03 15:02:48,038:INFO:_display_container: 2
2023-11-03 15:02:48,039:INFO:BayesianRidge()
2023-11-03 15:02:48,039:INFO:create_model() successfully completed......................................
2023-11-03 15:02:48,251:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:48,256:INFO:Creating metrics dataframe
2023-11-03 15:02:48,343:INFO:Initializing Passive Aggressive Regressor
2023-11-03 15:02:48,344:INFO:Total runtime is 0.6112192511558533 minutes
2023-11-03 15:02:48,359:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:48,360:INFO:Initializing create_model()
2023-11-03 15:02:48,361:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:48,361:INFO:Checking exceptions
2023-11-03 15:02:48,361:INFO:Importing libraries
2023-11-03 15:02:48,361:INFO:Copying training dataset
2023-11-03 15:02:48,423:INFO:Defining folds
2023-11-03 15:02:48,423:INFO:Declaring metric variables
2023-11-03 15:02:48,462:INFO:Importing untrained model
2023-11-03 15:02:48,478:INFO:Passive Aggressive Regressor Imported successfully
2023-11-03 15:02:48,502:INFO:Starting cross validation
2023-11-03 15:02:48,505:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:49,420:INFO:Calculating mean and std
2023-11-03 15:02:49,423:INFO:Creating metrics dataframe
2023-11-03 15:02:49,429:INFO:Uploading results into container
2023-11-03 15:02:49,430:INFO:Uploading model into container now
2023-11-03 15:02:49,432:INFO:_master_model_container: 9
2023-11-03 15:02:49,432:INFO:_display_container: 2
2023-11-03 15:02:49,433:INFO:PassiveAggressiveRegressor(random_state=123)
2023-11-03 15:02:49,433:INFO:create_model() successfully completed......................................
2023-11-03 15:02:49,594:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:49,594:INFO:Creating metrics dataframe
2023-11-03 15:02:49,605:INFO:Initializing Huber Regressor
2023-11-03 15:02:49,606:INFO:Total runtime is 0.632252283891042 minutes
2023-11-03 15:02:49,609:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:49,609:INFO:Initializing create_model()
2023-11-03 15:02:49,610:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:49,610:INFO:Checking exceptions
2023-11-03 15:02:49,610:INFO:Importing libraries
2023-11-03 15:02:49,610:INFO:Copying training dataset
2023-11-03 15:02:49,635:INFO:Defining folds
2023-11-03 15:02:49,636:INFO:Declaring metric variables
2023-11-03 15:02:49,645:INFO:Importing untrained model
2023-11-03 15:02:49,652:INFO:Huber Regressor Imported successfully
2023-11-03 15:02:49,666:INFO:Starting cross validation
2023-11-03 15:02:49,668:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:52,201:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,380:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,406:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,580:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,639:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:52,675:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:53,539:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:53,564:WARNING:/Users/olejacobmellgren/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_huber.py:335: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2023-11-03 15:02:53,704:INFO:Calculating mean and std
2023-11-03 15:02:53,707:INFO:Creating metrics dataframe
2023-11-03 15:02:53,713:INFO:Uploading results into container
2023-11-03 15:02:53,714:INFO:Uploading model into container now
2023-11-03 15:02:53,715:INFO:_master_model_container: 10
2023-11-03 15:02:53,715:INFO:_display_container: 2
2023-11-03 15:02:53,715:INFO:HuberRegressor()
2023-11-03 15:02:53,716:INFO:create_model() successfully completed......................................
2023-11-03 15:02:53,921:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:53,922:INFO:Creating metrics dataframe
2023-11-03 15:02:53,944:INFO:Initializing K Neighbors Regressor
2023-11-03 15:02:53,944:INFO:Total runtime is 0.7045547644297282 minutes
2023-11-03 15:02:53,949:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:53,950:INFO:Initializing create_model()
2023-11-03 15:02:53,950:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:53,951:INFO:Checking exceptions
2023-11-03 15:02:53,951:INFO:Importing libraries
2023-11-03 15:02:53,952:INFO:Copying training dataset
2023-11-03 15:02:53,982:INFO:Defining folds
2023-11-03 15:02:53,982:INFO:Declaring metric variables
2023-11-03 15:02:53,993:INFO:Importing untrained model
2023-11-03 15:02:54,004:INFO:K Neighbors Regressor Imported successfully
2023-11-03 15:02:54,021:INFO:Starting cross validation
2023-11-03 15:02:54,026:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:02:56,461:INFO:Calculating mean and std
2023-11-03 15:02:56,468:INFO:Creating metrics dataframe
2023-11-03 15:02:56,480:INFO:Uploading results into container
2023-11-03 15:02:56,482:INFO:Uploading model into container now
2023-11-03 15:02:56,483:INFO:_master_model_container: 11
2023-11-03 15:02:56,484:INFO:_display_container: 2
2023-11-03 15:02:56,484:INFO:KNeighborsRegressor(n_jobs=-1)
2023-11-03 15:02:56,484:INFO:create_model() successfully completed......................................
2023-11-03 15:02:56,691:INFO:SubProcess create_model() end ==================================
2023-11-03 15:02:56,691:INFO:Creating metrics dataframe
2023-11-03 15:02:56,715:INFO:Initializing Decision Tree Regressor
2023-11-03 15:02:56,715:INFO:Total runtime is 0.7507464488347372 minutes
2023-11-03 15:02:56,723:INFO:SubProcess create_model() called ==================================
2023-11-03 15:02:56,724:INFO:Initializing create_model()
2023-11-03 15:02:56,724:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:02:56,725:INFO:Checking exceptions
2023-11-03 15:02:56,725:INFO:Importing libraries
2023-11-03 15:02:56,726:INFO:Copying training dataset
2023-11-03 15:02:56,762:INFO:Defining folds
2023-11-03 15:02:56,763:INFO:Declaring metric variables
2023-11-03 15:02:56,770:INFO:Importing untrained model
2023-11-03 15:02:56,780:INFO:Decision Tree Regressor Imported successfully
2023-11-03 15:02:56,798:INFO:Starting cross validation
2023-11-03 15:02:56,800:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:03:00,012:INFO:Calculating mean and std
2023-11-03 15:03:00,015:INFO:Creating metrics dataframe
2023-11-03 15:03:00,022:INFO:Uploading results into container
2023-11-03 15:03:00,024:INFO:Uploading model into container now
2023-11-03 15:03:00,026:INFO:_master_model_container: 12
2023-11-03 15:03:00,026:INFO:_display_container: 2
2023-11-03 15:03:00,027:INFO:DecisionTreeRegressor(random_state=123)
2023-11-03 15:03:00,027:INFO:create_model() successfully completed......................................
2023-11-03 15:03:00,232:INFO:SubProcess create_model() end ==================================
2023-11-03 15:03:00,233:INFO:Creating metrics dataframe
2023-11-03 15:03:00,261:INFO:Initializing Random Forest Regressor
2023-11-03 15:03:00,261:INFO:Total runtime is 0.8098389863967896 minutes
2023-11-03 15:03:00,271:INFO:SubProcess create_model() called ==================================
2023-11-03 15:03:00,272:INFO:Initializing create_model()
2023-11-03 15:03:00,272:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:03:00,273:INFO:Checking exceptions
2023-11-03 15:03:00,274:INFO:Importing libraries
2023-11-03 15:03:00,274:INFO:Copying training dataset
2023-11-03 15:03:00,338:INFO:Defining folds
2023-11-03 15:03:00,339:INFO:Declaring metric variables
2023-11-03 15:03:00,368:INFO:Importing untrained model
2023-11-03 15:03:00,398:INFO:Random Forest Regressor Imported successfully
2023-11-03 15:03:00,430:INFO:Starting cross validation
2023-11-03 15:03:00,434:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:05:22,689:INFO:Calculating mean and std
2023-11-03 15:05:22,706:INFO:Creating metrics dataframe
2023-11-03 15:05:22,729:INFO:Uploading results into container
2023-11-03 15:05:22,731:INFO:Uploading model into container now
2023-11-03 15:05:22,734:INFO:_master_model_container: 13
2023-11-03 15:05:22,734:INFO:_display_container: 2
2023-11-03 15:05:22,737:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:05:22,738:INFO:create_model() successfully completed......................................
2023-11-03 15:05:23,309:INFO:SubProcess create_model() end ==================================
2023-11-03 15:05:23,310:INFO:Creating metrics dataframe
2023-11-03 15:05:23,329:INFO:Initializing Extra Trees Regressor
2023-11-03 15:05:23,329:INFO:Total runtime is 3.194313100973765 minutes
2023-11-03 15:05:23,336:INFO:SubProcess create_model() called ==================================
2023-11-03 15:05:23,336:INFO:Initializing create_model()
2023-11-03 15:05:23,337:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:05:23,337:INFO:Checking exceptions
2023-11-03 15:05:23,337:INFO:Importing libraries
2023-11-03 15:05:23,337:INFO:Copying training dataset
2023-11-03 15:05:23,376:INFO:Defining folds
2023-11-03 15:05:23,376:INFO:Declaring metric variables
2023-11-03 15:05:23,382:INFO:Importing untrained model
2023-11-03 15:05:23,389:INFO:Extra Trees Regressor Imported successfully
2023-11-03 15:05:23,401:INFO:Starting cross validation
2023-11-03 15:05:23,403:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:06:03,022:INFO:Calculating mean and std
2023-11-03 15:06:03,027:INFO:Creating metrics dataframe
2023-11-03 15:06:03,033:INFO:Uploading results into container
2023-11-03 15:06:03,034:INFO:Uploading model into container now
2023-11-03 15:06:03,035:INFO:_master_model_container: 14
2023-11-03 15:06:03,035:INFO:_display_container: 2
2023-11-03 15:06:03,036:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:06:03,036:INFO:create_model() successfully completed......................................
2023-11-03 15:06:03,198:INFO:SubProcess create_model() end ==================================
2023-11-03 15:06:03,198:INFO:Creating metrics dataframe
2023-11-03 15:06:03,214:INFO:Initializing AdaBoost Regressor
2023-11-03 15:06:03,214:INFO:Total runtime is 3.8590561151504517 minutes
2023-11-03 15:06:03,218:INFO:SubProcess create_model() called ==================================
2023-11-03 15:06:03,218:INFO:Initializing create_model()
2023-11-03 15:06:03,218:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:06:03,218:INFO:Checking exceptions
2023-11-03 15:06:03,218:INFO:Importing libraries
2023-11-03 15:06:03,218:INFO:Copying training dataset
2023-11-03 15:06:03,239:INFO:Defining folds
2023-11-03 15:06:03,239:INFO:Declaring metric variables
2023-11-03 15:06:03,244:INFO:Importing untrained model
2023-11-03 15:06:03,251:INFO:AdaBoost Regressor Imported successfully
2023-11-03 15:06:03,264:INFO:Starting cross validation
2023-11-03 15:06:03,266:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:06:22,693:INFO:Calculating mean and std
2023-11-03 15:06:22,698:INFO:Creating metrics dataframe
2023-11-03 15:06:22,706:INFO:Uploading results into container
2023-11-03 15:06:22,707:INFO:Uploading model into container now
2023-11-03 15:06:22,708:INFO:_master_model_container: 15
2023-11-03 15:06:22,708:INFO:_display_container: 2
2023-11-03 15:06:22,708:INFO:AdaBoostRegressor(random_state=123)
2023-11-03 15:06:22,709:INFO:create_model() successfully completed......................................
2023-11-03 15:06:22,876:INFO:SubProcess create_model() end ==================================
2023-11-03 15:06:22,876:INFO:Creating metrics dataframe
2023-11-03 15:06:22,892:INFO:Initializing Gradient Boosting Regressor
2023-11-03 15:06:22,892:INFO:Total runtime is 4.187024199962616 minutes
2023-11-03 15:06:22,897:INFO:SubProcess create_model() called ==================================
2023-11-03 15:06:22,897:INFO:Initializing create_model()
2023-11-03 15:06:22,897:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:06:22,897:INFO:Checking exceptions
2023-11-03 15:06:22,898:INFO:Importing libraries
2023-11-03 15:06:22,898:INFO:Copying training dataset
2023-11-03 15:06:22,918:INFO:Defining folds
2023-11-03 15:06:22,918:INFO:Declaring metric variables
2023-11-03 15:06:22,923:INFO:Importing untrained model
2023-11-03 15:06:22,928:INFO:Gradient Boosting Regressor Imported successfully
2023-11-03 15:06:22,938:INFO:Starting cross validation
2023-11-03 15:06:22,940:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:07:15,296:INFO:Calculating mean and std
2023-11-03 15:07:15,302:INFO:Creating metrics dataframe
2023-11-03 15:07:15,311:INFO:Uploading results into container
2023-11-03 15:07:15,313:INFO:Uploading model into container now
2023-11-03 15:07:15,314:INFO:_master_model_container: 16
2023-11-03 15:07:15,315:INFO:_display_container: 2
2023-11-03 15:07:15,316:INFO:GradientBoostingRegressor(random_state=123)
2023-11-03 15:07:15,317:INFO:create_model() successfully completed......................................
2023-11-03 15:07:15,481:INFO:SubProcess create_model() end ==================================
2023-11-03 15:07:15,482:INFO:Creating metrics dataframe
2023-11-03 15:07:15,495:INFO:Initializing Extreme Gradient Boosting
2023-11-03 15:07:15,495:INFO:Total runtime is 5.063737984498342 minutes
2023-11-03 15:07:15,499:INFO:SubProcess create_model() called ==================================
2023-11-03 15:07:15,499:INFO:Initializing create_model()
2023-11-03 15:07:15,499:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=xgboost, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:07:15,499:INFO:Checking exceptions
2023-11-03 15:07:15,500:INFO:Importing libraries
2023-11-03 15:07:15,500:INFO:Copying training dataset
2023-11-03 15:07:15,520:INFO:Defining folds
2023-11-03 15:07:15,520:INFO:Declaring metric variables
2023-11-03 15:07:15,524:INFO:Importing untrained model
2023-11-03 15:07:15,529:INFO:Extreme Gradient Boosting Imported successfully
2023-11-03 15:07:15,539:INFO:Starting cross validation
2023-11-03 15:07:15,541:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:07:22,484:INFO:Calculating mean and std
2023-11-03 15:07:22,489:INFO:Creating metrics dataframe
2023-11-03 15:07:22,496:INFO:Uploading results into container
2023-11-03 15:07:22,497:INFO:Uploading model into container now
2023-11-03 15:07:22,498:INFO:_master_model_container: 17
2023-11-03 15:07:22,498:INFO:_display_container: 2
2023-11-03 15:07:22,499:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-03 15:07:22,499:INFO:create_model() successfully completed......................................
2023-11-03 15:07:22,679:INFO:SubProcess create_model() end ==================================
2023-11-03 15:07:22,679:INFO:Creating metrics dataframe
2023-11-03 15:07:22,695:INFO:Initializing Light Gradient Boosting Machine
2023-11-03 15:07:22,696:INFO:Total runtime is 5.183753514289856 minutes
2023-11-03 15:07:22,702:INFO:SubProcess create_model() called ==================================
2023-11-03 15:07:22,702:INFO:Initializing create_model()
2023-11-03 15:07:22,703:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:07:22,703:INFO:Checking exceptions
2023-11-03 15:07:22,703:INFO:Importing libraries
2023-11-03 15:07:22,703:INFO:Copying training dataset
2023-11-03 15:07:22,727:INFO:Defining folds
2023-11-03 15:07:22,727:INFO:Declaring metric variables
2023-11-03 15:07:22,733:INFO:Importing untrained model
2023-11-03 15:07:22,741:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 15:07:22,752:INFO:Starting cross validation
2023-11-03 15:07:22,756:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:07:42,307:INFO:Calculating mean and std
2023-11-03 15:07:42,321:INFO:Creating metrics dataframe
2023-11-03 15:07:42,345:INFO:Uploading results into container
2023-11-03 15:07:42,347:INFO:Uploading model into container now
2023-11-03 15:07:42,351:INFO:_master_model_container: 18
2023-11-03 15:07:42,351:INFO:_display_container: 2
2023-11-03 15:07:42,356:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:07:42,356:INFO:create_model() successfully completed......................................
2023-11-03 15:07:42,622:INFO:SubProcess create_model() end ==================================
2023-11-03 15:07:42,622:INFO:Creating metrics dataframe
2023-11-03 15:07:42,636:INFO:Initializing Dummy Regressor
2023-11-03 15:07:42,636:INFO:Total runtime is 5.516091783841452 minutes
2023-11-03 15:07:42,640:INFO:SubProcess create_model() called ==================================
2023-11-03 15:07:42,640:INFO:Initializing create_model()
2023-11-03 15:07:42,640:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7fca02499790>, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:07:42,640:INFO:Checking exceptions
2023-11-03 15:07:42,640:INFO:Importing libraries
2023-11-03 15:07:42,641:INFO:Copying training dataset
2023-11-03 15:07:42,659:INFO:Defining folds
2023-11-03 15:07:42,660:INFO:Declaring metric variables
2023-11-03 15:07:42,665:INFO:Importing untrained model
2023-11-03 15:07:42,670:INFO:Dummy Regressor Imported successfully
2023-11-03 15:07:42,680:INFO:Starting cross validation
2023-11-03 15:07:42,681:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-03 15:07:43,233:INFO:Calculating mean and std
2023-11-03 15:07:43,238:INFO:Creating metrics dataframe
2023-11-03 15:07:43,245:INFO:Uploading results into container
2023-11-03 15:07:43,247:INFO:Uploading model into container now
2023-11-03 15:07:43,248:INFO:_master_model_container: 19
2023-11-03 15:07:43,248:INFO:_display_container: 2
2023-11-03 15:07:43,249:INFO:DummyRegressor()
2023-11-03 15:07:43,249:INFO:create_model() successfully completed......................................
2023-11-03 15:07:43,415:INFO:SubProcess create_model() end ==================================
2023-11-03 15:07:43,415:INFO:Creating metrics dataframe
2023-11-03 15:07:43,440:INFO:Initializing create_model()
2023-11-03 15:07:43,440:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-03 15:07:43,440:INFO:Checking exceptions
2023-11-03 15:07:43,443:INFO:Importing libraries
2023-11-03 15:07:43,443:INFO:Copying training dataset
2023-11-03 15:07:43,461:INFO:Defining folds
2023-11-03 15:07:43,462:INFO:Declaring metric variables
2023-11-03 15:07:43,462:INFO:Importing untrained model
2023-11-03 15:07:43,462:INFO:Declaring custom model
2023-11-03 15:07:43,463:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-03 15:07:43,464:INFO:Cross validation set to False
2023-11-03 15:07:43,464:INFO:Fitting Model
2023-11-03 15:07:43,598:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008414 seconds.
2023-11-03 15:07:43,599:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-03 15:07:43,599:INFO:[LightGBM] [Info] Total Bins 9798
2023-11-03 15:07:43,601:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 58
2023-11-03 15:07:43,602:INFO:[LightGBM] [Info] Start training from score 77.068662
2023-11-03 15:07:43,941:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:07:43,941:INFO:create_model() successfully completed......................................
2023-11-03 15:07:44,210:INFO:_master_model_container: 19
2023-11-03 15:07:44,210:INFO:_display_container: 2
2023-11-03 15:07:44,211:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-03 15:07:44,211:INFO:compare_models() successfully completed......................................
2023-11-03 15:07:44,307:INFO:Initializing evaluate_model()
2023-11-03 15:07:44,308:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-03 15:07:44,382:INFO:Initializing plot_model()
2023-11-03 15:07:44,382:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, system=True)
2023-11-03 15:07:44,383:INFO:Checking exceptions
2023-11-03 15:07:44,395:INFO:Preloading libraries
2023-11-03 15:07:44,407:INFO:Copying training dataset
2023-11-03 15:07:44,407:INFO:Plot type: pipeline
2023-11-03 15:07:44,733:INFO:Visual Rendered Successfully
2023-11-03 15:07:44,917:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:44,922:INFO:Initializing evaluate_model()
2023-11-03 15:07:44,922:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-03 15:07:44,955:INFO:Initializing plot_model()
2023-11-03 15:07:44,955:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, system=True)
2023-11-03 15:07:44,955:INFO:Checking exceptions
2023-11-03 15:07:44,962:INFO:Preloading libraries
2023-11-03 15:07:44,966:INFO:Copying training dataset
2023-11-03 15:07:44,966:INFO:Plot type: pipeline
2023-11-03 15:07:45,179:INFO:Visual Rendered Successfully
2023-11-03 15:07:45,353:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:45,358:INFO:Initializing evaluate_model()
2023-11-03 15:07:45,359:INFO:evaluate_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, fit_kwargs=None, plot_kwargs=None, feature_name=None, groups=None)
2023-11-03 15:07:45,380:INFO:Initializing plot_model()
2023-11-03 15:07:45,380:INFO:plot_model(plot=pipeline, fold=KFold(n_splits=10, random_state=None, shuffle=False), verbose=False, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs={}, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, system=True)
2023-11-03 15:07:45,380:INFO:Checking exceptions
2023-11-03 15:07:45,386:INFO:Preloading libraries
2023-11-03 15:07:45,390:INFO:Copying training dataset
2023-11-03 15:07:45,390:INFO:Plot type: pipeline
2023-11-03 15:07:45,606:INFO:Visual Rendered Successfully
2023-11-03 15:07:45,784:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:45,788:INFO:Initializing plot_model()
2023-11-03 15:07:45,789:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, system=True)
2023-11-03 15:07:45,789:INFO:Checking exceptions
2023-11-03 15:07:45,800:INFO:Preloading libraries
2023-11-03 15:07:45,805:INFO:Copying training dataset
2023-11-03 15:07:45,805:INFO:Plot type: feature
2023-11-03 15:07:45,806:WARNING:No coef_ found. Trying feature_importances_
2023-11-03 15:07:46,136:INFO:Visual Rendered Successfully
2023-11-03 15:07:46,294:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:46,294:INFO:Initializing plot_model()
2023-11-03 15:07:46,294:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, system=True)
2023-11-03 15:07:46,294:INFO:Checking exceptions
2023-11-03 15:07:46,307:INFO:Preloading libraries
2023-11-03 15:07:46,311:INFO:Copying training dataset
2023-11-03 15:07:46,311:INFO:Plot type: feature
2023-11-03 15:07:46,312:WARNING:No coef_ found. Trying feature_importances_
2023-11-03 15:07:46,618:INFO:Visual Rendered Successfully
2023-11-03 15:07:46,775:INFO:plot_model() successfully completed......................................
2023-11-03 15:07:46,776:INFO:Initializing plot_model()
2023-11-03 15:07:46,776:INFO:plot_model(plot=feature, fold=None, verbose=True, display=None, display_format=None, estimator=LGBMRegressor(n_jobs=-1, random_state=123), feature_name=None, fit_kwargs=None, groups=None, label=False, plot_kwargs=None, save=False, scale=1, self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, system=True)
2023-11-03 15:07:46,776:INFO:Checking exceptions
2023-11-03 15:07:46,789:INFO:Preloading libraries
2023-11-03 15:07:46,793:INFO:Copying training dataset
2023-11-03 15:07:46,793:INFO:Plot type: feature
2023-11-03 15:07:46,794:WARNING:No coef_ found. Trying feature_importances_
2023-11-03 15:07:47,245:INFO:Visual Rendered Successfully
2023-11-03 15:07:47,402:INFO:plot_model() successfully completed......................................
2023-11-03 15:10:32,033:INFO:Initializing predict_model()
2023-11-03 15:10:32,036:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9daa17490>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fc9ea8bb040>)
2023-11-03 15:10:32,037:INFO:Checking exceptions
2023-11-03 15:10:32,038:INFO:Preloading libraries
2023-11-03 15:10:32,069:INFO:Set up data.
2023-11-03 15:10:32,178:INFO:Set up index.
2023-11-03 15:10:33,872:INFO:Initializing predict_model()
2023-11-03 15:10:33,892:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fca043bcbb0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fc9ea1a5af0>)
2023-11-03 15:10:33,892:INFO:Checking exceptions
2023-11-03 15:10:33,892:INFO:Preloading libraries
2023-11-03 15:10:33,895:INFO:Set up data.
2023-11-03 15:10:33,942:INFO:Set up index.
2023-11-03 15:10:34,431:INFO:Initializing predict_model()
2023-11-03 15:10:34,431:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7fc9f3b90280>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7fc9ea1a5af0>)
2023-11-03 15:10:34,431:INFO:Checking exceptions
2023-11-03 15:10:34,431:INFO:Preloading libraries
2023-11-03 15:10:34,435:INFO:Set up data.
2023-11-03 15:10:34,524:INFO:Set up index.
2023-11-05 11:04:07,960:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-05 11:04:07,961:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-05 11:04:07,961:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-05 11:04:07,962:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2023-11-05 12:01:49,061:INFO:PyCaret RegressionExperiment
2023-11-05 12:01:49,062:INFO:Logging name: lightgbm_experiment
2023-11-05 12:01:49,063:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-05 12:01:49,063:INFO:version 3.1.0
2023-11-05 12:01:49,063:INFO:Initializing setup()
2023-11-05 12:01:49,063:INFO:self.USI: 887d
2023-11-05 12:01:49,063:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-05 12:01:49,063:INFO:Checking environment
2023-11-05 12:01:49,063:INFO:python_version: 3.9.6
2023-11-05 12:01:49,063:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-05 12:01:49,063:INFO:machine: x86_64
2023-11-05 12:01:49,134:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-05 12:01:49,135:INFO:Memory: svmem(total=8589934592, available=2356674560, percent=72.6, used=4724547584, free=116797440, active=2239787008, inactive=2226778112, wired=2484760576)
2023-11-05 12:01:49,135:INFO:Physical Core: 4
2023-11-05 12:01:49,135:INFO:Logical Core: 8
2023-11-05 12:01:49,135:INFO:Checking libraries
2023-11-05 12:01:49,135:INFO:System:
2023-11-05 12:01:49,135:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-05 12:01:49,136:INFO:executable: /usr/local/bin/python3.9
2023-11-05 12:01:49,136:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-05 12:01:49,136:INFO:PyCaret required dependencies:
2023-11-05 12:01:50,557:INFO:                 pip: 23.3.1
2023-11-05 12:01:50,585:INFO:          setuptools: 56.0.0
2023-11-05 12:01:50,586:INFO:             pycaret: 3.1.0
2023-11-05 12:01:50,586:INFO:             IPython: 7.28.0
2023-11-05 12:01:50,586:INFO:          ipywidgets: 8.1.1
2023-11-05 12:01:50,586:INFO:                tqdm: 4.66.1
2023-11-05 12:01:50,586:INFO:               numpy: 1.23.5
2023-11-05 12:01:50,586:INFO:              pandas: 1.5.3
2023-11-05 12:01:50,586:INFO:              jinja2: 3.0.1
2023-11-05 12:01:50,586:INFO:               scipy: 1.10.1
2023-11-05 12:01:50,587:INFO:              joblib: 1.3.2
2023-11-05 12:01:50,587:INFO:             sklearn: 1.1.3
2023-11-05 12:01:50,587:INFO:                pyod: 1.1.1
2023-11-05 12:01:50,587:INFO:            imblearn: 0.11.0
2023-11-05 12:01:50,587:INFO:   category_encoders: 2.6.3
2023-11-05 12:01:50,587:INFO:            lightgbm: 4.1.0
2023-11-05 12:01:50,587:INFO:               numba: 0.58.1
2023-11-05 12:01:50,587:INFO:            requests: 2.31.0
2023-11-05 12:01:50,588:INFO:          matplotlib: 3.4.2
2023-11-05 12:01:50,588:INFO:          scikitplot: 0.3.7
2023-11-05 12:01:50,588:INFO:         yellowbrick: 1.5
2023-11-05 12:01:50,588:INFO:              plotly: 5.18.0
2023-11-05 12:01:50,588:INFO:    plotly-resampler: Not installed
2023-11-05 12:01:50,588:INFO:             kaleido: 0.2.1
2023-11-05 12:01:50,588:INFO:           schemdraw: 0.15
2023-11-05 12:01:50,588:INFO:         statsmodels: 0.14.0
2023-11-05 12:01:50,588:INFO:              sktime: 0.21.1
2023-11-05 12:01:50,588:INFO:               tbats: 1.1.3
2023-11-05 12:01:50,588:INFO:            pmdarima: 2.0.4
2023-11-05 12:01:50,588:INFO:              psutil: 5.9.6
2023-11-05 12:01:50,589:INFO:          markupsafe: 2.1.3
2023-11-05 12:01:50,589:INFO:             pickle5: Not installed
2023-11-05 12:01:50,589:INFO:         cloudpickle: 2.2.1
2023-11-05 12:01:50,589:INFO:         deprecation: 2.1.0
2023-11-05 12:01:50,589:INFO:              xxhash: 3.4.1
2023-11-05 12:01:50,589:INFO:           wurlitzer: 3.0.3
2023-11-05 12:01:50,589:INFO:PyCaret optional dependencies:
2023-11-05 12:01:50,617:INFO:                shap: Not installed
2023-11-05 12:01:50,617:INFO:           interpret: Not installed
2023-11-05 12:01:50,617:INFO:                umap: Not installed
2023-11-05 12:01:50,617:INFO:     ydata_profiling: Not installed
2023-11-05 12:01:50,617:INFO:  explainerdashboard: Not installed
2023-11-05 12:01:50,617:INFO:             autoviz: Not installed
2023-11-05 12:01:50,617:INFO:           fairlearn: Not installed
2023-11-05 12:01:50,617:INFO:          deepchecks: Not installed
2023-11-05 12:01:50,617:INFO:             xgboost: 2.0.0
2023-11-05 12:01:50,617:INFO:            catboost: Not installed
2023-11-05 12:01:50,617:INFO:              kmodes: Not installed
2023-11-05 12:01:50,617:INFO:             mlxtend: Not installed
2023-11-05 12:01:50,617:INFO:       statsforecast: Not installed
2023-11-05 12:01:50,618:INFO:        tune_sklearn: Not installed
2023-11-05 12:01:50,618:INFO:                 ray: Not installed
2023-11-05 12:01:50,618:INFO:            hyperopt: 0.2.7
2023-11-05 12:01:50,618:INFO:              optuna: 3.4.0
2023-11-05 12:01:50,618:INFO:               skopt: Not installed
2023-11-05 12:01:50,618:INFO:              mlflow: Not installed
2023-11-05 12:01:50,618:INFO:              gradio: Not installed
2023-11-05 12:01:50,618:INFO:             fastapi: Not installed
2023-11-05 12:01:50,619:INFO:             uvicorn: Not installed
2023-11-05 12:01:50,619:INFO:              m2cgen: Not installed
2023-11-05 12:01:50,619:INFO:           evidently: Not installed
2023-11-05 12:01:50,619:INFO:               fugue: Not installed
2023-11-05 12:01:50,619:INFO:           streamlit: Not installed
2023-11-05 12:01:50,619:INFO:             prophet: Not installed
2023-11-05 12:01:50,619:INFO:None
2023-11-05 12:01:50,619:INFO:Set up data.
2023-11-05 12:01:50,700:INFO:Set up folding strategy.
2023-11-05 12:01:50,700:INFO:Set up train/test split.
2023-11-05 12:01:50,761:INFO:Set up index.
2023-11-05 12:01:50,765:INFO:Assigning column types.
2023-11-05 12:01:50,791:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-05 12:01:50,792:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 12:01:50,798:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 12:01:50,804:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:01:50,911:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:50,981:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:01:50,983:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:50,987:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:50,987:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 12:01:50,994:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,001:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,095:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,154:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,155:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:51,159:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:51,159:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-05 12:01:51,165:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,171:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,263:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,337:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,337:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:51,342:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:51,350:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,358:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,478:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,553:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,554:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:51,558:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:51,559:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-05 12:01:51,573:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,694:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,767:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,768:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:51,771:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:51,785:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,893:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,970:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:01:51,970:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:51,974:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:51,974:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-05 12:01:52,081:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:52,141:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:01:52,142:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:52,146:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:52,263:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:52,338:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:01:52,339:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:52,344:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:52,345:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-05 12:01:52,462:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:52,522:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:52,526:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:52,641:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:01:52,704:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:52,708:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:52,708:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-05 12:01:52,912:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:52,917:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:53,087:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:53,090:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:53,092:INFO:Preparing preprocessing pipeline...
2023-11-05 12:01:53,092:INFO:Set up simple imputation.
2023-11-05 12:01:53,104:INFO:Set up encoding of ordinal features.
2023-11-05 12:01:53,120:INFO:Set up encoding of categorical features.
2023-11-05 12:01:53,123:INFO:Set up column name cleaning.
2023-11-05 12:01:53,856:INFO:Finished creating preprocessing pipeline.
2023-11-05 12:01:54,179:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-05 12:01:54,179:INFO:Creating final display dataframe.
2023-11-05 12:01:56,643:INFO:Setup _display_container:                     Description                Value
0                    Session id                  123
1                        Target               target
2                   Target type           Regression
3           Original data shape          (34060, 57)
4        Transformed data shape          (34060, 63)
5   Transformed train set shape          (23842, 63)
6    Transformed test set shape          (10218, 63)
7              Ordinal features                    2
8              Numeric features                   52
9          Categorical features                    4
10                   Preprocess                 True
11              Imputation type               simple
12           Numeric imputation                 mean
13       Categorical imputation                 mode
14     Maximum one-hot encoding                   25
15              Encoding method                 None
16               Fold Generator                KFold
17                  Fold Number                   10
18                     CPU Jobs                   -1
19                      Use GPU                False
20               Log Experiment                False
21              Experiment Name  lightgbm_experiment
22                          USI                 887d
2023-11-05 12:01:57,074:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:57,082:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:57,589:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:01:57,601:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:01:57,606:INFO:setup() successfully completed in 8.56s...............
2023-11-05 12:01:57,609:INFO:Initializing create_model()
2023-11-05 12:01:57,609:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abd79ea90>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 12:01:57,609:INFO:Checking exceptions
2023-11-05 12:01:57,699:INFO:Importing libraries
2023-11-05 12:01:57,700:INFO:Copying training dataset
2023-11-05 12:01:57,782:INFO:Defining folds
2023-11-05 12:01:57,782:INFO:Declaring metric variables
2023-11-05 12:01:57,783:INFO:Importing untrained model
2023-11-05 12:01:57,784:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 12:01:57,785:INFO:Starting cross validation
2023-11-05 12:01:57,824:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 12:02:17,256:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.186491 seconds.
2023-11-05 12:02:17,259:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:02:17,260:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:02:17,260:INFO:[LightGBM] [Info] Total Bins 9705
2023-11-05 12:02:17,260:INFO:[LightGBM] [Info] Number of data points in the train set: 21458, number of used features: 60
2023-11-05 12:02:17,266:INFO:[LightGBM] [Info] Start training from score 637.271774
2023-11-05 12:02:17,268:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.182577 seconds.
2023-11-05 12:02:17,268:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:02:17,268:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:02:17,269:INFO:[LightGBM] [Info] Total Bins 9705
2023-11-05 12:02:17,271:INFO:[LightGBM] [Info] Number of data points in the train set: 21458, number of used features: 60
2023-11-05 12:02:17,276:INFO:[LightGBM] [Info] Start training from score 633.014997
2023-11-05 12:02:17,318:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056834 seconds.
2023-11-05 12:02:17,318:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:02:17,319:INFO:[LightGBM] [Info] Total Bins 9698
2023-11-05 12:02:17,323:INFO:[LightGBM] [Info] Number of data points in the train set: 21458, number of used features: 60
2023-11-05 12:02:17,327:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.126882 seconds.
2023-11-05 12:02:17,328:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:02:17,328:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:02:17,328:INFO:[LightGBM] [Info] Total Bins 9699
2023-11-05 12:02:17,330:INFO:[LightGBM] [Info] Start training from score 634.907850
2023-11-05 12:02:17,332:INFO:[LightGBM] [Info] Number of data points in the train set: 21458, number of used features: 60
2023-11-05 12:02:17,337:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066065 seconds.
2023-11-05 12:02:17,337:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:02:17,337:INFO:[LightGBM] [Info] Total Bins 9694
2023-11-05 12:02:17,340:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.114262 seconds.
2023-11-05 12:02:17,340:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:02:17,340:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:02:17,340:INFO:[LightGBM] [Info] Total Bins 9688
2023-11-05 12:02:17,343:INFO:[LightGBM] [Info] Number of data points in the train set: 21458, number of used features: 60[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.115406 seconds.
2023-11-05 12:02:17,343:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:02:17,343:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:02:17,344:INFO:[LightGBM] [Info] Start training from score 631.949508
2023-11-05 12:02:17,344:INFO:[LightGBM] [Info] Total Bins 9687
2023-11-05 12:02:17,344:INFO:
2023-11-05 12:02:17,344:INFO:[LightGBM] [Info] Number of data points in the train set: 21457, number of used features: 60
2023-11-05 12:02:17,346:INFO:[LightGBM] [Info] Number of data points in the train set: 21457, number of used features: 60
2023-11-05 12:02:17,350:INFO:[LightGBM] [Info] Start training from score 638.291276
2023-11-05 12:02:17,350:INFO:[LightGBM] [Info] Start training from score 638.046738
2023-11-05 12:02:17,352:INFO:[LightGBM] [Info] Start training from score 634.027896
2023-11-05 12:02:17,387:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.056956 seconds.
2023-11-05 12:02:17,387:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:02:17,387:INFO:[LightGBM] [Info] Total Bins 9706
2023-11-05 12:02:17,389:INFO:[LightGBM] [Info] Number of data points in the train set: 21458, number of used features: 60
2023-11-05 12:02:17,393:INFO:[LightGBM] [Info] Start training from score 637.383917
2023-11-05 12:02:33,269:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025614 seconds.
2023-11-05 12:02:33,269:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:02:33,269:INFO:[LightGBM] [Info] Total Bins 9701
2023-11-05 12:02:33,271:INFO:[LightGBM] [Info] Number of data points in the train set: 21458, number of used features: 60
2023-11-05 12:02:33,274:INFO:[LightGBM] [Info] Start training from score 634.444787
2023-11-05 12:02:33,434:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.026594 seconds.
2023-11-05 12:02:33,434:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:02:33,434:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:02:33,434:INFO:[LightGBM] [Info] Total Bins 9689
2023-11-05 12:02:33,436:INFO:[LightGBM] [Info] Number of data points in the train set: 21458, number of used features: 60
2023-11-05 12:02:33,439:INFO:[LightGBM] [Info] Start training from score 637.120082
2023-11-05 12:02:36,306:INFO:Calculating mean and std
2023-11-05 12:02:36,318:INFO:Creating metrics dataframe
2023-11-05 12:02:36,338:INFO:Finalizing model
2023-11-05 12:02:37,156:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013297 seconds.
2023-11-05 12:02:37,156:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:02:37,156:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:02:37,158:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:02:37,160:INFO:[LightGBM] [Info] Start training from score 635.645879
2023-11-05 12:02:40,723:INFO:Uploading results into container
2023-11-05 12:02:40,726:INFO:Uploading model into container now
2023-11-05 12:02:40,750:INFO:_master_model_container: 1
2023-11-05 12:02:40,751:INFO:_display_container: 2
2023-11-05 12:02:40,753:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-05 12:02:40,753:INFO:create_model() successfully completed......................................
2023-11-05 12:02:41,804:INFO:Initializing tune_model()
2023-11-05 12:02:41,805:INFO:tune_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abd79ea90>)
2023-11-05 12:02:41,805:INFO:Checking exceptions
2023-11-05 12:02:41,881:INFO:Copying training dataset
2023-11-05 12:02:41,905:INFO:Checking base model
2023-11-05 12:02:41,905:INFO:Base model : Light Gradient Boosting Machine
2023-11-05 12:02:41,907:INFO:Declaring metric variables
2023-11-05 12:02:41,908:INFO:Defining Hyperparameters
2023-11-05 12:02:42,241:INFO:Tuning with n_jobs=-1
2023-11-05 12:02:42,249:INFO:Initializing RandomizedSearchCV
2023-11-05 12:11:01,706:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2023-11-05 12:11:01,742:INFO:Hyperparameter search completed
2023-11-05 12:11:01,744:INFO:SubProcess create_model() called ==================================
2023-11-05 12:11:01,749:INFO:Initializing create_model()
2023-11-05 12:11:01,749:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abd79ea90>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ac5166670>, model_only=True, return_train_score=False, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2023-11-05 12:11:01,750:INFO:Checking exceptions
2023-11-05 12:11:01,751:INFO:Importing libraries
2023-11-05 12:11:01,752:INFO:Copying training dataset
2023-11-05 12:11:01,865:INFO:Defining folds
2023-11-05 12:11:01,866:INFO:Declaring metric variables
2023-11-05 12:11:01,868:INFO:Importing untrained model
2023-11-05 12:11:01,869:INFO:Declaring custom model
2023-11-05 12:11:01,877:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 12:11:01,878:INFO:Starting cross validation
2023-11-05 12:11:01,886:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 12:12:32,355:INFO:Calculating mean and std
2023-11-05 12:12:32,358:INFO:Creating metrics dataframe
2023-11-05 12:12:32,365:INFO:Finalizing model
2023-11-05 12:12:32,966:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:12:32,966:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:12:32,966:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:12:33,062:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:12:33,062:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:12:33,063:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:12:33,080:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011238 seconds.
2023-11-05 12:12:33,080:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:12:33,083:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:12:33,090:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:12:33,094:INFO:[LightGBM] [Info] Start training from score 635.645879
2023-11-05 12:12:35,390:INFO:Uploading results into container
2023-11-05 12:12:35,392:INFO:Uploading model into container now
2023-11-05 12:12:35,395:INFO:_master_model_container: 2
2023-11-05 12:12:35,395:INFO:_display_container: 3
2023-11-05 12:12:35,397:INFO:LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3)
2023-11-05 12:12:35,397:INFO:create_model() successfully completed......................................
2023-11-05 12:12:35,940:INFO:SubProcess create_model() end ==================================
2023-11-05 12:12:35,941:INFO:choose_better activated
2023-11-05 12:12:35,941:INFO:SubProcess create_model() called ==================================
2023-11-05 12:12:35,941:INFO:Initializing create_model()
2023-11-05 12:12:35,941:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abd79ea90>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 12:12:35,941:INFO:Checking exceptions
2023-11-05 12:12:35,942:INFO:Importing libraries
2023-11-05 12:12:35,942:INFO:Copying training dataset
2023-11-05 12:12:35,971:INFO:Defining folds
2023-11-05 12:12:35,971:INFO:Declaring metric variables
2023-11-05 12:12:35,971:INFO:Importing untrained model
2023-11-05 12:12:35,971:INFO:Declaring custom model
2023-11-05 12:12:35,973:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 12:12:35,973:INFO:Starting cross validation
2023-11-05 12:12:35,975:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 12:13:00,768:INFO:Calculating mean and std
2023-11-05 12:13:00,770:INFO:Creating metrics dataframe
2023-11-05 12:13:00,774:INFO:Finalizing model
2023-11-05 12:13:01,501:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014289 seconds.
2023-11-05 12:13:01,502:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:13:01,502:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:13:01,504:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:13:01,506:INFO:[LightGBM] [Info] Start training from score 635.645879
2023-11-05 12:13:02,010:INFO:Uploading results into container
2023-11-05 12:13:02,011:INFO:Uploading model into container now
2023-11-05 12:13:02,012:INFO:_master_model_container: 3
2023-11-05 12:13:02,012:INFO:_display_container: 4
2023-11-05 12:13:02,013:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-05 12:13:02,013:INFO:create_model() successfully completed......................................
2023-11-05 12:13:02,171:INFO:SubProcess create_model() end ==================================
2023-11-05 12:13:02,172:INFO:LGBMRegressor(n_jobs=-1, random_state=123) result for MAE is 182.2942
2023-11-05 12:13:02,173:INFO:LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3) result for MAE is 181.4849
2023-11-05 12:13:02,174:INFO:LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3) is best model
2023-11-05 12:13:02,174:INFO:choose_better completed
2023-11-05 12:13:02,182:INFO:_master_model_container: 3
2023-11-05 12:13:02,183:INFO:_display_container: 3
2023-11-05 12:13:02,184:INFO:LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3)
2023-11-05 12:13:02,185:INFO:tune_model() successfully completed......................................
2023-11-05 12:13:02,303:INFO:Initializing ensemble_model()
2023-11-05 12:13:02,303:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abd79ea90>, estimator=LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3), method=Bagging, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-05 12:13:02,303:INFO:Checking exceptions
2023-11-05 12:13:02,320:INFO:Importing libraries
2023-11-05 12:13:02,321:INFO:Copying training dataset
2023-11-05 12:13:02,321:INFO:Checking base model
2023-11-05 12:13:02,321:INFO:Base model : Light Gradient Boosting Machine
2023-11-05 12:13:02,321:INFO:Importing untrained ensembler
2023-11-05 12:13:02,322:INFO:Ensemble method set to Bagging
2023-11-05 12:13:02,322:INFO:SubProcess create_model() called ==================================
2023-11-05 12:13:02,324:INFO:Initializing create_model()
2023-11-05 12:13:02,325:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abd79ea90>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                              bagging_freq=2,
                                              feature_fraction=0.4,
                                              min_child_samples=41,
                                              min_split_gain=0.9,
                                              n_estimators=260, n_jobs=-1,
                                              num_leaves=70, random_state=123,
                                              reg_alpha=2, reg_lambda=3),
                 random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9abe284fa0>, model_only=True, return_train_score=False, kwargs={})
2023-11-05 12:13:02,325:INFO:Checking exceptions
2023-11-05 12:13:02,325:INFO:Importing libraries
2023-11-05 12:13:02,325:INFO:Copying training dataset
2023-11-05 12:13:02,344:INFO:Defining folds
2023-11-05 12:13:02,345:INFO:Declaring metric variables
2023-11-05 12:13:02,345:INFO:Importing untrained model
2023-11-05 12:13:02,345:INFO:Declaring custom model
2023-11-05 12:13:02,347:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 12:13:02,348:INFO:Starting cross validation
2023-11-05 12:13:02,350:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 12:28:39,846:INFO:Calculating mean and std
2023-11-05 12:28:39,859:INFO:Creating metrics dataframe
2023-11-05 12:28:39,879:INFO:Finalizing model
2023-11-05 12:28:40,580:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:40,580:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:40,580:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:40,677:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:40,677:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:40,677:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:40,697:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013652 seconds.
2023-11-05 12:28:40,697:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:28:40,701:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:28:40,705:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:28:40,708:INFO:[LightGBM] [Info] Start training from score 632.636072
2023-11-05 12:28:43,094:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:43,094:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:43,095:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:43,199:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:43,199:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:43,199:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:43,219:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012069 seconds.
2023-11-05 12:28:43,219:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:28:43,221:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:28:43,226:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:28:43,227:INFO:[LightGBM] [Info] Start training from score 640.698252
2023-11-05 12:28:48,522:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:48,523:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:48,523:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:48,648:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:48,648:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:48,648:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:48,677:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019019 seconds.
2023-11-05 12:28:48,678:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:28:48,680:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:28:48,685:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:28:48,688:INFO:[LightGBM] [Info] Start training from score 643.438407
2023-11-05 12:28:51,628:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:51,628:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:51,629:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:51,733:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:51,733:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:51,733:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:51,751:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010818 seconds.
2023-11-05 12:28:51,751:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:28:51,753:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:28:51,758:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:28:51,760:INFO:[LightGBM] [Info] Start training from score 631.130194
2023-11-05 12:28:53,617:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:53,617:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:53,617:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:53,726:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:53,727:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:53,727:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:53,744:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012308 seconds.
2023-11-05 12:28:53,744:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:28:53,747:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:28:53,751:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:28:53,753:INFO:[LightGBM] [Info] Start training from score 628.429713
2023-11-05 12:28:55,654:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:55,654:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:55,655:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:55,787:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:55,787:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:55,787:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:55,806:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010903 seconds.
2023-11-05 12:28:55,807:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:28:55,809:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:28:55,812:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:28:55,814:INFO:[LightGBM] [Info] Start training from score 627.772224
2023-11-05 12:28:57,854:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:57,855:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:57,855:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:57,959:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:57,959:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:57,960:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:57,976:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012431 seconds.
2023-11-05 12:28:57,977:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:28:57,979:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:28:57,983:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:28:57,984:INFO:[LightGBM] [Info] Start training from score 636.420266
2023-11-05 12:28:59,788:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:59,790:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:59,790:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:59,890:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:28:59,890:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:28:59,890:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:28:59,908:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012596 seconds.
2023-11-05 12:28:59,908:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:28:59,911:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:28:59,915:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:28:59,916:INFO:[LightGBM] [Info] Start training from score 628.991114
2023-11-05 12:29:04,530:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:04,530:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:04,530:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:04,650:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:04,650:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:04,651:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:04,668:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012412 seconds.
2023-11-05 12:29:04,668:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:29:04,671:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:29:04,675:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:29:04,676:INFO:[LightGBM] [Info] Start training from score 640.725266
2023-11-05 12:29:06,194:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:06,195:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:06,195:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:06,292:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:06,293:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:06,293:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:06,310:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011262 seconds.
2023-11-05 12:29:06,311:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:29:06,313:INFO:[LightGBM] [Info] Total Bins 9826
2023-11-05 12:29:06,317:INFO:[LightGBM] [Info] Number of data points in the train set: 23842, number of used features: 60
2023-11-05 12:29:06,319:INFO:[LightGBM] [Info] Start training from score 648.313932
2023-11-05 12:29:07,990:INFO:Uploading results into container
2023-11-05 12:29:07,993:INFO:Uploading model into container now
2023-11-05 12:29:07,996:INFO:_master_model_container: 4
2023-11-05 12:29:07,996:INFO:_display_container: 4
2023-11-05 12:29:08,002:INFO:BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                              bagging_freq=2,
                                              feature_fraction=0.4,
                                              min_child_samples=41,
                                              min_split_gain=0.9,
                                              n_estimators=260, n_jobs=-1,
                                              num_leaves=70, random_state=123,
                                              reg_alpha=2, reg_lambda=3),
                 random_state=123)
2023-11-05 12:29:08,003:INFO:create_model() successfully completed......................................
2023-11-05 12:29:08,540:INFO:SubProcess create_model() end ==================================
2023-11-05 12:29:08,546:INFO:_master_model_container: 4
2023-11-05 12:29:08,547:INFO:_display_container: 4
2023-11-05 12:29:08,549:INFO:BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                              bagging_freq=2,
                                              feature_fraction=0.4,
                                              min_child_samples=41,
                                              min_split_gain=0.9,
                                              n_estimators=260, n_jobs=-1,
                                              num_leaves=70, random_state=123,
                                              reg_alpha=2, reg_lambda=3),
                 random_state=123)
2023-11-05 12:29:08,549:INFO:ensemble_model() successfully completed......................................
2023-11-05 12:29:08,659:INFO:Initializing finalize_model()
2023-11-05 12:29:08,660:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abd79ea90>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                              bagging_freq=2,
                                              feature_fraction=0.4,
                                              min_child_samples=41,
                                              min_split_gain=0.9,
                                              n_estimators=260, n_jobs=-1,
                                              num_leaves=70, random_state=123,
                                              reg_alpha=2, reg_lambda=3),
                 random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-05 12:29:08,661:INFO:Finalizing BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                              bagging_freq=2,
                                              feature_fraction=0.4,
                                              min_child_samples=41,
                                              min_split_gain=0.9,
                                              n_estimators=260, n_jobs=-1,
                                              num_leaves=70, random_state=123,
                                              reg_alpha=2, reg_lambda=3),
                 random_state=123)
2023-11-05 12:29:08,695:INFO:Initializing create_model()
2023-11-05 12:29:08,695:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abd79ea90>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                              bagging_freq=2,
                                              feature_fraction=0.4,
                                              min_child_samples=41,
                                              min_split_gain=0.9,
                                              n_estimators=260, n_jobs=-1,
                                              num_leaves=70, random_state=123,
                                              reg_alpha=2, reg_lambda=3),
                 random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-05 12:29:08,695:INFO:Checking exceptions
2023-11-05 12:29:08,697:INFO:Importing libraries
2023-11-05 12:29:08,697:INFO:Copying training dataset
2023-11-05 12:29:08,700:INFO:Defining folds
2023-11-05 12:29:08,701:INFO:Declaring metric variables
2023-11-05 12:29:08,701:INFO:Importing untrained model
2023-11-05 12:29:08,701:INFO:Declaring custom model
2023-11-05 12:29:08,703:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 12:29:08,705:INFO:Cross validation set to False
2023-11-05 12:29:08,705:INFO:Fitting Model
2023-11-05 12:29:09,600:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:09,600:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:09,600:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:09,744:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:09,744:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:09,744:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:09,767:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005195 seconds.
2023-11-05 12:29:09,767:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:29:09,767:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:29:09,767:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:09,771:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:09,774:INFO:[LightGBM] [Info] Start training from score 629.241225
2023-11-05 12:29:12,220:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:12,221:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:12,221:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:12,368:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:12,368:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:12,368:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:12,391:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015332 seconds.
2023-11-05 12:29:12,392:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:29:12,395:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:12,398:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:12,399:INFO:[LightGBM] [Info] Start training from score 634.104795
2023-11-05 12:29:14,080:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:14,081:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:14,081:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:14,227:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:14,227:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:14,227:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:14,252:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016536 seconds.
2023-11-05 12:29:14,252:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:29:14,255:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:14,258:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:14,260:INFO:[LightGBM] [Info] Start training from score 629.458425
2023-11-05 12:29:16,326:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:16,326:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:16,326:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:16,488:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:16,489:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:16,489:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:16,512:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017090 seconds.
2023-11-05 12:29:16,513:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:29:16,517:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:16,522:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:16,523:INFO:[LightGBM] [Info] Start training from score 620.912885
2023-11-05 12:29:23,066:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:23,067:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:23,067:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:23,237:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:23,237:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:23,238:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:23,259:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002708 seconds.
2023-11-05 12:29:23,259:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:29:23,259:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:29:23,259:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:23,264:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:23,266:INFO:[LightGBM] [Info] Start training from score 626.489665
2023-11-05 12:29:31,113:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:31,114:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:31,114:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:31,310:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:31,310:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:31,310:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:31,349:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025430 seconds.
2023-11-05 12:29:31,349:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 12:29:31,352:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:31,360:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:31,366:INFO:[LightGBM] [Info] Start training from score 636.226084
2023-11-05 12:29:39,919:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:39,919:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:39,919:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:40,114:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:40,114:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:40,115:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:40,149:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008887 seconds.
2023-11-05 12:29:40,149:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:29:40,149:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:29:40,150:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:40,156:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:40,163:INFO:[LightGBM] [Info] Start training from score 627.869239
2023-11-05 12:29:43,201:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:43,202:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:43,202:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:43,371:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:43,371:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:43,372:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:43,395:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003224 seconds.
2023-11-05 12:29:43,395:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:29:43,395:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:29:43,396:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:43,401:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:43,405:INFO:[LightGBM] [Info] Start training from score 635.173365
2023-11-05 12:29:46,082:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:46,082:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:46,082:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:46,245:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:46,245:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:46,245:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:46,276:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005092 seconds.
2023-11-05 12:29:46,277:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:29:46,277:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:29:46,277:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:46,282:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:46,286:INFO:[LightGBM] [Info] Start training from score 624.166673
2023-11-05 12:29:49,217:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:49,218:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:49,218:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:49,386:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 12:29:49,387:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 12:29:49,387:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 12:29:49,415:INFO:[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004858 seconds.
2023-11-05 12:29:49,416:INFO:You can set `force_row_wise=true` to remove the overhead.
2023-11-05 12:29:49,416:INFO:And if memory is not enough, you can set `force_col_wise=true`.
2023-11-05 12:29:49,416:INFO:[LightGBM] [Info] Total Bins 10021
2023-11-05 12:29:49,420:INFO:[LightGBM] [Info] Number of data points in the train set: 34060, number of used features: 60
2023-11-05 12:29:49,423:INFO:[LightGBM] [Info] Start training from score 634.910108
2023-11-05 12:29:52,063:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                                               bagging_freq=2,
                                                               feature_fraction=0.4,
                                                               min_child_samples=41,
                                                               min_split_gain=0.9,
                                                               n_estimators=260,
                                                               n_jobs=-1,
                                                               num_leaves=70,
                                                               random_state=123,
                                                               reg_alpha=2,
                                                               reg_lambda=3),
                                  random_state=123))])
2023-11-05 12:29:52,064:INFO:create_model() successfully completed......................................
2023-11-05 12:29:52,205:INFO:_master_model_container: 4
2023-11-05 12:29:52,205:INFO:_display_container: 4
2023-11-05 12:29:52,294:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                                               bagging_freq=2,
                                                               feature_fraction=0.4,
                                                               min_child_samples=41,
                                                               min_split_gain=0.9,
                                                               n_estimators=260,
                                                               n_jobs=-1,
                                                               num_leaves=70,
                                                               random_state=123,
                                                               reg_alpha=2,
                                                               reg_lambda=3),
                                  random_state=123))])
2023-11-05 12:29:52,294:INFO:finalize_model() successfully completed......................................
2023-11-05 12:29:52,450:INFO:PyCaret RegressionExperiment
2023-11-05 12:29:52,451:INFO:Logging name: rf_experiment
2023-11-05 12:29:52,451:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-05 12:29:52,451:INFO:version 3.1.0
2023-11-05 12:29:52,451:INFO:Initializing setup()
2023-11-05 12:29:52,451:INFO:self.USI: 3262
2023-11-05 12:29:52,451:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-05 12:29:52,451:INFO:Checking environment
2023-11-05 12:29:52,451:INFO:python_version: 3.9.6
2023-11-05 12:29:52,451:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-05 12:29:52,451:INFO:machine: x86_64
2023-11-05 12:29:52,451:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-05 12:29:52,451:INFO:Memory: svmem(total=8589934592, available=2154795008, percent=74.9, used=4345974784, free=142962688, active=1918648320, inactive=1977122816, wired=2427326464)
2023-11-05 12:29:52,451:INFO:Physical Core: 4
2023-11-05 12:29:52,451:INFO:Logical Core: 8
2023-11-05 12:29:52,451:INFO:Checking libraries
2023-11-05 12:29:52,451:INFO:System:
2023-11-05 12:29:52,451:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-05 12:29:52,451:INFO:executable: /usr/local/bin/python3.9
2023-11-05 12:29:52,452:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-05 12:29:52,452:INFO:PyCaret required dependencies:
2023-11-05 12:29:52,452:INFO:                 pip: 23.3.1
2023-11-05 12:29:52,452:INFO:          setuptools: 56.0.0
2023-11-05 12:29:52,452:INFO:             pycaret: 3.1.0
2023-11-05 12:29:52,452:INFO:             IPython: 7.28.0
2023-11-05 12:29:52,452:INFO:          ipywidgets: 8.1.1
2023-11-05 12:29:52,452:INFO:                tqdm: 4.66.1
2023-11-05 12:29:52,452:INFO:               numpy: 1.23.5
2023-11-05 12:29:52,452:INFO:              pandas: 1.5.3
2023-11-05 12:29:52,452:INFO:              jinja2: 3.0.1
2023-11-05 12:29:52,452:INFO:               scipy: 1.10.1
2023-11-05 12:29:52,452:INFO:              joblib: 1.3.2
2023-11-05 12:29:52,452:INFO:             sklearn: 1.1.3
2023-11-05 12:29:52,452:INFO:                pyod: 1.1.1
2023-11-05 12:29:52,452:INFO:            imblearn: 0.11.0
2023-11-05 12:29:52,452:INFO:   category_encoders: 2.6.3
2023-11-05 12:29:52,452:INFO:            lightgbm: 4.1.0
2023-11-05 12:29:52,452:INFO:               numba: 0.58.1
2023-11-05 12:29:52,452:INFO:            requests: 2.31.0
2023-11-05 12:29:52,452:INFO:          matplotlib: 3.4.2
2023-11-05 12:29:52,452:INFO:          scikitplot: 0.3.7
2023-11-05 12:29:52,453:INFO:         yellowbrick: 1.5
2023-11-05 12:29:52,453:INFO:              plotly: 5.18.0
2023-11-05 12:29:52,453:INFO:    plotly-resampler: Not installed
2023-11-05 12:29:52,453:INFO:             kaleido: 0.2.1
2023-11-05 12:29:52,453:INFO:           schemdraw: 0.15
2023-11-05 12:29:52,453:INFO:         statsmodels: 0.14.0
2023-11-05 12:29:52,453:INFO:              sktime: 0.21.1
2023-11-05 12:29:52,453:INFO:               tbats: 1.1.3
2023-11-05 12:29:52,453:INFO:            pmdarima: 2.0.4
2023-11-05 12:29:52,453:INFO:              psutil: 5.9.6
2023-11-05 12:29:52,453:INFO:          markupsafe: 2.1.3
2023-11-05 12:29:52,453:INFO:             pickle5: Not installed
2023-11-05 12:29:52,453:INFO:         cloudpickle: 2.2.1
2023-11-05 12:29:52,453:INFO:         deprecation: 2.1.0
2023-11-05 12:29:52,453:INFO:              xxhash: 3.4.1
2023-11-05 12:29:52,453:INFO:           wurlitzer: 3.0.3
2023-11-05 12:29:52,453:INFO:PyCaret optional dependencies:
2023-11-05 12:29:52,453:INFO:                shap: Not installed
2023-11-05 12:29:52,453:INFO:           interpret: Not installed
2023-11-05 12:29:52,453:INFO:                umap: Not installed
2023-11-05 12:29:52,453:INFO:     ydata_profiling: Not installed
2023-11-05 12:29:52,453:INFO:  explainerdashboard: Not installed
2023-11-05 12:29:52,454:INFO:             autoviz: Not installed
2023-11-05 12:29:52,454:INFO:           fairlearn: Not installed
2023-11-05 12:29:52,454:INFO:          deepchecks: Not installed
2023-11-05 12:29:52,454:INFO:             xgboost: 2.0.0
2023-11-05 12:29:52,454:INFO:            catboost: Not installed
2023-11-05 12:29:52,454:INFO:              kmodes: Not installed
2023-11-05 12:29:52,454:INFO:             mlxtend: Not installed
2023-11-05 12:29:52,454:INFO:       statsforecast: Not installed
2023-11-05 12:29:52,454:INFO:        tune_sklearn: Not installed
2023-11-05 12:29:52,454:INFO:                 ray: Not installed
2023-11-05 12:29:52,454:INFO:            hyperopt: 0.2.7
2023-11-05 12:29:52,454:INFO:              optuna: 3.4.0
2023-11-05 12:29:52,454:INFO:               skopt: Not installed
2023-11-05 12:29:52,454:INFO:              mlflow: Not installed
2023-11-05 12:29:52,454:INFO:              gradio: Not installed
2023-11-05 12:29:52,454:INFO:             fastapi: Not installed
2023-11-05 12:29:52,454:INFO:             uvicorn: Not installed
2023-11-05 12:29:52,454:INFO:              m2cgen: Not installed
2023-11-05 12:29:52,454:INFO:           evidently: Not installed
2023-11-05 12:29:52,454:INFO:               fugue: Not installed
2023-11-05 12:29:52,454:INFO:           streamlit: Not installed
2023-11-05 12:29:52,454:INFO:             prophet: Not installed
2023-11-05 12:29:52,455:INFO:None
2023-11-05 12:29:52,455:INFO:Set up data.
2023-11-05 12:29:52,555:INFO:Set up folding strategy.
2023-11-05 12:29:52,555:INFO:Set up train/test split.
2023-11-05 12:29:52,597:INFO:Set up index.
2023-11-05 12:29:52,600:INFO:Assigning column types.
2023-11-05 12:29:52,629:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-05 12:29:52,630:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 12:29:52,636:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 12:29:52,643:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:29:52,742:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:52,813:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:29:52,814:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:52,819:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:52,820:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 12:29:52,828:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 12:29:52,834:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:29:52,932:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,001:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,001:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:53,005:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:53,005:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-05 12:29:53,013:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,019:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,133:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,231:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,232:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:53,237:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:53,247:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,256:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,371:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,453:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:29:53,454:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:53,458:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:53,459:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-05 12:29:53,476:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:29:54,109:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:54,731:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:29:54,735:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:54,757:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:54,863:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 12:29:55,472:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:55,854:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:29:55,860:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:55,885:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:55,889:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-05 12:29:56,440:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:56,623:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:29:56,627:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:56,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:56,881:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:56,974:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 12:29:56,976:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:56,981:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:56,982:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-05 12:29:57,119:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:57,215:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:57,221:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:57,370:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 12:29:57,469:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:57,473:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:57,474:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-05 12:29:57,727:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:57,732:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:57,940:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:29:57,945:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:29:57,947:INFO:Preparing preprocessing pipeline...
2023-11-05 12:29:57,947:INFO:Set up simple imputation.
2023-11-05 12:29:57,960:INFO:Set up encoding of ordinal features.
2023-11-05 12:29:57,989:INFO:Set up encoding of categorical features.
2023-11-05 12:29:57,996:INFO:Set up column name cleaning.
2023-11-05 12:29:59,746:INFO:Finished creating preprocessing pipeline.
2023-11-05 12:30:00,124:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-05 12:30:00,125:INFO:Creating final display dataframe.
2023-11-05 12:30:00,612:INFO:Setup _display_container:                     Description          Value
0                    Session id            123
1                        Target         target
2                   Target type     Regression
3           Original data shape    (34060, 57)
4        Transformed data shape    (34060, 63)
5   Transformed train set shape    (23842, 63)
6    Transformed test set shape    (10218, 63)
7              Ordinal features              2
8              Numeric features             52
9          Categorical features              4
10                   Preprocess           True
11              Imputation type         simple
12           Numeric imputation           mean
13       Categorical imputation           mode
14     Maximum one-hot encoding             25
15              Encoding method           None
16               Fold Generator          KFold
17                  Fold Number             10
18                     CPU Jobs             -1
19                      Use GPU          False
20               Log Experiment          False
21              Experiment Name  rf_experiment
22                          USI           3262
2023-11-05 12:30:01,030:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:30:01,043:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:30:01,486:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 12:30:01,493:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 12:30:01,494:INFO:setup() successfully completed in 9.05s...............
2023-11-05 12:30:01,494:INFO:Initializing create_model()
2023-11-05 12:30:01,494:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac45888e0>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 12:30:01,495:INFO:Checking exceptions
2023-11-05 12:30:01,499:INFO:Importing libraries
2023-11-05 12:30:01,499:INFO:Copying training dataset
2023-11-05 12:30:01,536:INFO:Defining folds
2023-11-05 12:30:01,537:INFO:Declaring metric variables
2023-11-05 12:30:01,537:INFO:Importing untrained model
2023-11-05 12:30:01,538:INFO:Random Forest Regressor Imported successfully
2023-11-05 12:30:01,539:INFO:Starting cross validation
2023-11-05 12:30:01,543:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 12:33:42,632:INFO:Calculating mean and std
2023-11-05 12:33:42,667:INFO:Creating metrics dataframe
2023-11-05 12:33:42,673:INFO:Finalizing model
2023-11-05 12:34:09,668:INFO:Uploading results into container
2023-11-05 12:34:09,669:INFO:Uploading model into container now
2023-11-05 12:34:09,675:INFO:_master_model_container: 1
2023-11-05 12:34:09,675:INFO:_display_container: 2
2023-11-05 12:34:09,676:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-05 12:34:09,676:INFO:create_model() successfully completed......................................
2023-11-05 12:34:10,138:INFO:Initializing tune_model()
2023-11-05 12:34:10,138:INFO:tune_model(estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac45888e0>)
2023-11-05 12:34:10,138:INFO:Checking exceptions
2023-11-05 12:34:10,155:INFO:Copying training dataset
2023-11-05 12:34:10,171:INFO:Checking base model
2023-11-05 12:34:10,171:INFO:Base model : Random Forest Regressor
2023-11-05 12:34:10,172:INFO:Declaring metric variables
2023-11-05 12:34:10,172:INFO:Defining Hyperparameters
2023-11-05 12:34:10,315:INFO:Tuning with n_jobs=-1
2023-11-05 12:34:10,315:INFO:Initializing RandomizedSearchCV
2023-11-05 17:51:35,180:INFO:best_params: {'actual_estimator__n_estimators': 200, 'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 5, 'actual_estimator__min_impurity_decrease': 0.0002, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 11, 'actual_estimator__criterion': 'squared_error', 'actual_estimator__bootstrap': False}
2023-11-05 17:51:35,212:INFO:Hyperparameter search completed
2023-11-05 17:51:35,213:INFO:SubProcess create_model() called ==================================
2023-11-05 17:51:35,224:INFO:Initializing create_model()
2023-11-05 17:51:35,225:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac45888e0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ac52e0100>, model_only=True, return_train_score=False, kwargs={'n_estimators': 200, 'min_samples_split': 7, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0002, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'squared_error', 'bootstrap': False})
2023-11-05 17:51:35,225:INFO:Checking exceptions
2023-11-05 17:51:35,228:INFO:Importing libraries
2023-11-05 17:51:35,231:INFO:Copying training dataset
2023-11-05 17:51:35,366:INFO:Defining folds
2023-11-05 17:51:35,367:INFO:Declaring metric variables
2023-11-05 17:51:35,369:INFO:Importing untrained model
2023-11-05 17:51:35,369:INFO:Declaring custom model
2023-11-05 17:51:35,388:INFO:Random Forest Regressor Imported successfully
2023-11-05 17:51:35,390:INFO:Starting cross validation
2023-11-05 17:51:35,405:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 17:52:34,017:INFO:Calculating mean and std
2023-11-05 17:52:34,026:INFO:Creating metrics dataframe
2023-11-05 17:52:34,065:INFO:Finalizing model
2023-11-05 17:52:40,069:INFO:Uploading results into container
2023-11-05 17:52:40,071:INFO:Uploading model into container now
2023-11-05 17:52:40,074:INFO:_master_model_container: 2
2023-11-05 17:52:40,074:INFO:_display_container: 3
2023-11-05 17:52:40,078:INFO:RandomForestRegressor(bootstrap=False, max_depth=11, max_features='sqrt',
                      min_impurity_decrease=0.0002, min_samples_leaf=5,
                      min_samples_split=7, n_estimators=200, n_jobs=-1,
                      random_state=123)
2023-11-05 17:52:40,078:INFO:create_model() successfully completed......................................
2023-11-05 17:52:41,184:INFO:SubProcess create_model() end ==================================
2023-11-05 17:52:41,184:INFO:choose_better activated
2023-11-05 17:52:41,184:INFO:SubProcess create_model() called ==================================
2023-11-05 17:52:41,185:INFO:Initializing create_model()
2023-11-05 17:52:41,185:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac45888e0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 17:52:41,185:INFO:Checking exceptions
2023-11-05 17:52:41,189:INFO:Importing libraries
2023-11-05 17:52:41,190:INFO:Copying training dataset
2023-11-05 17:52:41,235:INFO:Defining folds
2023-11-05 17:52:41,235:INFO:Declaring metric variables
2023-11-05 17:52:41,236:INFO:Importing untrained model
2023-11-05 17:52:41,236:INFO:Declaring custom model
2023-11-05 17:52:41,238:INFO:Random Forest Regressor Imported successfully
2023-11-05 17:52:41,238:INFO:Starting cross validation
2023-11-05 17:52:41,241:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 17:55:45,262:INFO:Calculating mean and std
2023-11-05 17:55:45,268:INFO:Creating metrics dataframe
2023-11-05 17:55:45,282:INFO:Finalizing model
2023-11-05 17:56:06,385:INFO:Uploading results into container
2023-11-05 17:56:06,387:INFO:Uploading model into container now
2023-11-05 17:56:06,388:INFO:_master_model_container: 3
2023-11-05 17:56:06,388:INFO:_display_container: 4
2023-11-05 17:56:06,389:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-05 17:56:06,389:INFO:create_model() successfully completed......................................
2023-11-05 17:56:06,521:INFO:SubProcess create_model() end ==================================
2023-11-05 17:56:06,523:INFO:RandomForestRegressor(n_jobs=-1, random_state=123) result for MAE is 186.3094
2023-11-05 17:56:06,524:INFO:RandomForestRegressor(bootstrap=False, max_depth=11, max_features='sqrt',
                      min_impurity_decrease=0.0002, min_samples_leaf=5,
                      min_samples_split=7, n_estimators=200, n_jobs=-1,
                      random_state=123) result for MAE is 190.2233
2023-11-05 17:56:06,524:INFO:RandomForestRegressor(n_jobs=-1, random_state=123) is best model
2023-11-05 17:56:06,524:INFO:choose_better completed
2023-11-05 17:56:06,525:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-11-05 17:56:06,543:INFO:_master_model_container: 3
2023-11-05 17:56:06,543:INFO:_display_container: 3
2023-11-05 17:56:06,544:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-05 17:56:06,544:INFO:tune_model() successfully completed......................................
2023-11-05 17:56:06,668:INFO:Initializing ensemble_model()
2023-11-05 17:56:06,668:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac45888e0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), method=Boosting, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-05 17:56:06,668:INFO:Checking exceptions
2023-11-05 17:59:23,720:INFO:Importing libraries
2023-11-05 17:59:23,723:INFO:Copying training dataset
2023-11-05 17:59:23,724:INFO:Checking base model
2023-11-05 17:59:23,727:INFO:Base model : Random Forest Regressor
2023-11-05 17:59:23,729:INFO:Importing untrained ensembler
2023-11-05 17:59:23,729:INFO:Ensemble method set to Boosting
2023-11-05 17:59:23,729:INFO:SubProcess create_model() called ==================================
2023-11-05 17:59:23,736:INFO:Initializing create_model()
2023-11-05 17:59:23,736:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac45888e0>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9abd7bcc40>, model_only=True, return_train_score=False, kwargs={})
2023-11-05 17:59:23,737:INFO:Checking exceptions
2023-11-05 17:59:23,737:INFO:Importing libraries
2023-11-05 17:59:23,737:INFO:Copying training dataset
2023-11-05 17:59:23,777:INFO:Defining folds
2023-11-05 17:59:23,778:INFO:Declaring metric variables
2023-11-05 17:59:23,779:INFO:Importing untrained model
2023-11-05 17:59:23,779:INFO:Declaring custom model
2023-11-05 17:59:23,787:INFO:Random Forest Regressor Imported successfully
2023-11-05 17:59:23,790:INFO:Starting cross validation
2023-11-05 17:59:23,808:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 19:40:26,506:INFO:Calculating mean and std
2023-11-05 19:40:26,529:INFO:Creating metrics dataframe
2023-11-05 19:40:26,565:INFO:Finalizing model
2023-11-05 19:46:29,635:INFO:Uploading results into container
2023-11-05 19:46:29,640:INFO:Uploading model into container now
2023-11-05 19:46:29,643:INFO:_master_model_container: 4
2023-11-05 19:46:29,643:INFO:_display_container: 4
2023-11-05 19:46:29,647:INFO:AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-05 19:46:29,647:INFO:create_model() successfully completed......................................
2023-11-05 19:46:30,750:INFO:SubProcess create_model() end ==================================
2023-11-05 19:46:30,757:INFO:_master_model_container: 4
2023-11-05 19:46:30,758:INFO:_display_container: 4
2023-11-05 19:46:30,761:INFO:AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-05 19:46:30,762:INFO:ensemble_model() successfully completed......................................
2023-11-05 19:46:31,003:INFO:Initializing finalize_model()
2023-11-05 19:46:31,004:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac45888e0>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-05 19:46:31,006:INFO:Finalizing AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-05 19:46:31,088:INFO:Initializing create_model()
2023-11-05 19:46:31,089:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac45888e0>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-05 19:46:31,089:INFO:Checking exceptions
2023-11-05 19:46:31,090:INFO:Importing libraries
2023-11-05 19:46:31,091:INFO:Copying training dataset
2023-11-05 19:46:31,095:INFO:Defining folds
2023-11-05 19:46:31,095:INFO:Declaring metric variables
2023-11-05 19:46:31,096:INFO:Importing untrained model
2023-11-05 19:46:31,096:INFO:Declaring custom model
2023-11-05 19:46:31,098:INFO:Random Forest Regressor Imported successfully
2023-11-05 19:46:31,102:INFO:Cross validation set to False
2023-11-05 19:46:31,103:INFO:Fitting Model
2023-11-05 19:52:01,414:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))])
2023-11-05 19:52:01,422:INFO:create_model() successfully completed......................................
2023-11-05 19:52:01,981:INFO:_master_model_container: 4
2023-11-05 19:52:01,981:INFO:_display_container: 4
2023-11-05 19:52:02,068:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))])
2023-11-05 19:52:02,068:INFO:finalize_model() successfully completed......................................
2023-11-05 19:52:02,566:INFO:PyCaret RegressionExperiment
2023-11-05 19:52:02,567:INFO:Logging name: xgboost_experiment
2023-11-05 19:52:02,567:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-05 19:52:02,568:INFO:version 3.1.0
2023-11-05 19:52:02,568:INFO:Initializing setup()
2023-11-05 19:52:02,568:INFO:self.USI: e374
2023-11-05 19:52:02,568:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-05 19:52:02,568:INFO:Checking environment
2023-11-05 19:52:02,569:INFO:python_version: 3.9.6
2023-11-05 19:52:02,569:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-05 19:52:02,569:INFO:machine: x86_64
2023-11-05 19:52:02,569:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-05 19:52:02,569:INFO:Memory: svmem(total=8589934592, available=2662658048, percent=69.0, used=4235042816, free=884428800, active=1752342528, inactive=1740197888, wired=2482700288)
2023-11-05 19:52:02,570:INFO:Physical Core: 4
2023-11-05 19:52:02,570:INFO:Logical Core: 8
2023-11-05 19:52:02,570:INFO:Checking libraries
2023-11-05 19:52:02,570:INFO:System:
2023-11-05 19:52:02,570:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-05 19:52:02,570:INFO:executable: /usr/local/bin/python3.9
2023-11-05 19:52:02,571:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-05 19:52:02,571:INFO:PyCaret required dependencies:
2023-11-05 19:52:02,571:INFO:                 pip: 23.3.1
2023-11-05 19:52:02,571:INFO:          setuptools: 56.0.0
2023-11-05 19:52:02,572:INFO:             pycaret: 3.1.0
2023-11-05 19:52:02,572:INFO:             IPython: 7.28.0
2023-11-05 19:52:02,572:INFO:          ipywidgets: 8.1.1
2023-11-05 19:52:02,572:INFO:                tqdm: 4.66.1
2023-11-05 19:52:02,572:INFO:               numpy: 1.23.5
2023-11-05 19:52:02,572:INFO:              pandas: 1.5.3
2023-11-05 19:52:02,572:INFO:              jinja2: 3.0.1
2023-11-05 19:52:02,572:INFO:               scipy: 1.10.1
2023-11-05 19:52:02,572:INFO:              joblib: 1.3.2
2023-11-05 19:52:02,572:INFO:             sklearn: 1.1.3
2023-11-05 19:52:02,573:INFO:                pyod: 1.1.1
2023-11-05 19:52:02,573:INFO:            imblearn: 0.11.0
2023-11-05 19:52:02,573:INFO:   category_encoders: 2.6.3
2023-11-05 19:52:02,573:INFO:            lightgbm: 4.1.0
2023-11-05 19:52:02,573:INFO:               numba: 0.58.1
2023-11-05 19:52:02,573:INFO:            requests: 2.31.0
2023-11-05 19:52:02,574:INFO:          matplotlib: 3.4.2
2023-11-05 19:52:02,574:INFO:          scikitplot: 0.3.7
2023-11-05 19:52:02,574:INFO:         yellowbrick: 1.5
2023-11-05 19:52:02,574:INFO:              plotly: 5.18.0
2023-11-05 19:52:02,574:INFO:    plotly-resampler: Not installed
2023-11-05 19:52:02,574:INFO:             kaleido: 0.2.1
2023-11-05 19:52:02,575:INFO:           schemdraw: 0.15
2023-11-05 19:52:02,575:INFO:         statsmodels: 0.14.0
2023-11-05 19:52:02,575:INFO:              sktime: 0.21.1
2023-11-05 19:52:02,575:INFO:               tbats: 1.1.3
2023-11-05 19:52:02,575:INFO:            pmdarima: 2.0.4
2023-11-05 19:52:02,575:INFO:              psutil: 5.9.6
2023-11-05 19:52:02,576:INFO:          markupsafe: 2.1.3
2023-11-05 19:52:02,576:INFO:             pickle5: Not installed
2023-11-05 19:52:02,576:INFO:         cloudpickle: 2.2.1
2023-11-05 19:52:02,576:INFO:         deprecation: 2.1.0
2023-11-05 19:52:02,576:INFO:              xxhash: 3.4.1
2023-11-05 19:52:02,576:INFO:           wurlitzer: 3.0.3
2023-11-05 19:52:02,576:INFO:PyCaret optional dependencies:
2023-11-05 19:52:02,577:INFO:                shap: Not installed
2023-11-05 19:52:02,577:INFO:           interpret: Not installed
2023-11-05 19:52:02,577:INFO:                umap: Not installed
2023-11-05 19:52:02,577:INFO:     ydata_profiling: Not installed
2023-11-05 19:52:02,577:INFO:  explainerdashboard: Not installed
2023-11-05 19:52:02,577:INFO:             autoviz: Not installed
2023-11-05 19:52:02,578:INFO:           fairlearn: Not installed
2023-11-05 19:52:02,578:INFO:          deepchecks: Not installed
2023-11-05 19:52:02,578:INFO:             xgboost: 2.0.0
2023-11-05 19:52:02,578:INFO:            catboost: Not installed
2023-11-05 19:52:02,578:INFO:              kmodes: Not installed
2023-11-05 19:52:02,578:INFO:             mlxtend: Not installed
2023-11-05 19:52:02,579:INFO:       statsforecast: Not installed
2023-11-05 19:52:02,579:INFO:        tune_sklearn: Not installed
2023-11-05 19:52:02,579:INFO:                 ray: Not installed
2023-11-05 19:52:02,579:INFO:            hyperopt: 0.2.7
2023-11-05 19:52:02,579:INFO:              optuna: 3.4.0
2023-11-05 19:52:02,579:INFO:               skopt: Not installed
2023-11-05 19:52:02,579:INFO:              mlflow: Not installed
2023-11-05 19:52:02,579:INFO:              gradio: Not installed
2023-11-05 19:52:02,580:INFO:             fastapi: Not installed
2023-11-05 19:52:02,580:INFO:             uvicorn: Not installed
2023-11-05 19:52:02,580:INFO:              m2cgen: Not installed
2023-11-05 19:52:02,580:INFO:           evidently: Not installed
2023-11-05 19:52:02,580:INFO:               fugue: Not installed
2023-11-05 19:52:02,580:INFO:           streamlit: Not installed
2023-11-05 19:52:02,580:INFO:             prophet: Not installed
2023-11-05 19:52:02,580:INFO:None
2023-11-05 19:52:02,581:INFO:Set up data.
2023-11-05 19:52:02,715:INFO:Set up folding strategy.
2023-11-05 19:52:02,715:INFO:Set up train/test split.
2023-11-05 19:52:02,763:INFO:Set up index.
2023-11-05 19:52:02,767:INFO:Assigning column types.
2023-11-05 19:52:02,793:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-05 19:52:02,793:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 19:52:02,799:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 19:52:02,805:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 19:52:02,915:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:02,988:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 19:52:02,989:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:02,992:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:02,993:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 19:52:02,999:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,006:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,125:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,208:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,209:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:03,212:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:03,213:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-05 19:52:03,221:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,227:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,321:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,390:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,390:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:03,394:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:03,402:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,410:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,521:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,591:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,591:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:03,595:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:03,596:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-05 19:52:03,609:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,736:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,829:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,830:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:03,833:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:03,849:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 19:52:03,953:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:04,038:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 19:52:04,040:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:04,044:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:04,044:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-05 19:52:04,173:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:04,238:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 19:52:04,239:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:04,243:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:04,369:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:04,432:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 19:52:04,433:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:04,437:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:04,437:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-05 19:52:04,544:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:04,611:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:04,616:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:04,748:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 19:52:04,823:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:04,828:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:04,829:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-05 19:52:05,033:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:05,039:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:05,224:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:05,228:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:05,229:INFO:Preparing preprocessing pipeline...
2023-11-05 19:52:05,229:INFO:Set up simple imputation.
2023-11-05 19:52:05,239:INFO:Set up encoding of ordinal features.
2023-11-05 19:52:05,257:INFO:Set up encoding of categorical features.
2023-11-05 19:52:05,261:INFO:Set up column name cleaning.
2023-11-05 19:52:05,602:INFO:Finished creating preprocessing pipeline.
2023-11-05 19:52:05,696:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-05 19:52:05,696:INFO:Creating final display dataframe.
2023-11-05 19:52:07,781:WARNING:/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pycaret/internal/pipeline.py:278: UserWarning: Persisting input arguments took 0.51s to run.If this happens often in your code, it can cause performance problems (results will be correct in all cases). The reason for this is probably some large input arguments for a wrapped function.
  X, y = self._memory_full_transform(

2023-11-05 19:52:08,223:INFO:Setup _display_container:                     Description               Value
0                    Session id                 123
1                        Target              target
2                   Target type          Regression
3           Original data shape         (34060, 57)
4        Transformed data shape         (34060, 63)
5   Transformed train set shape         (23842, 63)
6    Transformed test set shape         (10218, 63)
7              Ordinal features                   2
8              Numeric features                  52
9          Categorical features                   4
10                   Preprocess                True
11              Imputation type              simple
12           Numeric imputation                mean
13       Categorical imputation                mode
14     Maximum one-hot encoding                  25
15              Encoding method                None
16               Fold Generator               KFold
17                  Fold Number                  10
18                     CPU Jobs                  -1
19                      Use GPU               False
20               Log Experiment               False
21              Experiment Name  xgboost_experiment
22                          USI                e374
2023-11-05 19:52:08,718:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:08,747:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:08,986:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 19:52:08,990:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 19:52:08,990:INFO:setup() successfully completed in 6.44s...............
2023-11-05 19:52:08,991:INFO:Initializing create_model()
2023-11-05 19:52:08,991:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 19:52:08,991:INFO:Checking exceptions
2023-11-05 19:52:08,994:INFO:Importing libraries
2023-11-05 19:52:08,994:INFO:Copying training dataset
2023-11-05 19:52:09,020:INFO:Defining folds
2023-11-05 19:52:09,020:INFO:Declaring metric variables
2023-11-05 19:52:09,020:INFO:Importing untrained model
2023-11-05 19:52:09,022:INFO:Extreme Gradient Boosting Imported successfully
2023-11-05 19:52:09,023:INFO:Starting cross validation
2023-11-05 19:52:09,029:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 19:52:37,373:INFO:Calculating mean and std
2023-11-05 19:52:37,385:INFO:Creating metrics dataframe
2023-11-05 19:52:37,403:INFO:Finalizing model
2023-11-05 19:52:39,754:INFO:Uploading results into container
2023-11-05 19:52:39,756:INFO:Uploading model into container now
2023-11-05 19:52:39,779:INFO:_master_model_container: 1
2023-11-05 19:52:39,779:INFO:_display_container: 2
2023-11-05 19:52:39,782:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-05 19:52:39,782:INFO:create_model() successfully completed......................................
2023-11-05 19:52:40,416:INFO:Initializing tune_model()
2023-11-05 19:52:40,416:INFO:tune_model(estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>)
2023-11-05 19:52:40,417:INFO:Checking exceptions
2023-11-05 19:52:40,439:INFO:Copying training dataset
2023-11-05 19:52:40,457:INFO:Checking base model
2023-11-05 19:52:40,458:INFO:Base model : Extreme Gradient Boosting
2023-11-05 19:52:40,459:INFO:Declaring metric variables
2023-11-05 19:52:40,460:INFO:Defining Hyperparameters
2023-11-05 19:52:40,630:INFO:Tuning with n_jobs=-1
2023-11-05 19:52:40,631:INFO:Initializing RandomizedSearchCV
2023-11-05 19:56:00,169:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__scale_pos_weight': 37.1, 'actual_estimator__reg_lambda': 0.7, 'actual_estimator__reg_alpha': 2, 'actual_estimator__n_estimators': 290, 'actual_estimator__min_child_weight': 3, 'actual_estimator__max_depth': 7, 'actual_estimator__learning_rate': 0.15, 'actual_estimator__colsample_bytree': 0.9}
2023-11-05 19:56:00,202:INFO:Hyperparameter search completed
2023-11-05 19:56:00,203:INFO:SubProcess create_model() called ==================================
2023-11-05 19:56:00,205:INFO:Initializing create_model()
2023-11-05 19:56:00,205:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ab02389a0>, model_only=True, return_train_score=False, kwargs={'subsample': 0.7, 'scale_pos_weight': 37.1, 'reg_lambda': 0.7, 'reg_alpha': 2, 'n_estimators': 290, 'min_child_weight': 3, 'max_depth': 7, 'learning_rate': 0.15, 'colsample_bytree': 0.9})
2023-11-05 19:56:00,205:INFO:Checking exceptions
2023-11-05 19:56:00,206:INFO:Importing libraries
2023-11-05 19:56:00,206:INFO:Copying training dataset
2023-11-05 19:56:00,262:INFO:Defining folds
2023-11-05 19:56:00,262:INFO:Declaring metric variables
2023-11-05 19:56:00,264:INFO:Importing untrained model
2023-11-05 19:56:00,264:INFO:Declaring custom model
2023-11-05 19:56:00,282:INFO:Extreme Gradient Boosting Imported successfully
2023-11-05 19:56:00,283:INFO:Starting cross validation
2023-11-05 19:56:00,287:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 19:56:30,667:INFO:Calculating mean and std
2023-11-05 19:56:30,670:INFO:Creating metrics dataframe
2023-11-05 19:56:30,678:INFO:Finalizing model
2023-11-05 19:56:36,911:INFO:Uploading results into container
2023-11-05 19:56:36,916:INFO:Uploading model into container now
2023-11-05 19:56:36,920:INFO:_master_model_container: 2
2023-11-05 19:56:36,921:INFO:_display_container: 3
2023-11-05 19:56:36,924:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-05 19:56:36,925:INFO:create_model() successfully completed......................................
2023-11-05 19:56:38,031:INFO:SubProcess create_model() end ==================================
2023-11-05 19:56:38,032:INFO:choose_better activated
2023-11-05 19:56:38,032:INFO:SubProcess create_model() called ==================================
2023-11-05 19:56:38,037:INFO:Initializing create_model()
2023-11-05 19:56:38,038:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 19:56:38,038:INFO:Checking exceptions
2023-11-05 19:56:38,041:INFO:Importing libraries
2023-11-05 19:56:38,041:INFO:Copying training dataset
2023-11-05 19:56:38,109:INFO:Defining folds
2023-11-05 19:56:38,109:INFO:Declaring metric variables
2023-11-05 19:56:38,110:INFO:Importing untrained model
2023-11-05 19:56:38,110:INFO:Declaring custom model
2023-11-05 19:56:38,120:INFO:Extreme Gradient Boosting Imported successfully
2023-11-05 19:56:38,121:INFO:Starting cross validation
2023-11-05 19:56:38,136:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 19:56:56,767:INFO:Calculating mean and std
2023-11-05 19:56:56,769:INFO:Creating metrics dataframe
2023-11-05 19:56:56,781:INFO:Finalizing model
2023-11-05 19:56:59,033:INFO:Uploading results into container
2023-11-05 19:56:59,034:INFO:Uploading model into container now
2023-11-05 19:56:59,035:INFO:_master_model_container: 3
2023-11-05 19:56:59,036:INFO:_display_container: 4
2023-11-05 19:56:59,038:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-05 19:56:59,038:INFO:create_model() successfully completed......................................
2023-11-05 19:56:59,215:INFO:SubProcess create_model() end ==================================
2023-11-05 19:56:59,219:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) result for MAE is 188.4788
2023-11-05 19:56:59,222:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) result for MAE is 179.4931
2023-11-05 19:56:59,224:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) is best model
2023-11-05 19:56:59,224:INFO:choose_better completed
2023-11-05 19:56:59,231:INFO:_master_model_container: 3
2023-11-05 19:56:59,231:INFO:_display_container: 3
2023-11-05 19:56:59,233:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-05 19:56:59,234:INFO:tune_model() successfully completed......................................
2023-11-05 19:56:59,362:INFO:Initializing ensemble_model()
2023-11-05 19:56:59,362:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), method=Bagging, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-05 19:56:59,362:INFO:Checking exceptions
2023-11-05 19:56:59,377:INFO:Importing libraries
2023-11-05 19:56:59,378:INFO:Copying training dataset
2023-11-05 19:56:59,378:INFO:Checking base model
2023-11-05 19:56:59,378:INFO:Base model : Extreme Gradient Boosting
2023-11-05 19:56:59,378:INFO:Importing untrained ensembler
2023-11-05 19:56:59,379:INFO:Ensemble method set to Bagging
2023-11-05 19:56:59,379:INFO:SubProcess create_model() called ==================================
2023-11-05 19:56:59,383:INFO:Initializing create_model()
2023-11-05 19:56:59,384:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ad8d033a0>, model_only=True, return_train_score=False, kwargs={})
2023-11-05 19:56:59,384:INFO:Checking exceptions
2023-11-05 19:56:59,384:INFO:Importing libraries
2023-11-05 19:56:59,384:INFO:Copying training dataset
2023-11-05 19:56:59,410:INFO:Defining folds
2023-11-05 19:56:59,410:INFO:Declaring metric variables
2023-11-05 19:56:59,410:INFO:Importing untrained model
2023-11-05 19:56:59,410:INFO:Declaring custom model
2023-11-05 19:56:59,414:INFO:Extreme Gradient Boosting Imported successfully
2023-11-05 19:56:59,415:INFO:Starting cross validation
2023-11-05 19:56:59,420:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 20:00:59,834:INFO:Calculating mean and std
2023-11-05 20:00:59,849:INFO:Creating metrics dataframe
2023-11-05 20:00:59,869:INFO:Finalizing model
2023-11-05 20:01:44,886:INFO:Uploading results into container
2023-11-05 20:01:44,889:INFO:Uploading model into container now
2023-11-05 20:01:44,893:INFO:_master_model_container: 4
2023-11-05 20:01:44,893:INFO:_display_container: 4
2023-11-05 20:01:44,900:INFO:BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-05 20:01:44,900:INFO:create_model() successfully completed......................................
2023-11-05 20:01:45,571:INFO:SubProcess create_model() end ==================================
2023-11-05 20:01:45,576:INFO:_master_model_container: 4
2023-11-05 20:01:45,577:INFO:_display_container: 4
2023-11-05 20:01:45,581:INFO:BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-05 20:01:45,581:INFO:ensemble_model() successfully completed......................................
2023-11-05 20:01:45,695:INFO:Initializing finalize_model()
2023-11-05 20:01:45,695:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-05 20:01:45,697:INFO:Finalizing BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-05 20:01:45,727:INFO:Initializing create_model()
2023-11-05 20:01:45,727:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-05 20:01:45,727:INFO:Checking exceptions
2023-11-05 20:01:45,728:INFO:Importing libraries
2023-11-05 20:01:45,728:INFO:Copying training dataset
2023-11-05 20:01:45,730:INFO:Defining folds
2023-11-05 20:01:45,730:INFO:Declaring metric variables
2023-11-05 20:01:45,730:INFO:Importing untrained model
2023-11-05 20:01:45,730:INFO:Declaring custom model
2023-11-05 20:01:45,732:INFO:Extreme Gradient Boosting Imported successfully
2023-11-05 20:01:45,734:INFO:Cross validation set to False
2023-11-05 20:01:45,734:INFO:Fitting Model
2023-11-05 20:02:32,576:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))])
2023-11-05 20:02:32,577:INFO:create_model() successfully completed......................................
2023-11-05 20:02:32,733:INFO:_master_model_container: 4
2023-11-05 20:02:32,733:INFO:_display_container: 4
2023-11-05 20:02:32,813:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))])
2023-11-05 20:02:32,814:INFO:finalize_model() successfully completed......................................
2023-11-05 20:02:33,017:INFO:Initializing predict_model()
2023-11-05 20:02:33,017:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(bagging_fraction=0.6,
                                                               bagging_freq=2,
                                                               feature_fraction=0.4,
                                                               min_child_samples=41,
                                                               min_split_gain=0.9,
                                                               n_estimators=260,
                                                               n_jobs=-1,
                                                               num_leaves=70,
                                                               random_state=123,
                                                               reg_alpha=2,
                                                               reg_lambda=3),
                                  random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9ab0291d30>)
2023-11-05 20:02:33,017:INFO:Checking exceptions
2023-11-05 20:02:33,017:INFO:Preloading libraries
2023-11-05 20:02:33,017:INFO:Set up data.
2023-11-05 20:02:33,046:INFO:Set up index.
2023-11-05 20:02:33,607:INFO:Initializing predict_model()
2023-11-05 20:02:33,607:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9ab0291d30>)
2023-11-05 20:02:33,607:INFO:Checking exceptions
2023-11-05 20:02:33,607:INFO:Preloading libraries
2023-11-05 20:02:33,607:INFO:Set up data.
2023-11-05 20:02:33,631:INFO:Set up index.
2023-11-05 20:02:37,431:INFO:Initializing predict_model()
2023-11-05 20:02:37,432:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab02ba3a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9ab0291d30>)
2023-11-05 20:02:37,432:INFO:Checking exceptions
2023-11-05 20:02:37,432:INFO:Preloading libraries
2023-11-05 20:02:37,433:INFO:Set up data.
2023-11-05 20:02:37,507:INFO:Set up index.
2023-11-05 20:02:38,031:INFO:PyCaret RegressionExperiment
2023-11-05 20:02:38,031:INFO:Logging name: lightgbm_experiment
2023-11-05 20:02:38,031:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-05 20:02:38,031:INFO:version 3.1.0
2023-11-05 20:02:38,032:INFO:Initializing setup()
2023-11-05 20:02:38,032:INFO:self.USI: 1e26
2023-11-05 20:02:38,032:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-05 20:02:38,033:INFO:Checking environment
2023-11-05 20:02:38,033:INFO:python_version: 3.9.6
2023-11-05 20:02:38,033:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-05 20:02:38,033:INFO:machine: x86_64
2023-11-05 20:02:38,033:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-05 20:02:38,033:INFO:Memory: svmem(total=8589934592, available=1778995200, percent=79.3, used=4254789632, free=31453184, active=1749729280, inactive=1741561856, wired=2505060352)
2023-11-05 20:02:38,033:INFO:Physical Core: 4
2023-11-05 20:02:38,033:INFO:Logical Core: 8
2023-11-05 20:02:38,033:INFO:Checking libraries
2023-11-05 20:02:38,033:INFO:System:
2023-11-05 20:02:38,034:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-05 20:02:38,034:INFO:executable: /usr/local/bin/python3.9
2023-11-05 20:02:38,034:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-05 20:02:38,034:INFO:PyCaret required dependencies:
2023-11-05 20:02:38,034:INFO:                 pip: 23.3.1
2023-11-05 20:02:38,034:INFO:          setuptools: 56.0.0
2023-11-05 20:02:38,034:INFO:             pycaret: 3.1.0
2023-11-05 20:02:38,034:INFO:             IPython: 7.28.0
2023-11-05 20:02:38,034:INFO:          ipywidgets: 8.1.1
2023-11-05 20:02:38,034:INFO:                tqdm: 4.66.1
2023-11-05 20:02:38,035:INFO:               numpy: 1.23.5
2023-11-05 20:02:38,035:INFO:              pandas: 1.5.3
2023-11-05 20:02:38,035:INFO:              jinja2: 3.0.1
2023-11-05 20:02:38,035:INFO:               scipy: 1.10.1
2023-11-05 20:02:38,035:INFO:              joblib: 1.3.2
2023-11-05 20:02:38,035:INFO:             sklearn: 1.1.3
2023-11-05 20:02:38,035:INFO:                pyod: 1.1.1
2023-11-05 20:02:38,035:INFO:            imblearn: 0.11.0
2023-11-05 20:02:38,035:INFO:   category_encoders: 2.6.3
2023-11-05 20:02:38,035:INFO:            lightgbm: 4.1.0
2023-11-05 20:02:38,035:INFO:               numba: 0.58.1
2023-11-05 20:02:38,035:INFO:            requests: 2.31.0
2023-11-05 20:02:38,035:INFO:          matplotlib: 3.4.2
2023-11-05 20:02:38,035:INFO:          scikitplot: 0.3.7
2023-11-05 20:02:38,036:INFO:         yellowbrick: 1.5
2023-11-05 20:02:38,036:INFO:              plotly: 5.18.0
2023-11-05 20:02:38,036:INFO:    plotly-resampler: Not installed
2023-11-05 20:02:38,036:INFO:             kaleido: 0.2.1
2023-11-05 20:02:38,036:INFO:           schemdraw: 0.15
2023-11-05 20:02:38,036:INFO:         statsmodels: 0.14.0
2023-11-05 20:02:38,036:INFO:              sktime: 0.21.1
2023-11-05 20:02:38,036:INFO:               tbats: 1.1.3
2023-11-05 20:02:38,037:INFO:            pmdarima: 2.0.4
2023-11-05 20:02:38,037:INFO:              psutil: 5.9.6
2023-11-05 20:02:38,037:INFO:          markupsafe: 2.1.3
2023-11-05 20:02:38,037:INFO:             pickle5: Not installed
2023-11-05 20:02:38,037:INFO:         cloudpickle: 2.2.1
2023-11-05 20:02:38,037:INFO:         deprecation: 2.1.0
2023-11-05 20:02:38,037:INFO:              xxhash: 3.4.1
2023-11-05 20:02:38,037:INFO:           wurlitzer: 3.0.3
2023-11-05 20:02:38,037:INFO:PyCaret optional dependencies:
2023-11-05 20:02:38,038:INFO:                shap: Not installed
2023-11-05 20:02:38,038:INFO:           interpret: Not installed
2023-11-05 20:02:38,038:INFO:                umap: Not installed
2023-11-05 20:02:38,038:INFO:     ydata_profiling: Not installed
2023-11-05 20:02:38,038:INFO:  explainerdashboard: Not installed
2023-11-05 20:02:38,038:INFO:             autoviz: Not installed
2023-11-05 20:02:38,038:INFO:           fairlearn: Not installed
2023-11-05 20:02:38,038:INFO:          deepchecks: Not installed
2023-11-05 20:02:38,038:INFO:             xgboost: 2.0.0
2023-11-05 20:02:38,038:INFO:            catboost: Not installed
2023-11-05 20:02:38,038:INFO:              kmodes: Not installed
2023-11-05 20:02:38,038:INFO:             mlxtend: Not installed
2023-11-05 20:02:38,038:INFO:       statsforecast: Not installed
2023-11-05 20:02:38,038:INFO:        tune_sklearn: Not installed
2023-11-05 20:02:38,038:INFO:                 ray: Not installed
2023-11-05 20:02:38,039:INFO:            hyperopt: 0.2.7
2023-11-05 20:02:38,039:INFO:              optuna: 3.4.0
2023-11-05 20:02:38,039:INFO:               skopt: Not installed
2023-11-05 20:02:38,039:INFO:              mlflow: Not installed
2023-11-05 20:02:38,039:INFO:              gradio: Not installed
2023-11-05 20:02:38,039:INFO:             fastapi: Not installed
2023-11-05 20:02:38,039:INFO:             uvicorn: Not installed
2023-11-05 20:02:38,039:INFO:              m2cgen: Not installed
2023-11-05 20:02:38,039:INFO:           evidently: Not installed
2023-11-05 20:02:38,039:INFO:               fugue: Not installed
2023-11-05 20:02:38,039:INFO:           streamlit: Not installed
2023-11-05 20:02:38,039:INFO:             prophet: Not installed
2023-11-05 20:02:38,039:INFO:None
2023-11-05 20:02:38,039:INFO:Set up data.
2023-11-05 20:02:38,215:INFO:Set up folding strategy.
2023-11-05 20:02:38,216:INFO:Set up train/test split.
2023-11-05 20:02:38,266:INFO:Set up index.
2023-11-05 20:02:38,271:INFO:Assigning column types.
2023-11-05 20:02:38,298:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-05 20:02:38,299:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,312:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,321:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,480:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,570:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,571:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:38,577:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:38,578:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,586:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,595:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,725:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,834:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,835:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:38,841:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:38,841:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-05 20:02:38,851:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 20:02:38,863:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,012:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,138:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,139:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:39,144:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:39,153:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,162:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,312:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,416:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,417:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:39,423:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:39,424:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-05 20:02:39,442:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,619:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,731:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,731:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:39,736:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:39,753:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,853:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,922:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:02:39,923:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:39,927:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:39,927:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-05 20:02:40,046:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:40,117:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:02:40,118:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:40,122:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:40,226:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:40,301:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:02:40,302:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:40,309:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:40,309:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-05 20:02:40,430:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:40,507:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:40,513:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:40,637:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:02:40,702:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:40,706:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:40,706:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-05 20:02:40,878:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:40,882:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:41,094:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:41,097:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:41,098:INFO:Preparing preprocessing pipeline...
2023-11-05 20:02:41,098:INFO:Set up simple imputation.
2023-11-05 20:02:41,107:INFO:Set up encoding of ordinal features.
2023-11-05 20:02:41,123:INFO:Set up encoding of categorical features.
2023-11-05 20:02:41,126:INFO:Set up column name cleaning.
2023-11-05 20:02:41,853:INFO:Finished creating preprocessing pipeline.
2023-11-05 20:02:41,948:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-05 20:02:41,948:INFO:Creating final display dataframe.
2023-11-05 20:02:44,797:INFO:Setup _display_container:                     Description                Value
0                    Session id                  123
1                        Target               target
2                   Target type           Regression
3           Original data shape          (29596, 57)
4        Transformed data shape          (29596, 62)
5   Transformed train set shape          (20717, 62)
6    Transformed test set shape           (8879, 62)
7              Ordinal features                    2
8              Numeric features                   52
9          Categorical features                    4
10                   Preprocess                 True
11              Imputation type               simple
12           Numeric imputation                 mean
13       Categorical imputation                 mode
14     Maximum one-hot encoding                   25
15              Encoding method                 None
16               Fold Generator                KFold
17                  Fold Number                   10
18                     CPU Jobs                   -1
19                      Use GPU                False
20               Log Experiment                False
21              Experiment Name  lightgbm_experiment
22                          USI                 1e26
2023-11-05 20:02:45,203:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:45,210:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:45,705:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:02:45,716:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:02:45,719:INFO:setup() successfully completed in 7.69s...............
2023-11-05 20:02:45,719:INFO:Initializing create_model()
2023-11-05 20:02:45,720:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac515b9a0>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 20:02:45,720:INFO:Checking exceptions
2023-11-05 20:02:45,725:INFO:Importing libraries
2023-11-05 20:02:45,726:INFO:Copying training dataset
2023-11-05 20:02:45,779:INFO:Defining folds
2023-11-05 20:02:45,780:INFO:Declaring metric variables
2023-11-05 20:02:45,781:INFO:Importing untrained model
2023-11-05 20:02:45,783:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 20:02:45,784:INFO:Starting cross validation
2023-11-05 20:02:45,792:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 20:03:14,832:INFO:Calculating mean and std
2023-11-05 20:03:14,858:INFO:Creating metrics dataframe
2023-11-05 20:03:14,863:INFO:Finalizing model
2023-11-05 20:03:15,564:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012533 seconds.
2023-11-05 20:03:15,564:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:03:15,565:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:03:15,568:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:03:15,569:INFO:[LightGBM] [Info] Start training from score 94.273259
2023-11-05 20:03:16,369:INFO:Uploading results into container
2023-11-05 20:03:16,371:INFO:Uploading model into container now
2023-11-05 20:03:16,386:INFO:_master_model_container: 1
2023-11-05 20:03:16,387:INFO:_display_container: 2
2023-11-05 20:03:16,388:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-05 20:03:16,389:INFO:create_model() successfully completed......................................
2023-11-05 20:03:16,612:INFO:Initializing tune_model()
2023-11-05 20:03:16,632:INFO:tune_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac515b9a0>)
2023-11-05 20:03:16,633:INFO:Checking exceptions
2023-11-05 20:03:16,653:INFO:Copying training dataset
2023-11-05 20:03:16,676:INFO:Checking base model
2023-11-05 20:03:16,676:INFO:Base model : Light Gradient Boosting Machine
2023-11-05 20:03:16,678:INFO:Declaring metric variables
2023-11-05 20:03:16,678:INFO:Defining Hyperparameters
2023-11-05 20:03:16,835:INFO:Tuning with n_jobs=-1
2023-11-05 20:03:16,835:INFO:Initializing RandomizedSearchCV
2023-11-05 20:10:19,285:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2023-11-05 20:10:19,300:INFO:Hyperparameter search completed
2023-11-05 20:10:19,300:INFO:SubProcess create_model() called ==================================
2023-11-05 20:10:19,304:INFO:Initializing create_model()
2023-11-05 20:10:19,305:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac515b9a0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ada010f40>, model_only=True, return_train_score=False, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2023-11-05 20:10:19,305:INFO:Checking exceptions
2023-11-05 20:10:19,305:INFO:Importing libraries
2023-11-05 20:10:19,306:INFO:Copying training dataset
2023-11-05 20:10:19,392:INFO:Defining folds
2023-11-05 20:10:19,393:INFO:Declaring metric variables
2023-11-05 20:10:19,395:INFO:Importing untrained model
2023-11-05 20:10:19,395:INFO:Declaring custom model
2023-11-05 20:10:19,399:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 20:10:19,400:INFO:Starting cross validation
2023-11-05 20:10:19,406:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 20:12:05,368:INFO:Calculating mean and std
2023-11-05 20:12:05,371:INFO:Creating metrics dataframe
2023-11-05 20:12:05,379:INFO:Finalizing model
2023-11-05 20:12:05,926:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 20:12:05,926:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 20:12:05,926:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 20:12:06,017:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-05 20:12:06,018:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-05 20:12:06,018:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-05 20:12:06,036:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010896 seconds.
2023-11-05 20:12:06,036:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:12:06,038:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:12:06,046:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:12:06,050:INFO:[LightGBM] [Info] Start training from score 94.273259
2023-11-05 20:12:06,063:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-05 20:12:06,078:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-05 20:12:06,094:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-05 20:12:09,002:INFO:Uploading results into container
2023-11-05 20:12:09,005:INFO:Uploading model into container now
2023-11-05 20:12:09,008:INFO:_master_model_container: 2
2023-11-05 20:12:09,008:INFO:_display_container: 3
2023-11-05 20:12:09,011:INFO:LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3)
2023-11-05 20:12:09,011:INFO:create_model() successfully completed......................................
2023-11-05 20:12:09,628:INFO:SubProcess create_model() end ==================================
2023-11-05 20:12:09,628:INFO:choose_better activated
2023-11-05 20:12:09,629:INFO:SubProcess create_model() called ==================================
2023-11-05 20:12:09,630:INFO:Initializing create_model()
2023-11-05 20:12:09,630:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac515b9a0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 20:12:09,631:INFO:Checking exceptions
2023-11-05 20:12:09,632:INFO:Importing libraries
2023-11-05 20:12:09,633:INFO:Copying training dataset
2023-11-05 20:12:09,661:INFO:Defining folds
2023-11-05 20:12:09,661:INFO:Declaring metric variables
2023-11-05 20:12:09,661:INFO:Importing untrained model
2023-11-05 20:12:09,662:INFO:Declaring custom model
2023-11-05 20:12:09,663:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 20:12:09,663:INFO:Starting cross validation
2023-11-05 20:12:09,665:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 20:12:36,478:INFO:Calculating mean and std
2023-11-05 20:12:36,479:INFO:Creating metrics dataframe
2023-11-05 20:12:36,483:INFO:Finalizing model
2023-11-05 20:12:37,106:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013784 seconds.
2023-11-05 20:12:37,107:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:12:37,107:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:12:37,109:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:12:37,110:INFO:[LightGBM] [Info] Start training from score 94.273259
2023-11-05 20:12:37,603:INFO:Uploading results into container
2023-11-05 20:12:37,605:INFO:Uploading model into container now
2023-11-05 20:12:37,605:INFO:_master_model_container: 3
2023-11-05 20:12:37,605:INFO:_display_container: 4
2023-11-05 20:12:37,606:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-05 20:12:37,606:INFO:create_model() successfully completed......................................
2023-11-05 20:12:37,768:INFO:SubProcess create_model() end ==================================
2023-11-05 20:12:37,769:INFO:LGBMRegressor(n_jobs=-1, random_state=123) result for MAE is 24.1582
2023-11-05 20:12:37,771:INFO:LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3) result for MAE is 24.9544
2023-11-05 20:12:37,771:INFO:LGBMRegressor(n_jobs=-1, random_state=123) is best model
2023-11-05 20:12:37,772:INFO:choose_better completed
2023-11-05 20:12:37,772:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-11-05 20:12:37,781:INFO:_master_model_container: 3
2023-11-05 20:12:37,781:INFO:_display_container: 3
2023-11-05 20:12:37,782:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-05 20:12:37,782:INFO:tune_model() successfully completed......................................
2023-11-05 20:12:37,903:INFO:Initializing ensemble_model()
2023-11-05 20:12:37,904:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac515b9a0>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), method=Bagging, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-05 20:12:37,904:INFO:Checking exceptions
2023-11-05 20:12:37,915:INFO:Importing libraries
2023-11-05 20:12:37,915:INFO:Copying training dataset
2023-11-05 20:12:37,915:INFO:Checking base model
2023-11-05 20:12:37,915:INFO:Base model : Light Gradient Boosting Machine
2023-11-05 20:12:37,916:INFO:Importing untrained ensembler
2023-11-05 20:12:37,916:INFO:Ensemble method set to Bagging
2023-11-05 20:12:37,917:INFO:SubProcess create_model() called ==================================
2023-11-05 20:12:37,918:INFO:Initializing create_model()
2023-11-05 20:12:37,919:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac515b9a0>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9abdc20400>, model_only=True, return_train_score=False, kwargs={})
2023-11-05 20:12:37,919:INFO:Checking exceptions
2023-11-05 20:12:37,919:INFO:Importing libraries
2023-11-05 20:12:37,919:INFO:Copying training dataset
2023-11-05 20:12:37,938:INFO:Defining folds
2023-11-05 20:12:37,939:INFO:Declaring metric variables
2023-11-05 20:12:37,939:INFO:Importing untrained model
2023-11-05 20:12:37,939:INFO:Declaring custom model
2023-11-05 20:12:37,941:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 20:12:37,941:INFO:Starting cross validation
2023-11-05 20:12:37,945:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 20:15:54,365:INFO:Calculating mean and std
2023-11-05 20:15:54,370:INFO:Creating metrics dataframe
2023-11-05 20:15:54,374:INFO:Finalizing model
2023-11-05 20:15:55,017:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011824 seconds.
2023-11-05 20:15:55,018:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:15:55,018:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:15:55,020:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:15:55,021:INFO:[LightGBM] [Info] Start training from score 92.657530
2023-11-05 20:15:55,724:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022241 seconds.
2023-11-05 20:15:55,724:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:15:55,724:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:15:55,726:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:15:55,728:INFO:[LightGBM] [Info] Start training from score 91.918549
2023-11-05 20:15:56,510:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017502 seconds.
2023-11-05 20:15:56,511:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:15:56,511:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:15:56,513:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:15:56,514:INFO:[LightGBM] [Info] Start training from score 92.751592
2023-11-05 20:15:57,262:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014591 seconds.
2023-11-05 20:15:57,262:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:15:57,263:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:15:57,264:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:15:57,266:INFO:[LightGBM] [Info] Start training from score 92.467848
2023-11-05 20:15:57,974:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017281 seconds.
2023-11-05 20:15:57,974:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:15:57,975:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:15:57,977:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:15:57,978:INFO:[LightGBM] [Info] Start training from score 93.446612
2023-11-05 20:15:58,777:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011285 seconds.
2023-11-05 20:15:58,777:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:15:58,777:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:15:58,779:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:15:58,780:INFO:[LightGBM] [Info] Start training from score 94.666790
2023-11-05 20:15:59,631:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013083 seconds.
2023-11-05 20:15:59,631:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:15:59,631:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:15:59,633:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:15:59,634:INFO:[LightGBM] [Info] Start training from score 94.581870
2023-11-05 20:16:03,798:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015903 seconds.
2023-11-05 20:16:03,798:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:03,798:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:16:03,800:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:16:03,802:INFO:[LightGBM] [Info] Start training from score 94.747882
2023-11-05 20:16:05,070:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013225 seconds.
2023-11-05 20:16:05,070:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:05,070:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:16:05,073:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:16:05,074:INFO:[LightGBM] [Info] Start training from score 92.610634
2023-11-05 20:16:05,723:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013497 seconds.
2023-11-05 20:16:05,723:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:05,724:INFO:[LightGBM] [Info] Total Bins 9893
2023-11-05 20:16:05,725:INFO:[LightGBM] [Info] Number of data points in the train set: 20717, number of used features: 60
2023-11-05 20:16:05,726:INFO:[LightGBM] [Info] Start training from score 94.572736
2023-11-05 20:16:06,289:INFO:Uploading results into container
2023-11-05 20:16:06,291:INFO:Uploading model into container now
2023-11-05 20:16:06,293:INFO:_master_model_container: 4
2023-11-05 20:16:06,293:INFO:_display_container: 4
2023-11-05 20:16:06,296:INFO:BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123)
2023-11-05 20:16:06,296:INFO:create_model() successfully completed......................................
2023-11-05 20:16:06,465:INFO:SubProcess create_model() end ==================================
2023-11-05 20:16:06,473:INFO:_master_model_container: 4
2023-11-05 20:16:06,474:INFO:_display_container: 4
2023-11-05 20:16:06,476:INFO:BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123)
2023-11-05 20:16:06,476:INFO:ensemble_model() successfully completed......................................
2023-11-05 20:16:06,602:INFO:Initializing finalize_model()
2023-11-05 20:16:06,602:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac515b9a0>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-05 20:16:06,604:INFO:Finalizing BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123)
2023-11-05 20:16:06,631:INFO:Initializing create_model()
2023-11-05 20:16:06,631:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac515b9a0>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-05 20:16:06,631:INFO:Checking exceptions
2023-11-05 20:16:06,632:INFO:Importing libraries
2023-11-05 20:16:06,632:INFO:Copying training dataset
2023-11-05 20:16:06,634:INFO:Defining folds
2023-11-05 20:16:06,634:INFO:Declaring metric variables
2023-11-05 20:16:06,634:INFO:Importing untrained model
2023-11-05 20:16:06,635:INFO:Declaring custom model
2023-11-05 20:16:06,636:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-05 20:16:06,639:INFO:Cross validation set to False
2023-11-05 20:16:06,639:INFO:Fitting Model
2023-11-05 20:16:07,531:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.018722 seconds.
2023-11-05 20:16:07,532:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:07,532:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:07,533:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:07,535:INFO:[LightGBM] [Info] Start training from score 92.572037
2023-11-05 20:16:08,624:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015106 seconds.
2023-11-05 20:16:08,625:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:08,625:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:08,627:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:08,627:INFO:[LightGBM] [Info] Start training from score 91.311253
2023-11-05 20:16:09,520:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031756 seconds.
2023-11-05 20:16:09,521:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:09,521:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:09,523:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:09,523:INFO:[LightGBM] [Info] Start training from score 92.514357
2023-11-05 20:16:10,593:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017134 seconds.
2023-11-05 20:16:10,594:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:10,594:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:10,596:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:10,597:INFO:[LightGBM] [Info] Start training from score 92.324060
2023-11-05 20:16:11,793:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.032138 seconds.
2023-11-05 20:16:11,793:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:11,794:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:11,797:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:11,811:INFO:[LightGBM] [Info] Start training from score 91.223632
2023-11-05 20:16:15,843:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022004 seconds.
2023-11-05 20:16:15,843:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:15,844:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:15,846:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:15,848:INFO:[LightGBM] [Info] Start training from score 93.641266
2023-11-05 20:16:16,817:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.024050 seconds.
2023-11-05 20:16:16,817:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:16,818:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:16,819:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:16,820:INFO:[LightGBM] [Info] Start training from score 92.987896
2023-11-05 20:16:17,563:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020315 seconds.
2023-11-05 20:16:17,563:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:17,564:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:17,565:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:17,565:INFO:[LightGBM] [Info] Start training from score 94.451583
2023-11-05 20:16:18,338:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.020756 seconds.
2023-11-05 20:16:18,339:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:18,339:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:18,341:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:18,342:INFO:[LightGBM] [Info] Start training from score 91.595155
2023-11-05 20:16:19,270:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019682 seconds.
2023-11-05 20:16:19,271:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-05 20:16:19,271:INFO:[LightGBM] [Info] Total Bins 9997
2023-11-05 20:16:19,273:INFO:[LightGBM] [Info] Number of data points in the train set: 29596, number of used features: 60
2023-11-05 20:16:19,274:INFO:[LightGBM] [Info] Start training from score 94.135274
2023-11-05 20:16:20,084:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1,
                                                               random_state=123),
                                  random_state=123))])
2023-11-05 20:16:20,085:INFO:create_model() successfully completed......................................
2023-11-05 20:16:20,221:INFO:_master_model_container: 4
2023-11-05 20:16:20,222:INFO:_display_container: 4
2023-11-05 20:16:20,311:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1,
                                                               random_state=123),
                                  random_state=123))])
2023-11-05 20:16:20,311:INFO:finalize_model() successfully completed......................................
2023-11-05 20:16:20,497:INFO:PyCaret RegressionExperiment
2023-11-05 20:16:20,497:INFO:Logging name: rf_experiment
2023-11-05 20:16:20,497:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-05 20:16:20,497:INFO:version 3.1.0
2023-11-05 20:16:20,497:INFO:Initializing setup()
2023-11-05 20:16:20,497:INFO:self.USI: b7ab
2023-11-05 20:16:20,497:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-05 20:16:20,497:INFO:Checking environment
2023-11-05 20:16:20,497:INFO:python_version: 3.9.6
2023-11-05 20:16:20,497:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-05 20:16:20,497:INFO:machine: x86_64
2023-11-05 20:16:20,497:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-05 20:16:20,497:INFO:Memory: svmem(total=8589934592, available=2248957952, percent=73.8, used=4421353472, free=201244672, active=2048540672, inactive=2034278400, wired=2372812800)
2023-11-05 20:16:20,497:INFO:Physical Core: 4
2023-11-05 20:16:20,497:INFO:Logical Core: 8
2023-11-05 20:16:20,498:INFO:Checking libraries
2023-11-05 20:16:20,498:INFO:System:
2023-11-05 20:16:20,498:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-05 20:16:20,498:INFO:executable: /usr/local/bin/python3.9
2023-11-05 20:16:20,498:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-05 20:16:20,498:INFO:PyCaret required dependencies:
2023-11-05 20:16:20,498:INFO:                 pip: 23.3.1
2023-11-05 20:16:20,498:INFO:          setuptools: 56.0.0
2023-11-05 20:16:20,498:INFO:             pycaret: 3.1.0
2023-11-05 20:16:20,498:INFO:             IPython: 7.28.0
2023-11-05 20:16:20,498:INFO:          ipywidgets: 8.1.1
2023-11-05 20:16:20,498:INFO:                tqdm: 4.66.1
2023-11-05 20:16:20,498:INFO:               numpy: 1.23.5
2023-11-05 20:16:20,498:INFO:              pandas: 1.5.3
2023-11-05 20:16:20,498:INFO:              jinja2: 3.0.1
2023-11-05 20:16:20,498:INFO:               scipy: 1.10.1
2023-11-05 20:16:20,498:INFO:              joblib: 1.3.2
2023-11-05 20:16:20,498:INFO:             sklearn: 1.1.3
2023-11-05 20:16:20,498:INFO:                pyod: 1.1.1
2023-11-05 20:16:20,498:INFO:            imblearn: 0.11.0
2023-11-05 20:16:20,498:INFO:   category_encoders: 2.6.3
2023-11-05 20:16:20,498:INFO:            lightgbm: 4.1.0
2023-11-05 20:16:20,498:INFO:               numba: 0.58.1
2023-11-05 20:16:20,499:INFO:            requests: 2.31.0
2023-11-05 20:16:20,499:INFO:          matplotlib: 3.4.2
2023-11-05 20:16:20,499:INFO:          scikitplot: 0.3.7
2023-11-05 20:16:20,499:INFO:         yellowbrick: 1.5
2023-11-05 20:16:20,499:INFO:              plotly: 5.18.0
2023-11-05 20:16:20,499:INFO:    plotly-resampler: Not installed
2023-11-05 20:16:20,499:INFO:             kaleido: 0.2.1
2023-11-05 20:16:20,499:INFO:           schemdraw: 0.15
2023-11-05 20:16:20,499:INFO:         statsmodels: 0.14.0
2023-11-05 20:16:20,499:INFO:              sktime: 0.21.1
2023-11-05 20:16:20,499:INFO:               tbats: 1.1.3
2023-11-05 20:16:20,499:INFO:            pmdarima: 2.0.4
2023-11-05 20:16:20,499:INFO:              psutil: 5.9.6
2023-11-05 20:16:20,499:INFO:          markupsafe: 2.1.3
2023-11-05 20:16:20,499:INFO:             pickle5: Not installed
2023-11-05 20:16:20,499:INFO:         cloudpickle: 2.2.1
2023-11-05 20:16:20,499:INFO:         deprecation: 2.1.0
2023-11-05 20:16:20,499:INFO:              xxhash: 3.4.1
2023-11-05 20:16:20,499:INFO:           wurlitzer: 3.0.3
2023-11-05 20:16:20,499:INFO:PyCaret optional dependencies:
2023-11-05 20:16:20,499:INFO:                shap: Not installed
2023-11-05 20:16:20,499:INFO:           interpret: Not installed
2023-11-05 20:16:20,499:INFO:                umap: Not installed
2023-11-05 20:16:20,499:INFO:     ydata_profiling: Not installed
2023-11-05 20:16:20,499:INFO:  explainerdashboard: Not installed
2023-11-05 20:16:20,500:INFO:             autoviz: Not installed
2023-11-05 20:16:20,500:INFO:           fairlearn: Not installed
2023-11-05 20:16:20,500:INFO:          deepchecks: Not installed
2023-11-05 20:16:20,500:INFO:             xgboost: 2.0.0
2023-11-05 20:16:20,500:INFO:            catboost: Not installed
2023-11-05 20:16:20,500:INFO:              kmodes: Not installed
2023-11-05 20:16:20,500:INFO:             mlxtend: Not installed
2023-11-05 20:16:20,500:INFO:       statsforecast: Not installed
2023-11-05 20:16:20,500:INFO:        tune_sklearn: Not installed
2023-11-05 20:16:20,500:INFO:                 ray: Not installed
2023-11-05 20:16:20,500:INFO:            hyperopt: 0.2.7
2023-11-05 20:16:20,500:INFO:              optuna: 3.4.0
2023-11-05 20:16:20,500:INFO:               skopt: Not installed
2023-11-05 20:16:20,500:INFO:              mlflow: Not installed
2023-11-05 20:16:20,500:INFO:              gradio: Not installed
2023-11-05 20:16:20,500:INFO:             fastapi: Not installed
2023-11-05 20:16:20,500:INFO:             uvicorn: Not installed
2023-11-05 20:16:20,500:INFO:              m2cgen: Not installed
2023-11-05 20:16:20,500:INFO:           evidently: Not installed
2023-11-05 20:16:20,500:INFO:               fugue: Not installed
2023-11-05 20:16:20,500:INFO:           streamlit: Not installed
2023-11-05 20:16:20,500:INFO:             prophet: Not installed
2023-11-05 20:16:20,500:INFO:None
2023-11-05 20:16:20,500:INFO:Set up data.
2023-11-05 20:16:20,555:INFO:Set up folding strategy.
2023-11-05 20:16:20,571:INFO:Set up train/test split.
2023-11-05 20:16:20,614:INFO:Set up index.
2023-11-05 20:16:20,617:INFO:Assigning column types.
2023-11-05 20:16:20,639:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-05 20:16:20,640:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 20:16:20,651:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 20:16:20,659:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:16:20,783:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:20,858:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:16:20,860:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:20,864:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:20,865:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-05 20:16:20,872:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 20:16:20,880:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:16:20,977:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,049:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,050:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:21,054:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:21,055:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-05 20:16:21,062:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,070:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,180:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,250:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,251:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:21,258:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:21,270:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,281:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,386:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,452:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,453:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:21,457:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:21,457:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-05 20:16:21,471:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,568:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,637:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,637:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:21,641:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:21,658:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,753:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,818:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:16:21,820:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:21,824:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:21,825:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-05 20:16:21,969:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:22,036:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:16:22,037:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:22,041:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:22,153:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:22,227:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-05 20:16:22,228:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:22,232:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:22,233:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-05 20:16:22,349:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:22,431:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:22,437:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:22,567:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-05 20:16:22,649:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:22,653:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:22,654:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-05 20:16:22,822:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:22,826:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:23,021:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:23,025:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:23,026:INFO:Preparing preprocessing pipeline...
2023-11-05 20:16:23,026:INFO:Set up simple imputation.
2023-11-05 20:16:23,035:INFO:Set up encoding of ordinal features.
2023-11-05 20:16:23,055:INFO:Set up encoding of categorical features.
2023-11-05 20:16:23,059:INFO:Set up column name cleaning.
2023-11-05 20:16:23,422:INFO:Finished creating preprocessing pipeline.
2023-11-05 20:16:23,512:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-05 20:16:23,513:INFO:Creating final display dataframe.
2023-11-05 20:16:23,725:INFO:Setup _display_container:                     Description          Value
0                    Session id            123
1                        Target         target
2                   Target type     Regression
3           Original data shape    (29596, 57)
4        Transformed data shape    (29596, 62)
5   Transformed train set shape    (20717, 62)
6    Transformed test set shape     (8879, 62)
7              Ordinal features              2
8              Numeric features             52
9          Categorical features              4
10                   Preprocess           True
11              Imputation type         simple
12           Numeric imputation           mean
13       Categorical imputation           mode
14     Maximum one-hot encoding             25
15              Encoding method           None
16               Fold Generator          KFold
17                  Fold Number             10
18                     CPU Jobs             -1
19                      Use GPU          False
20               Log Experiment          False
21              Experiment Name  rf_experiment
22                          USI           b7ab
2023-11-05 20:16:24,103:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:24,113:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:24,749:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-05 20:16:24,759:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-05 20:16:24,762:INFO:setup() successfully completed in 4.27s...............
2023-11-05 20:16:24,762:INFO:Initializing create_model()
2023-11-05 20:16:24,763:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9aaeb11a30>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 20:16:24,763:INFO:Checking exceptions
2023-11-05 20:16:24,768:INFO:Importing libraries
2023-11-05 20:16:24,769:INFO:Copying training dataset
2023-11-05 20:16:24,834:INFO:Defining folds
2023-11-05 20:16:24,835:INFO:Declaring metric variables
2023-11-05 20:16:24,835:INFO:Importing untrained model
2023-11-05 20:16:24,837:INFO:Random Forest Regressor Imported successfully
2023-11-05 20:16:24,837:INFO:Starting cross validation
2023-11-05 20:16:24,850:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 20:19:19,848:INFO:Calculating mean and std
2023-11-05 20:19:19,888:INFO:Creating metrics dataframe
2023-11-05 20:19:19,901:INFO:Finalizing model
2023-11-05 20:19:41,526:INFO:Uploading results into container
2023-11-05 20:19:41,528:INFO:Uploading model into container now
2023-11-05 20:19:41,538:INFO:_master_model_container: 1
2023-11-05 20:19:41,539:INFO:_display_container: 2
2023-11-05 20:19:41,539:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-05 20:19:41,540:INFO:create_model() successfully completed......................................
2023-11-05 20:19:42,036:INFO:Initializing tune_model()
2023-11-05 20:19:42,037:INFO:tune_model(estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9aaeb11a30>)
2023-11-05 20:19:42,037:INFO:Checking exceptions
2023-11-05 20:19:42,057:INFO:Copying training dataset
2023-11-05 20:19:42,072:INFO:Checking base model
2023-11-05 20:19:42,072:INFO:Base model : Random Forest Regressor
2023-11-05 20:19:42,073:INFO:Declaring metric variables
2023-11-05 20:19:42,073:INFO:Defining Hyperparameters
2023-11-05 20:19:42,217:INFO:Tuning with n_jobs=-1
2023-11-05 20:19:42,217:INFO:Initializing RandomizedSearchCV
2023-11-05 22:02:13,210:INFO:best_params: {'actual_estimator__n_estimators': 200, 'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 5, 'actual_estimator__min_impurity_decrease': 0.0002, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 11, 'actual_estimator__criterion': 'squared_error', 'actual_estimator__bootstrap': False}
2023-11-05 22:02:13,225:INFO:Hyperparameter search completed
2023-11-05 22:02:13,226:INFO:SubProcess create_model() called ==================================
2023-11-05 22:02:13,232:INFO:Initializing create_model()
2023-11-05 22:02:13,232:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9aaeb11a30>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ab1211760>, model_only=True, return_train_score=False, kwargs={'n_estimators': 200, 'min_samples_split': 7, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0002, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'squared_error', 'bootstrap': False})
2023-11-05 22:02:13,233:INFO:Checking exceptions
2023-11-05 22:02:13,233:INFO:Importing libraries
2023-11-05 22:02:13,234:INFO:Copying training dataset
2023-11-05 22:02:13,304:INFO:Defining folds
2023-11-05 22:02:13,304:INFO:Declaring metric variables
2023-11-05 22:02:13,305:INFO:Importing untrained model
2023-11-05 22:02:13,306:INFO:Declaring custom model
2023-11-05 22:02:13,311:INFO:Random Forest Regressor Imported successfully
2023-11-05 22:02:13,312:INFO:Starting cross validation
2023-11-05 22:02:13,317:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 22:03:11,258:INFO:Calculating mean and std
2023-11-05 22:03:11,265:INFO:Creating metrics dataframe
2023-11-05 22:03:11,275:INFO:Finalizing model
2023-11-05 22:03:17,047:INFO:Uploading results into container
2023-11-05 22:03:17,049:INFO:Uploading model into container now
2023-11-05 22:03:17,051:INFO:_master_model_container: 2
2023-11-05 22:03:17,051:INFO:_display_container: 3
2023-11-05 22:03:17,052:INFO:RandomForestRegressor(bootstrap=False, max_depth=11, max_features='sqrt',
                      min_impurity_decrease=0.0002, min_samples_leaf=5,
                      min_samples_split=7, n_estimators=200, n_jobs=-1,
                      random_state=123)
2023-11-05 22:03:17,052:INFO:create_model() successfully completed......................................
2023-11-05 22:03:17,722:INFO:SubProcess create_model() end ==================================
2023-11-05 22:03:17,723:INFO:choose_better activated
2023-11-05 22:03:17,723:INFO:SubProcess create_model() called ==================================
2023-11-05 22:03:17,725:INFO:Initializing create_model()
2023-11-05 22:03:17,725:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9aaeb11a30>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-05 22:03:17,726:INFO:Checking exceptions
2023-11-05 22:03:17,730:INFO:Importing libraries
2023-11-05 22:03:17,731:INFO:Copying training dataset
2023-11-05 22:03:17,799:INFO:Defining folds
2023-11-05 22:03:17,800:INFO:Declaring metric variables
2023-11-05 22:03:17,801:INFO:Importing untrained model
2023-11-05 22:03:17,801:INFO:Declaring custom model
2023-11-05 22:03:17,803:INFO:Random Forest Regressor Imported successfully
2023-11-05 22:03:17,804:INFO:Starting cross validation
2023-11-05 22:03:17,810:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-05 22:06:22,373:INFO:Calculating mean and std
2023-11-05 22:06:22,376:INFO:Creating metrics dataframe
2023-11-05 22:06:22,383:INFO:Finalizing model
2023-11-05 22:06:46,563:INFO:Uploading results into container
2023-11-05 22:06:46,565:INFO:Uploading model into container now
2023-11-05 22:06:46,566:INFO:_master_model_container: 3
2023-11-05 22:06:46,566:INFO:_display_container: 4
2023-11-05 22:06:46,567:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-05 22:06:46,567:INFO:create_model() successfully completed......................................
2023-11-05 22:06:46,717:INFO:SubProcess create_model() end ==================================
2023-11-05 22:06:46,718:INFO:RandomForestRegressor(n_jobs=-1, random_state=123) result for MAE is 25.2153
2023-11-05 22:06:46,719:INFO:RandomForestRegressor(bootstrap=False, max_depth=11, max_features='sqrt',
                      min_impurity_decrease=0.0002, min_samples_leaf=5,
                      min_samples_split=7, n_estimators=200, n_jobs=-1,
                      random_state=123) result for MAE is 26.3171
2023-11-05 22:06:46,719:INFO:RandomForestRegressor(n_jobs=-1, random_state=123) is best model
2023-11-05 22:06:46,719:INFO:choose_better completed
2023-11-05 22:06:46,720:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-11-05 22:06:46,725:INFO:_master_model_container: 3
2023-11-05 22:06:46,726:INFO:_display_container: 3
2023-11-05 22:06:46,726:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-05 22:06:46,726:INFO:tune_model() successfully completed......................................
2023-11-05 22:06:46,858:INFO:Initializing ensemble_model()
2023-11-05 22:06:46,859:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9aaeb11a30>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), method=Boosting, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-05 22:06:46,859:INFO:Checking exceptions
2023-11-05 22:09:30,209:INFO:Importing libraries
2023-11-05 22:09:30,213:INFO:Copying training dataset
2023-11-05 22:09:30,213:INFO:Checking base model
2023-11-05 22:09:30,214:INFO:Base model : Random Forest Regressor
2023-11-05 22:09:30,215:INFO:Importing untrained ensembler
2023-11-05 22:09:30,215:INFO:Ensemble method set to Boosting
2023-11-05 22:09:30,216:INFO:SubProcess create_model() called ==================================
2023-11-05 22:09:30,218:INFO:Initializing create_model()
2023-11-05 22:09:30,218:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9aaeb11a30>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ac1b6a7c0>, model_only=True, return_train_score=False, kwargs={})
2023-11-05 22:09:30,218:INFO:Checking exceptions
2023-11-05 22:09:30,218:INFO:Importing libraries
2023-11-05 22:09:30,218:INFO:Copying training dataset
2023-11-05 22:09:30,267:INFO:Defining folds
2023-11-05 22:09:30,268:INFO:Declaring metric variables
2023-11-05 22:09:30,268:INFO:Importing untrained model
2023-11-05 22:09:30,269:INFO:Declaring custom model
2023-11-05 22:09:30,272:INFO:Random Forest Regressor Imported successfully
2023-11-05 22:09:30,273:INFO:Starting cross validation
2023-11-05 22:09:30,284:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 09:21:23,450:INFO:Calculating mean and std
2023-11-06 09:21:23,488:INFO:Creating metrics dataframe
2023-11-06 09:21:23,554:INFO:Finalizing model
2023-11-06 09:36:02,698:INFO:Uploading results into container
2023-11-06 09:36:02,707:INFO:Uploading model into container now
2023-11-06 09:36:02,713:INFO:_master_model_container: 4
2023-11-06 09:36:02,714:INFO:_display_container: 4
2023-11-06 09:36:02,717:INFO:AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-06 09:36:02,718:INFO:create_model() successfully completed......................................
2023-11-06 09:36:04,088:INFO:SubProcess create_model() end ==================================
2023-11-06 09:36:04,106:INFO:_master_model_container: 4
2023-11-06 09:36:04,106:INFO:_display_container: 4
2023-11-06 09:36:04,111:INFO:AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-06 09:36:04,111:INFO:ensemble_model() successfully completed......................................
2023-11-06 09:36:04,362:INFO:Initializing finalize_model()
2023-11-06 09:36:04,362:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9aaeb11a30>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-06 09:36:04,365:INFO:Finalizing AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-06 09:36:04,449:INFO:Initializing create_model()
2023-11-06 09:36:04,449:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9aaeb11a30>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-06 09:36:04,450:INFO:Checking exceptions
2023-11-06 09:36:04,454:INFO:Importing libraries
2023-11-06 09:36:04,454:INFO:Copying training dataset
2023-11-06 09:36:04,457:INFO:Defining folds
2023-11-06 09:36:04,458:INFO:Declaring metric variables
2023-11-06 09:36:04,459:INFO:Importing untrained model
2023-11-06 09:36:04,459:INFO:Declaring custom model
2023-11-06 09:36:04,461:INFO:Random Forest Regressor Imported successfully
2023-11-06 09:36:04,465:INFO:Cross validation set to False
2023-11-06 09:36:04,465:INFO:Fitting Model
2023-11-06 09:40:07,398:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))])
2023-11-06 09:40:07,401:INFO:create_model() successfully completed......................................
2023-11-06 09:40:08,047:INFO:_master_model_container: 4
2023-11-06 09:40:08,047:INFO:_display_container: 4
2023-11-06 09:40:08,142:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))])
2023-11-06 09:40:08,142:INFO:finalize_model() successfully completed......................................
2023-11-06 09:40:08,618:INFO:PyCaret RegressionExperiment
2023-11-06 09:40:08,618:INFO:Logging name: xgboost_experiment
2023-11-06 09:40:08,618:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-06 09:40:08,618:INFO:version 3.1.0
2023-11-06 09:40:08,618:INFO:Initializing setup()
2023-11-06 09:40:08,618:INFO:self.USI: 4486
2023-11-06 09:40:08,618:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-06 09:40:08,618:INFO:Checking environment
2023-11-06 09:40:08,619:INFO:python_version: 3.9.6
2023-11-06 09:40:08,619:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-06 09:40:08,619:INFO:machine: x86_64
2023-11-06 09:40:08,619:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-06 09:40:08,619:INFO:Memory: svmem(total=8589934592, available=2423447552, percent=71.8, used=4569047040, free=347869184, active=2048966656, inactive=2074374144, wired=2520080384)
2023-11-06 09:40:08,619:INFO:Physical Core: 4
2023-11-06 09:40:08,619:INFO:Logical Core: 8
2023-11-06 09:40:08,619:INFO:Checking libraries
2023-11-06 09:40:08,619:INFO:System:
2023-11-06 09:40:08,619:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-06 09:40:08,619:INFO:executable: /usr/local/bin/python3.9
2023-11-06 09:40:08,620:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-06 09:40:08,620:INFO:PyCaret required dependencies:
2023-11-06 09:40:08,620:INFO:                 pip: 23.3.1
2023-11-06 09:40:08,620:INFO:          setuptools: 56.0.0
2023-11-06 09:40:08,620:INFO:             pycaret: 3.1.0
2023-11-06 09:40:08,620:INFO:             IPython: 7.28.0
2023-11-06 09:40:08,620:INFO:          ipywidgets: 8.1.1
2023-11-06 09:40:08,620:INFO:                tqdm: 4.66.1
2023-11-06 09:40:08,620:INFO:               numpy: 1.23.5
2023-11-06 09:40:08,620:INFO:              pandas: 1.5.3
2023-11-06 09:40:08,620:INFO:              jinja2: 3.0.1
2023-11-06 09:40:08,620:INFO:               scipy: 1.10.1
2023-11-06 09:40:08,620:INFO:              joblib: 1.3.2
2023-11-06 09:40:08,621:INFO:             sklearn: 1.1.3
2023-11-06 09:40:08,621:INFO:                pyod: 1.1.1
2023-11-06 09:40:08,621:INFO:            imblearn: 0.11.0
2023-11-06 09:40:08,621:INFO:   category_encoders: 2.6.3
2023-11-06 09:40:08,621:INFO:            lightgbm: 4.1.0
2023-11-06 09:40:08,621:INFO:               numba: 0.58.1
2023-11-06 09:40:08,621:INFO:            requests: 2.31.0
2023-11-06 09:40:08,621:INFO:          matplotlib: 3.4.2
2023-11-06 09:40:08,621:INFO:          scikitplot: 0.3.7
2023-11-06 09:40:08,621:INFO:         yellowbrick: 1.5
2023-11-06 09:40:08,621:INFO:              plotly: 5.18.0
2023-11-06 09:40:08,621:INFO:    plotly-resampler: Not installed
2023-11-06 09:40:08,621:INFO:             kaleido: 0.2.1
2023-11-06 09:40:08,621:INFO:           schemdraw: 0.15
2023-11-06 09:40:08,621:INFO:         statsmodels: 0.14.0
2023-11-06 09:40:08,622:INFO:              sktime: 0.21.1
2023-11-06 09:40:08,622:INFO:               tbats: 1.1.3
2023-11-06 09:40:08,622:INFO:            pmdarima: 2.0.4
2023-11-06 09:40:08,622:INFO:              psutil: 5.9.6
2023-11-06 09:40:08,622:INFO:          markupsafe: 2.1.3
2023-11-06 09:40:08,622:INFO:             pickle5: Not installed
2023-11-06 09:40:08,622:INFO:         cloudpickle: 2.2.1
2023-11-06 09:40:08,622:INFO:         deprecation: 2.1.0
2023-11-06 09:40:08,622:INFO:              xxhash: 3.4.1
2023-11-06 09:40:08,622:INFO:           wurlitzer: 3.0.3
2023-11-06 09:40:08,622:INFO:PyCaret optional dependencies:
2023-11-06 09:40:08,622:INFO:                shap: Not installed
2023-11-06 09:40:08,622:INFO:           interpret: Not installed
2023-11-06 09:40:08,622:INFO:                umap: Not installed
2023-11-06 09:40:08,623:INFO:     ydata_profiling: Not installed
2023-11-06 09:40:08,623:INFO:  explainerdashboard: Not installed
2023-11-06 09:40:08,623:INFO:             autoviz: Not installed
2023-11-06 09:40:08,623:INFO:           fairlearn: Not installed
2023-11-06 09:40:08,623:INFO:          deepchecks: Not installed
2023-11-06 09:40:08,623:INFO:             xgboost: 2.0.0
2023-11-06 09:40:08,623:INFO:            catboost: Not installed
2023-11-06 09:40:08,623:INFO:              kmodes: Not installed
2023-11-06 09:40:08,623:INFO:             mlxtend: Not installed
2023-11-06 09:40:08,623:INFO:       statsforecast: Not installed
2023-11-06 09:40:08,623:INFO:        tune_sklearn: Not installed
2023-11-06 09:40:08,623:INFO:                 ray: Not installed
2023-11-06 09:40:08,623:INFO:            hyperopt: 0.2.7
2023-11-06 09:40:08,623:INFO:              optuna: 3.4.0
2023-11-06 09:40:08,623:INFO:               skopt: Not installed
2023-11-06 09:40:08,624:INFO:              mlflow: Not installed
2023-11-06 09:40:08,624:INFO:              gradio: Not installed
2023-11-06 09:40:08,624:INFO:             fastapi: Not installed
2023-11-06 09:40:08,624:INFO:             uvicorn: Not installed
2023-11-06 09:40:08,624:INFO:              m2cgen: Not installed
2023-11-06 09:40:08,624:INFO:           evidently: Not installed
2023-11-06 09:40:08,624:INFO:               fugue: Not installed
2023-11-06 09:40:08,624:INFO:           streamlit: Not installed
2023-11-06 09:40:08,624:INFO:             prophet: Not installed
2023-11-06 09:40:08,624:INFO:None
2023-11-06 09:40:08,624:INFO:Set up data.
2023-11-06 09:40:08,754:INFO:Set up folding strategy.
2023-11-06 09:40:08,754:INFO:Set up train/test split.
2023-11-06 09:40:08,799:INFO:Set up index.
2023-11-06 09:40:08,802:INFO:Assigning column types.
2023-11-06 09:40:08,820:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-06 09:40:08,821:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-06 09:40:08,828:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 09:40:08,835:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:40:08,939:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,015:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,015:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:09,019:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:09,019:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,027:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,034:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,149:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,225:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,226:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:09,231:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:09,232:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-06 09:40:09,240:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,248:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,355:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,426:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,427:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:09,431:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:09,440:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,449:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,560:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,646:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,647:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:09,651:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:09,652:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-06 09:40:09,667:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,778:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,859:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:40:09,860:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:09,865:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:09,884:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,001:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,085:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,086:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:10,091:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:10,092:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-06 09:40:10,231:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,315:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,316:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:10,321:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:10,449:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,526:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,527:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:10,530:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:10,531:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-06 09:40:10,662:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,741:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:10,747:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:10,901:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:40:10,971:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:10,975:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:10,976:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-06 09:40:11,174:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:11,178:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:11,386:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:11,390:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:11,394:INFO:Preparing preprocessing pipeline...
2023-11-06 09:40:11,394:INFO:Set up simple imputation.
2023-11-06 09:40:11,403:INFO:Set up encoding of ordinal features.
2023-11-06 09:40:11,420:INFO:Set up encoding of categorical features.
2023-11-06 09:40:11,425:INFO:Set up column name cleaning.
2023-11-06 09:40:11,798:INFO:Finished creating preprocessing pipeline.
2023-11-06 09:40:11,895:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-06 09:40:11,895:INFO:Creating final display dataframe.
2023-11-06 09:40:12,098:INFO:Setup _display_container:                     Description               Value
0                    Session id                 123
1                        Target              target
2                   Target type          Regression
3           Original data shape         (29596, 57)
4        Transformed data shape         (29596, 62)
5   Transformed train set shape         (20717, 62)
6    Transformed test set shape          (8879, 62)
7              Ordinal features                   2
8              Numeric features                  52
9          Categorical features                   4
10                   Preprocess                True
11              Imputation type              simple
12           Numeric imputation                mean
13       Categorical imputation                mode
14     Maximum one-hot encoding                  25
15              Encoding method                None
16               Fold Generator               KFold
17                  Fold Number                  10
18                     CPU Jobs                  -1
19                      Use GPU               False
20               Log Experiment               False
21              Experiment Name  xgboost_experiment
22                          USI                4486
2023-11-06 09:40:12,311:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:12,317:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:12,547:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:40:12,551:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:40:12,552:INFO:setup() successfully completed in 3.95s...............
2023-11-06 09:40:12,552:INFO:Initializing create_model()
2023-11-06 09:40:12,552:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-06 09:40:12,552:INFO:Checking exceptions
2023-11-06 09:40:12,556:INFO:Importing libraries
2023-11-06 09:40:12,556:INFO:Copying training dataset
2023-11-06 09:40:12,584:INFO:Defining folds
2023-11-06 09:40:12,584:INFO:Declaring metric variables
2023-11-06 09:40:12,585:INFO:Importing untrained model
2023-11-06 09:40:12,588:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 09:40:12,589:INFO:Starting cross validation
2023-11-06 09:40:12,593:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 09:40:45,088:INFO:Calculating mean and std
2023-11-06 09:40:45,133:INFO:Creating metrics dataframe
2023-11-06 09:40:45,148:INFO:Finalizing model
2023-11-06 09:40:47,375:INFO:Uploading results into container
2023-11-06 09:40:47,378:INFO:Uploading model into container now
2023-11-06 09:40:47,405:INFO:_master_model_container: 1
2023-11-06 09:40:47,406:INFO:_display_container: 2
2023-11-06 09:40:47,409:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-06 09:40:47,409:INFO:create_model() successfully completed......................................
2023-11-06 09:40:48,061:INFO:Initializing tune_model()
2023-11-06 09:40:48,061:INFO:tune_model(estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>)
2023-11-06 09:40:48,061:INFO:Checking exceptions
2023-11-06 09:40:48,084:INFO:Copying training dataset
2023-11-06 09:40:48,107:INFO:Checking base model
2023-11-06 09:40:48,107:INFO:Base model : Extreme Gradient Boosting
2023-11-06 09:40:48,109:INFO:Declaring metric variables
2023-11-06 09:40:48,109:INFO:Defining Hyperparameters
2023-11-06 09:40:48,280:INFO:Tuning with n_jobs=-1
2023-11-06 09:40:48,280:INFO:Initializing RandomizedSearchCV
2023-11-06 09:43:41,187:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__scale_pos_weight': 37.1, 'actual_estimator__reg_lambda': 0.7, 'actual_estimator__reg_alpha': 2, 'actual_estimator__n_estimators': 290, 'actual_estimator__min_child_weight': 3, 'actual_estimator__max_depth': 7, 'actual_estimator__learning_rate': 0.15, 'actual_estimator__colsample_bytree': 0.9}
2023-11-06 09:43:41,197:INFO:Hyperparameter search completed
2023-11-06 09:43:41,197:INFO:SubProcess create_model() called ==================================
2023-11-06 09:43:41,200:INFO:Initializing create_model()
2023-11-06 09:43:41,200:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9abce475e0>, model_only=True, return_train_score=False, kwargs={'subsample': 0.7, 'scale_pos_weight': 37.1, 'reg_lambda': 0.7, 'reg_alpha': 2, 'n_estimators': 290, 'min_child_weight': 3, 'max_depth': 7, 'learning_rate': 0.15, 'colsample_bytree': 0.9})
2023-11-06 09:43:41,200:INFO:Checking exceptions
2023-11-06 09:43:41,201:INFO:Importing libraries
2023-11-06 09:43:41,201:INFO:Copying training dataset
2023-11-06 09:43:41,252:INFO:Defining folds
2023-11-06 09:43:41,253:INFO:Declaring metric variables
2023-11-06 09:43:41,255:INFO:Importing untrained model
2023-11-06 09:43:41,255:INFO:Declaring custom model
2023-11-06 09:43:41,263:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 09:43:41,264:INFO:Starting cross validation
2023-11-06 09:43:41,270:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 09:44:08,928:INFO:Calculating mean and std
2023-11-06 09:44:08,931:INFO:Creating metrics dataframe
2023-11-06 09:44:08,943:INFO:Finalizing model
2023-11-06 09:44:13,475:INFO:Uploading results into container
2023-11-06 09:44:13,477:INFO:Uploading model into container now
2023-11-06 09:44:13,479:INFO:_master_model_container: 2
2023-11-06 09:44:13,479:INFO:_display_container: 3
2023-11-06 09:44:13,482:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-06 09:44:13,482:INFO:create_model() successfully completed......................................
2023-11-06 09:44:14,221:INFO:SubProcess create_model() end ==================================
2023-11-06 09:44:14,223:INFO:choose_better activated
2023-11-06 09:44:14,224:INFO:SubProcess create_model() called ==================================
2023-11-06 09:44:14,226:INFO:Initializing create_model()
2023-11-06 09:44:14,226:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-06 09:44:14,226:INFO:Checking exceptions
2023-11-06 09:44:14,228:INFO:Importing libraries
2023-11-06 09:44:14,228:INFO:Copying training dataset
2023-11-06 09:44:14,280:INFO:Defining folds
2023-11-06 09:44:14,280:INFO:Declaring metric variables
2023-11-06 09:44:14,282:INFO:Importing untrained model
2023-11-06 09:44:14,283:INFO:Declaring custom model
2023-11-06 09:44:14,289:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 09:44:14,290:INFO:Starting cross validation
2023-11-06 09:44:14,295:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 09:44:28,925:INFO:Calculating mean and std
2023-11-06 09:44:28,928:INFO:Creating metrics dataframe
2023-11-06 09:44:28,932:INFO:Finalizing model
2023-11-06 09:44:30,449:INFO:Uploading results into container
2023-11-06 09:44:30,450:INFO:Uploading model into container now
2023-11-06 09:44:30,451:INFO:_master_model_container: 3
2023-11-06 09:44:30,451:INFO:_display_container: 4
2023-11-06 09:44:30,454:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-06 09:44:30,454:INFO:create_model() successfully completed......................................
2023-11-06 09:44:30,620:INFO:SubProcess create_model() end ==================================
2023-11-06 09:44:30,623:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) result for MAE is 24.8346
2023-11-06 09:44:30,625:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) result for MAE is 23.7025
2023-11-06 09:44:30,627:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) is best model
2023-11-06 09:44:30,627:INFO:choose_better completed
2023-11-06 09:44:30,636:INFO:_master_model_container: 3
2023-11-06 09:44:30,636:INFO:_display_container: 3
2023-11-06 09:44:30,639:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-06 09:44:30,640:INFO:tune_model() successfully completed......................................
2023-11-06 09:44:30,758:INFO:Initializing ensemble_model()
2023-11-06 09:44:30,758:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), method=Bagging, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-06 09:44:30,758:INFO:Checking exceptions
2023-11-06 09:44:30,770:INFO:Importing libraries
2023-11-06 09:44:30,770:INFO:Copying training dataset
2023-11-06 09:44:30,770:INFO:Checking base model
2023-11-06 09:44:30,770:INFO:Base model : Extreme Gradient Boosting
2023-11-06 09:44:30,771:INFO:Importing untrained ensembler
2023-11-06 09:44:30,772:INFO:Ensemble method set to Bagging
2023-11-06 09:44:30,772:INFO:SubProcess create_model() called ==================================
2023-11-06 09:44:30,776:INFO:Initializing create_model()
2023-11-06 09:44:30,776:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ab10abca0>, model_only=True, return_train_score=False, kwargs={})
2023-11-06 09:44:30,776:INFO:Checking exceptions
2023-11-06 09:44:30,776:INFO:Importing libraries
2023-11-06 09:44:30,777:INFO:Copying training dataset
2023-11-06 09:44:30,799:INFO:Defining folds
2023-11-06 09:44:30,799:INFO:Declaring metric variables
2023-11-06 09:44:30,800:INFO:Importing untrained model
2023-11-06 09:44:30,800:INFO:Declaring custom model
2023-11-06 09:44:30,806:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 09:44:30,807:INFO:Starting cross validation
2023-11-06 09:44:30,812:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 09:48:29,808:INFO:Calculating mean and std
2023-11-06 09:48:29,823:INFO:Creating metrics dataframe
2023-11-06 09:48:29,842:INFO:Finalizing model
2023-11-06 09:49:10,203:INFO:Uploading results into container
2023-11-06 09:49:10,206:INFO:Uploading model into container now
2023-11-06 09:49:10,208:INFO:_master_model_container: 4
2023-11-06 09:49:10,209:INFO:_display_container: 4
2023-11-06 09:49:10,217:INFO:BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-06 09:49:10,218:INFO:create_model() successfully completed......................................
2023-11-06 09:49:10,895:INFO:SubProcess create_model() end ==================================
2023-11-06 09:49:10,900:INFO:_master_model_container: 4
2023-11-06 09:49:10,900:INFO:_display_container: 4
2023-11-06 09:49:10,905:INFO:BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-06 09:49:10,905:INFO:ensemble_model() successfully completed......................................
2023-11-06 09:49:11,030:INFO:Initializing finalize_model()
2023-11-06 09:49:11,030:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-06 09:49:11,033:INFO:Finalizing BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-06 09:49:11,062:INFO:Initializing create_model()
2023-11-06 09:49:11,063:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-06 09:49:11,063:INFO:Checking exceptions
2023-11-06 09:49:11,064:INFO:Importing libraries
2023-11-06 09:49:11,064:INFO:Copying training dataset
2023-11-06 09:49:11,068:INFO:Defining folds
2023-11-06 09:49:11,068:INFO:Declaring metric variables
2023-11-06 09:49:11,068:INFO:Importing untrained model
2023-11-06 09:49:11,068:INFO:Declaring custom model
2023-11-06 09:49:11,071:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 09:49:11,074:INFO:Cross validation set to False
2023-11-06 09:49:11,074:INFO:Fitting Model
2023-11-06 09:49:55,557:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))])
2023-11-06 09:49:55,559:INFO:create_model() successfully completed......................................
2023-11-06 09:49:55,713:INFO:_master_model_container: 4
2023-11-06 09:49:55,713:INFO:_display_container: 4
2023-11-06 09:49:55,800:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))])
2023-11-06 09:49:55,801:INFO:finalize_model() successfully completed......................................
2023-11-06 09:49:56,228:INFO:Initializing predict_model()
2023-11-06 09:49:56,228:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1,
                                                               random_state=123),
                                  random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9ac4e11dc0>)
2023-11-06 09:49:56,228:INFO:Checking exceptions
2023-11-06 09:49:56,228:INFO:Preloading libraries
2023-11-06 09:49:56,228:INFO:Set up data.
2023-11-06 09:49:56,259:INFO:Set up index.
2023-11-06 09:49:56,658:INFO:Initializing predict_model()
2023-11-06 09:49:56,658:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9ac4e11dc0>)
2023-11-06 09:49:56,658:INFO:Checking exceptions
2023-11-06 09:49:56,658:INFO:Preloading libraries
2023-11-06 09:49:56,659:INFO:Set up data.
2023-11-06 09:49:56,683:INFO:Set up index.
2023-11-06 09:49:59,077:INFO:Initializing predict_model()
2023-11-06 09:49:59,077:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab1311cd0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9ac4e11dc0>)
2023-11-06 09:49:59,077:INFO:Checking exceptions
2023-11-06 09:49:59,077:INFO:Preloading libraries
2023-11-06 09:49:59,078:INFO:Set up data.
2023-11-06 09:49:59,101:INFO:Set up index.
2023-11-06 09:49:59,458:INFO:PyCaret RegressionExperiment
2023-11-06 09:49:59,458:INFO:Logging name: lightgbm_experiment
2023-11-06 09:49:59,458:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-06 09:49:59,458:INFO:version 3.1.0
2023-11-06 09:49:59,458:INFO:Initializing setup()
2023-11-06 09:49:59,458:INFO:self.USI: 9cb0
2023-11-06 09:49:59,458:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-06 09:49:59,458:INFO:Checking environment
2023-11-06 09:49:59,458:INFO:python_version: 3.9.6
2023-11-06 09:49:59,458:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-06 09:49:59,458:INFO:machine: x86_64
2023-11-06 09:49:59,458:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-06 09:49:59,458:INFO:Memory: svmem(total=8589934592, available=2215153664, percent=74.2, used=4537626624, free=183472128, active=2034176000, inactive=2030014464, wired=2503450624)
2023-11-06 09:49:59,459:INFO:Physical Core: 4
2023-11-06 09:49:59,459:INFO:Logical Core: 8
2023-11-06 09:49:59,459:INFO:Checking libraries
2023-11-06 09:49:59,459:INFO:System:
2023-11-06 09:49:59,459:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-06 09:49:59,459:INFO:executable: /usr/local/bin/python3.9
2023-11-06 09:49:59,459:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-06 09:49:59,459:INFO:PyCaret required dependencies:
2023-11-06 09:49:59,459:INFO:                 pip: 23.3.1
2023-11-06 09:49:59,459:INFO:          setuptools: 56.0.0
2023-11-06 09:49:59,459:INFO:             pycaret: 3.1.0
2023-11-06 09:49:59,459:INFO:             IPython: 7.28.0
2023-11-06 09:49:59,459:INFO:          ipywidgets: 8.1.1
2023-11-06 09:49:59,459:INFO:                tqdm: 4.66.1
2023-11-06 09:49:59,459:INFO:               numpy: 1.23.5
2023-11-06 09:49:59,459:INFO:              pandas: 1.5.3
2023-11-06 09:49:59,459:INFO:              jinja2: 3.0.1
2023-11-06 09:49:59,460:INFO:               scipy: 1.10.1
2023-11-06 09:49:59,460:INFO:              joblib: 1.3.2
2023-11-06 09:49:59,460:INFO:             sklearn: 1.1.3
2023-11-06 09:49:59,460:INFO:                pyod: 1.1.1
2023-11-06 09:49:59,460:INFO:            imblearn: 0.11.0
2023-11-06 09:49:59,460:INFO:   category_encoders: 2.6.3
2023-11-06 09:49:59,460:INFO:            lightgbm: 4.1.0
2023-11-06 09:49:59,460:INFO:               numba: 0.58.1
2023-11-06 09:49:59,460:INFO:            requests: 2.31.0
2023-11-06 09:49:59,460:INFO:          matplotlib: 3.4.2
2023-11-06 09:49:59,460:INFO:          scikitplot: 0.3.7
2023-11-06 09:49:59,460:INFO:         yellowbrick: 1.5
2023-11-06 09:49:59,460:INFO:              plotly: 5.18.0
2023-11-06 09:49:59,460:INFO:    plotly-resampler: Not installed
2023-11-06 09:49:59,460:INFO:             kaleido: 0.2.1
2023-11-06 09:49:59,460:INFO:           schemdraw: 0.15
2023-11-06 09:49:59,460:INFO:         statsmodels: 0.14.0
2023-11-06 09:49:59,460:INFO:              sktime: 0.21.1
2023-11-06 09:49:59,460:INFO:               tbats: 1.1.3
2023-11-06 09:49:59,460:INFO:            pmdarima: 2.0.4
2023-11-06 09:49:59,461:INFO:              psutil: 5.9.6
2023-11-06 09:49:59,461:INFO:          markupsafe: 2.1.3
2023-11-06 09:49:59,461:INFO:             pickle5: Not installed
2023-11-06 09:49:59,461:INFO:         cloudpickle: 2.2.1
2023-11-06 09:49:59,461:INFO:         deprecation: 2.1.0
2023-11-06 09:49:59,461:INFO:              xxhash: 3.4.1
2023-11-06 09:49:59,461:INFO:           wurlitzer: 3.0.3
2023-11-06 09:49:59,461:INFO:PyCaret optional dependencies:
2023-11-06 09:49:59,461:INFO:                shap: Not installed
2023-11-06 09:49:59,461:INFO:           interpret: Not installed
2023-11-06 09:49:59,461:INFO:                umap: Not installed
2023-11-06 09:49:59,461:INFO:     ydata_profiling: Not installed
2023-11-06 09:49:59,461:INFO:  explainerdashboard: Not installed
2023-11-06 09:49:59,461:INFO:             autoviz: Not installed
2023-11-06 09:49:59,461:INFO:           fairlearn: Not installed
2023-11-06 09:49:59,461:INFO:          deepchecks: Not installed
2023-11-06 09:49:59,461:INFO:             xgboost: 2.0.0
2023-11-06 09:49:59,461:INFO:            catboost: Not installed
2023-11-06 09:49:59,462:INFO:              kmodes: Not installed
2023-11-06 09:49:59,462:INFO:             mlxtend: Not installed
2023-11-06 09:49:59,462:INFO:       statsforecast: Not installed
2023-11-06 09:49:59,462:INFO:        tune_sklearn: Not installed
2023-11-06 09:49:59,462:INFO:                 ray: Not installed
2023-11-06 09:49:59,462:INFO:            hyperopt: 0.2.7
2023-11-06 09:49:59,462:INFO:              optuna: 3.4.0
2023-11-06 09:49:59,462:INFO:               skopt: Not installed
2023-11-06 09:49:59,462:INFO:              mlflow: Not installed
2023-11-06 09:49:59,462:INFO:              gradio: Not installed
2023-11-06 09:49:59,462:INFO:             fastapi: Not installed
2023-11-06 09:49:59,462:INFO:             uvicorn: Not installed
2023-11-06 09:49:59,462:INFO:              m2cgen: Not installed
2023-11-06 09:49:59,462:INFO:           evidently: Not installed
2023-11-06 09:49:59,462:INFO:               fugue: Not installed
2023-11-06 09:49:59,462:INFO:           streamlit: Not installed
2023-11-06 09:49:59,462:INFO:             prophet: Not installed
2023-11-06 09:49:59,462:INFO:None
2023-11-06 09:49:59,462:INFO:Set up data.
2023-11-06 09:49:59,580:INFO:Set up folding strategy.
2023-11-06 09:49:59,581:INFO:Set up train/test split.
2023-11-06 09:49:59,612:INFO:Set up index.
2023-11-06 09:49:59,614:INFO:Assigning column types.
2023-11-06 09:49:59,629:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-06 09:49:59,629:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-06 09:49:59,635:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 09:49:59,642:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:49:59,741:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:49:59,820:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:49:59,821:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:49:59,825:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:49:59,825:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-06 09:49:59,833:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 09:49:59,841:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:49:59,957:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,039:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,069:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:00,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:00,074:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-06 09:50:00,082:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,091:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,205:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,292:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,293:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:00,299:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:00,311:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,320:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,451:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,526:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,527:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:00,531:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:00,532:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-06 09:50:00,548:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,676:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,751:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,751:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:00,755:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:00,771:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,875:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,957:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:50:00,959:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:00,966:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:00,966:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-06 09:50:01,121:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:01,201:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:50:01,203:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:01,210:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:01,334:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:01,401:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 09:50:01,403:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:01,407:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:01,407:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-06 09:50:01,524:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:01,597:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:01,601:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:01,741:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 09:50:01,828:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:01,832:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:01,832:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-06 09:50:02,030:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:02,034:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:02,244:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:02,248:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:02,250:INFO:Preparing preprocessing pipeline...
2023-11-06 09:50:02,251:INFO:Set up simple imputation.
2023-11-06 09:50:02,260:INFO:Set up encoding of ordinal features.
2023-11-06 09:50:02,279:INFO:Set up encoding of categorical features.
2023-11-06 09:50:02,282:INFO:Set up column name cleaning.
2023-11-06 09:50:03,182:INFO:Finished creating preprocessing pipeline.
2023-11-06 09:50:03,263:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-06 09:50:03,263:INFO:Creating final display dataframe.
2023-11-06 09:50:04,160:INFO:Setup _display_container:                     Description                Value
0                    Session id                  123
1                        Target               target
2                   Target type           Regression
3           Original data shape          (26028, 57)
4        Transformed data shape          (26028, 63)
5   Transformed train set shape          (18219, 63)
6    Transformed test set shape           (7809, 63)
7              Ordinal features                    2
8              Numeric features                   52
9          Categorical features                    4
10                   Preprocess                 True
11              Imputation type               simple
12           Numeric imputation                 mean
13       Categorical imputation                 mode
14     Maximum one-hot encoding                   25
15              Encoding method                 None
16               Fold Generator                KFold
17                  Fold Number                   10
18                     CPU Jobs                   -1
19                      Use GPU                False
20               Log Experiment                False
21              Experiment Name  lightgbm_experiment
22                          USI                 9cb0
2023-11-06 09:50:04,886:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:04,893:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:05,417:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 09:50:05,433:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 09:50:05,435:INFO:setup() successfully completed in 5.98s...............
2023-11-06 09:50:05,436:INFO:Initializing create_model()
2023-11-06 09:50:05,436:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab0261970>, estimator=lightgbm, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-06 09:50:05,436:INFO:Checking exceptions
2023-11-06 09:50:05,446:INFO:Importing libraries
2023-11-06 09:50:05,446:INFO:Copying training dataset
2023-11-06 09:50:05,504:INFO:Defining folds
2023-11-06 09:50:05,505:INFO:Declaring metric variables
2023-11-06 09:50:05,507:INFO:Importing untrained model
2023-11-06 09:50:05,509:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-06 09:50:05,514:INFO:Starting cross validation
2023-11-06 09:50:05,524:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 09:50:39,430:INFO:Calculating mean and std
2023-11-06 09:50:39,434:INFO:Creating metrics dataframe
2023-11-06 09:50:39,439:INFO:Finalizing model
2023-11-06 09:50:40,070:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013700 seconds.
2023-11-06 09:50:40,070:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 09:50:40,072:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 09:50:40,076:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 09:50:40,078:INFO:[LightGBM] [Info] Start training from score 77.068662
2023-11-06 09:50:40,666:INFO:Uploading results into container
2023-11-06 09:50:40,668:INFO:Uploading model into container now
2023-11-06 09:50:40,684:INFO:_master_model_container: 1
2023-11-06 09:50:40,684:INFO:_display_container: 2
2023-11-06 09:50:40,685:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-06 09:50:40,686:INFO:create_model() successfully completed......................................
2023-11-06 09:50:40,914:INFO:Initializing tune_model()
2023-11-06 09:50:40,914:INFO:tune_model(estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab0261970>)
2023-11-06 09:50:40,914:INFO:Checking exceptions
2023-11-06 09:50:40,929:INFO:Copying training dataset
2023-11-06 09:50:40,946:INFO:Checking base model
2023-11-06 09:50:40,947:INFO:Base model : Light Gradient Boosting Machine
2023-11-06 09:50:40,947:INFO:Declaring metric variables
2023-11-06 09:50:40,948:INFO:Defining Hyperparameters
2023-11-06 09:50:41,172:INFO:Tuning with n_jobs=-1
2023-11-06 09:50:41,172:INFO:Initializing RandomizedSearchCV
2023-11-06 09:57:42,624:INFO:best_params: {'actual_estimator__reg_lambda': 3, 'actual_estimator__reg_alpha': 2, 'actual_estimator__num_leaves': 70, 'actual_estimator__n_estimators': 260, 'actual_estimator__min_split_gain': 0.9, 'actual_estimator__min_child_samples': 41, 'actual_estimator__learning_rate': 0.1, 'actual_estimator__feature_fraction': 0.4, 'actual_estimator__bagging_freq': 2, 'actual_estimator__bagging_fraction': 0.6}
2023-11-06 09:57:42,635:INFO:Hyperparameter search completed
2023-11-06 09:57:42,636:INFO:SubProcess create_model() called ==================================
2023-11-06 09:57:42,639:INFO:Initializing create_model()
2023-11-06 09:57:42,640:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab0261970>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ab12c45b0>, model_only=True, return_train_score=False, kwargs={'reg_lambda': 3, 'reg_alpha': 2, 'num_leaves': 70, 'n_estimators': 260, 'min_split_gain': 0.9, 'min_child_samples': 41, 'learning_rate': 0.1, 'feature_fraction': 0.4, 'bagging_freq': 2, 'bagging_fraction': 0.6})
2023-11-06 09:57:42,640:INFO:Checking exceptions
2023-11-06 09:57:42,640:INFO:Importing libraries
2023-11-06 09:57:42,641:INFO:Copying training dataset
2023-11-06 09:57:42,713:INFO:Defining folds
2023-11-06 09:57:42,714:INFO:Declaring metric variables
2023-11-06 09:57:42,717:INFO:Importing untrained model
2023-11-06 09:57:42,717:INFO:Declaring custom model
2023-11-06 09:57:42,723:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-06 09:57:42,725:INFO:Starting cross validation
2023-11-06 09:57:42,733:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 09:59:15,190:INFO:Calculating mean and std
2023-11-06 09:59:15,193:INFO:Creating metrics dataframe
2023-11-06 09:59:15,201:INFO:Finalizing model
2023-11-06 09:59:15,724:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-06 09:59:15,725:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-06 09:59:15,725:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-06 09:59:15,801:INFO:[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2
2023-11-06 09:59:15,802:INFO:[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4
2023-11-06 09:59:15,802:INFO:[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6
2023-11-06 09:59:15,821:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009997 seconds.
2023-11-06 09:59:15,822:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 09:59:15,824:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 09:59:15,830:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 09:59:15,833:INFO:[LightGBM] [Info] Start training from score 77.068662
2023-11-06 09:59:15,838:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-06 09:59:15,843:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-06 09:59:15,847:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-06 09:59:15,853:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-06 09:59:15,857:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-06 09:59:15,867:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-06 09:59:15,870:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-06 09:59:15,893:INFO:[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
2023-11-06 09:59:18,276:INFO:Uploading results into container
2023-11-06 09:59:18,278:INFO:Uploading model into container now
2023-11-06 09:59:18,281:INFO:_master_model_container: 2
2023-11-06 09:59:18,281:INFO:_display_container: 3
2023-11-06 09:59:18,283:INFO:LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3)
2023-11-06 09:59:18,284:INFO:create_model() successfully completed......................................
2023-11-06 09:59:18,833:INFO:SubProcess create_model() end ==================================
2023-11-06 09:59:18,833:INFO:choose_better activated
2023-11-06 09:59:18,834:INFO:SubProcess create_model() called ==================================
2023-11-06 09:59:18,834:INFO:Initializing create_model()
2023-11-06 09:59:18,834:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab0261970>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-06 09:59:18,834:INFO:Checking exceptions
2023-11-06 09:59:18,836:INFO:Importing libraries
2023-11-06 09:59:18,836:INFO:Copying training dataset
2023-11-06 09:59:18,861:INFO:Defining folds
2023-11-06 09:59:18,861:INFO:Declaring metric variables
2023-11-06 09:59:18,861:INFO:Importing untrained model
2023-11-06 09:59:18,861:INFO:Declaring custom model
2023-11-06 09:59:18,863:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-06 09:59:18,863:INFO:Starting cross validation
2023-11-06 09:59:18,866:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 09:59:43,321:INFO:Calculating mean and std
2023-11-06 09:59:43,323:INFO:Creating metrics dataframe
2023-11-06 09:59:43,327:INFO:Finalizing model
2023-11-06 09:59:43,877:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009863 seconds.
2023-11-06 09:59:43,877:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 09:59:43,878:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 09:59:43,880:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 09:59:43,881:INFO:[LightGBM] [Info] Start training from score 77.068662
2023-11-06 09:59:44,298:INFO:Uploading results into container
2023-11-06 09:59:44,300:INFO:Uploading model into container now
2023-11-06 09:59:44,300:INFO:_master_model_container: 3
2023-11-06 09:59:44,300:INFO:_display_container: 4
2023-11-06 09:59:44,301:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-06 09:59:44,301:INFO:create_model() successfully completed......................................
2023-11-06 09:59:44,461:INFO:SubProcess create_model() end ==================================
2023-11-06 09:59:44,462:INFO:LGBMRegressor(n_jobs=-1, random_state=123) result for MAE is 19.7075
2023-11-06 09:59:44,464:INFO:LGBMRegressor(bagging_fraction=0.6, bagging_freq=2, feature_fraction=0.4,
              min_child_samples=41, min_split_gain=0.9, n_estimators=260,
              n_jobs=-1, num_leaves=70, random_state=123, reg_alpha=2,
              reg_lambda=3) result for MAE is 20.2283
2023-11-06 09:59:44,465:INFO:LGBMRegressor(n_jobs=-1, random_state=123) is best model
2023-11-06 09:59:44,465:INFO:choose_better completed
2023-11-06 09:59:44,465:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-11-06 09:59:44,474:INFO:_master_model_container: 3
2023-11-06 09:59:44,474:INFO:_display_container: 3
2023-11-06 09:59:44,475:INFO:LGBMRegressor(n_jobs=-1, random_state=123)
2023-11-06 09:59:44,475:INFO:tune_model() successfully completed......................................
2023-11-06 09:59:44,596:INFO:Initializing ensemble_model()
2023-11-06 09:59:44,597:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab0261970>, estimator=LGBMRegressor(n_jobs=-1, random_state=123), method=Bagging, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-06 09:59:44,597:INFO:Checking exceptions
2023-11-06 09:59:44,609:INFO:Importing libraries
2023-11-06 09:59:44,609:INFO:Copying training dataset
2023-11-06 09:59:44,609:INFO:Checking base model
2023-11-06 09:59:44,609:INFO:Base model : Light Gradient Boosting Machine
2023-11-06 09:59:44,610:INFO:Importing untrained ensembler
2023-11-06 09:59:44,610:INFO:Ensemble method set to Bagging
2023-11-06 09:59:44,610:INFO:SubProcess create_model() called ==================================
2023-11-06 09:59:44,612:INFO:Initializing create_model()
2023-11-06 09:59:44,612:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab0261970>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ab131a970>, model_only=True, return_train_score=False, kwargs={})
2023-11-06 09:59:44,613:INFO:Checking exceptions
2023-11-06 09:59:44,613:INFO:Importing libraries
2023-11-06 09:59:44,613:INFO:Copying training dataset
2023-11-06 09:59:44,635:INFO:Defining folds
2023-11-06 09:59:44,636:INFO:Declaring metric variables
2023-11-06 09:59:44,636:INFO:Importing untrained model
2023-11-06 09:59:44,636:INFO:Declaring custom model
2023-11-06 09:59:44,639:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-06 09:59:44,640:INFO:Starting cross validation
2023-11-06 09:59:44,650:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 10:02:46,102:INFO:Calculating mean and std
2023-11-06 10:02:46,106:INFO:Creating metrics dataframe
2023-11-06 10:02:46,111:INFO:Finalizing model
2023-11-06 10:02:46,737:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021565 seconds.
2023-11-06 10:02:46,737:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:46,739:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:46,742:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:46,744:INFO:[LightGBM] [Info] Start training from score 78.724775
2023-11-06 10:02:47,864:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011179 seconds.
2023-11-06 10:02:47,864:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:47,864:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:47,866:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:47,867:INFO:[LightGBM] [Info] Start training from score 77.324872
2023-11-06 10:02:48,566:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012833 seconds.
2023-11-06 10:02:48,566:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:48,566:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:48,568:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:48,570:INFO:[LightGBM] [Info] Start training from score 77.003495
2023-11-06 10:02:49,417:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017139 seconds.
2023-11-06 10:02:49,417:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:49,418:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:49,421:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:49,422:INFO:[LightGBM] [Info] Start training from score 78.079017
2023-11-06 10:02:56,179:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015284 seconds.
2023-11-06 10:02:56,181:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:56,182:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:56,184:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:56,185:INFO:[LightGBM] [Info] Start training from score 76.171256
2023-11-06 10:02:57,873:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013941 seconds.
2023-11-06 10:02:57,873:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:57,874:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:57,875:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:57,876:INFO:[LightGBM] [Info] Start training from score 75.679096
2023-11-06 10:02:58,383:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009721 seconds.
2023-11-06 10:02:58,384:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:58,384:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:58,385:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:58,387:INFO:[LightGBM] [Info] Start training from score 78.164507
2023-11-06 10:02:58,977:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016464 seconds.
2023-11-06 10:02:58,978:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:58,978:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:58,979:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:58,980:INFO:[LightGBM] [Info] Start training from score 77.332421
2023-11-06 10:02:59,519:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011643 seconds.
2023-11-06 10:02:59,519:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:02:59,519:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:02:59,521:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:02:59,522:INFO:[LightGBM] [Info] Start training from score 79.257135
2023-11-06 10:03:00,117:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010707 seconds.
2023-11-06 10:03:00,118:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:00,118:INFO:[LightGBM] [Info] Total Bins 10100
2023-11-06 10:03:00,120:INFO:[LightGBM] [Info] Number of data points in the train set: 18219, number of used features: 61
2023-11-06 10:03:00,121:INFO:[LightGBM] [Info] Start training from score 78.770416
2023-11-06 10:03:00,530:INFO:Uploading results into container
2023-11-06 10:03:00,532:INFO:Uploading model into container now
2023-11-06 10:03:00,535:INFO:_master_model_container: 4
2023-11-06 10:03:00,535:INFO:_display_container: 4
2023-11-06 10:03:00,537:INFO:BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123)
2023-11-06 10:03:00,538:INFO:create_model() successfully completed......................................
2023-11-06 10:03:00,708:INFO:SubProcess create_model() end ==================================
2023-11-06 10:03:00,717:INFO:_master_model_container: 4
2023-11-06 10:03:00,717:INFO:_display_container: 4
2023-11-06 10:03:00,719:INFO:BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123)
2023-11-06 10:03:00,719:INFO:ensemble_model() successfully completed......................................
2023-11-06 10:03:00,835:INFO:Initializing finalize_model()
2023-11-06 10:03:00,835:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab0261970>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-06 10:03:00,836:INFO:Finalizing BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123)
2023-11-06 10:03:00,855:INFO:Initializing create_model()
2023-11-06 10:03:00,855:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ab0261970>, estimator=BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1, random_state=123),
                 random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-06 10:03:00,855:INFO:Checking exceptions
2023-11-06 10:03:00,857:INFO:Importing libraries
2023-11-06 10:03:00,857:INFO:Copying training dataset
2023-11-06 10:03:00,858:INFO:Defining folds
2023-11-06 10:03:00,858:INFO:Declaring metric variables
2023-11-06 10:03:00,859:INFO:Importing untrained model
2023-11-06 10:03:00,859:INFO:Declaring custom model
2023-11-06 10:03:00,860:INFO:Light Gradient Boosting Machine Imported successfully
2023-11-06 10:03:00,862:INFO:Cross validation set to False
2023-11-06 10:03:00,862:INFO:Fitting Model
2023-11-06 10:03:01,709:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016187 seconds.
2023-11-06 10:03:01,709:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:01,710:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:01,711:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:01,712:INFO:[LightGBM] [Info] Start training from score 79.159229
2023-11-06 10:03:02,576:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013640 seconds.
2023-11-06 10:03:02,577:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:02,577:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:02,579:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:02,581:INFO:[LightGBM] [Info] Start training from score 78.029513
2023-11-06 10:03:03,373:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015450 seconds.
2023-11-06 10:03:03,374:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:03,374:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:03,376:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:03,378:INFO:[LightGBM] [Info] Start training from score 77.037803
2023-11-06 10:03:04,308:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019245 seconds.
2023-11-06 10:03:04,309:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:04,309:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:04,312:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:04,312:INFO:[LightGBM] [Info] Start training from score 77.993085
2023-11-06 10:03:04,955:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014504 seconds.
2023-11-06 10:03:04,956:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:04,956:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:04,957:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:04,958:INFO:[LightGBM] [Info] Start training from score 76.109424
2023-11-06 10:03:05,788:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.019236 seconds.
2023-11-06 10:03:05,788:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:05,789:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:05,791:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:05,792:INFO:[LightGBM] [Info] Start training from score 76.730050
2023-11-06 10:03:09,475:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022298 seconds.
2023-11-06 10:03:09,475:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:09,476:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:09,477:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:09,479:INFO:[LightGBM] [Info] Start training from score 78.330470
2023-11-06 10:03:10,906:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013204 seconds.
2023-11-06 10:03:10,906:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:10,907:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:10,908:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:10,909:INFO:[LightGBM] [Info] Start training from score 78.015394
2023-11-06 10:03:11,536:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014157 seconds.
2023-11-06 10:03:11,537:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:11,537:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:11,539:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:11,540:INFO:[LightGBM] [Info] Start training from score 79.326767
2023-11-06 10:03:12,152:INFO:[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012729 seconds.
2023-11-06 10:03:12,152:INFO:You can set `force_col_wise=true` to remove the overhead.
2023-11-06 10:03:12,153:INFO:[LightGBM] [Info] Total Bins 10345
2023-11-06 10:03:12,155:INFO:[LightGBM] [Info] Number of data points in the train set: 26028, number of used features: 61
2023-11-06 10:03:12,156:INFO:[LightGBM] [Info] Start training from score 80.338978
2023-11-06 10:03:12,775:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1,
                                                               random_state=123),
                                  random_state=123))])
2023-11-06 10:03:12,775:INFO:create_model() successfully completed......................................
2023-11-06 10:03:12,916:INFO:_master_model_container: 4
2023-11-06 10:03:12,916:INFO:_display_container: 4
2023-11-06 10:03:12,986:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1,
                                                               random_state=123),
                                  random_state=123))])
2023-11-06 10:03:12,986:INFO:finalize_model() successfully completed......................................
2023-11-06 10:03:13,139:INFO:PyCaret RegressionExperiment
2023-11-06 10:03:13,139:INFO:Logging name: rf_experiment
2023-11-06 10:03:13,139:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-06 10:03:13,139:INFO:version 3.1.0
2023-11-06 10:03:13,139:INFO:Initializing setup()
2023-11-06 10:03:13,139:INFO:self.USI: dad7
2023-11-06 10:03:13,139:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-06 10:03:13,139:INFO:Checking environment
2023-11-06 10:03:13,139:INFO:python_version: 3.9.6
2023-11-06 10:03:13,139:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-06 10:03:13,139:INFO:machine: x86_64
2023-11-06 10:03:13,139:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-06 10:03:13,139:INFO:Memory: svmem(total=8589934592, available=2150965248, percent=75.0, used=4457070592, free=133910528, active=2018496512, inactive=2013130752, wired=2438574080)
2023-11-06 10:03:13,139:INFO:Physical Core: 4
2023-11-06 10:03:13,139:INFO:Logical Core: 8
2023-11-06 10:03:13,139:INFO:Checking libraries
2023-11-06 10:03:13,139:INFO:System:
2023-11-06 10:03:13,139:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-06 10:03:13,139:INFO:executable: /usr/local/bin/python3.9
2023-11-06 10:03:13,140:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-06 10:03:13,140:INFO:PyCaret required dependencies:
2023-11-06 10:03:13,140:INFO:                 pip: 23.3.1
2023-11-06 10:03:13,140:INFO:          setuptools: 56.0.0
2023-11-06 10:03:13,140:INFO:             pycaret: 3.1.0
2023-11-06 10:03:13,140:INFO:             IPython: 7.28.0
2023-11-06 10:03:13,140:INFO:          ipywidgets: 8.1.1
2023-11-06 10:03:13,140:INFO:                tqdm: 4.66.1
2023-11-06 10:03:13,140:INFO:               numpy: 1.23.5
2023-11-06 10:03:13,140:INFO:              pandas: 1.5.3
2023-11-06 10:03:13,140:INFO:              jinja2: 3.0.1
2023-11-06 10:03:13,140:INFO:               scipy: 1.10.1
2023-11-06 10:03:13,140:INFO:              joblib: 1.3.2
2023-11-06 10:03:13,140:INFO:             sklearn: 1.1.3
2023-11-06 10:03:13,140:INFO:                pyod: 1.1.1
2023-11-06 10:03:13,140:INFO:            imblearn: 0.11.0
2023-11-06 10:03:13,140:INFO:   category_encoders: 2.6.3
2023-11-06 10:03:13,140:INFO:            lightgbm: 4.1.0
2023-11-06 10:03:13,140:INFO:               numba: 0.58.1
2023-11-06 10:03:13,140:INFO:            requests: 2.31.0
2023-11-06 10:03:13,140:INFO:          matplotlib: 3.4.2
2023-11-06 10:03:13,140:INFO:          scikitplot: 0.3.7
2023-11-06 10:03:13,140:INFO:         yellowbrick: 1.5
2023-11-06 10:03:13,140:INFO:              plotly: 5.18.0
2023-11-06 10:03:13,140:INFO:    plotly-resampler: Not installed
2023-11-06 10:03:13,140:INFO:             kaleido: 0.2.1
2023-11-06 10:03:13,141:INFO:           schemdraw: 0.15
2023-11-06 10:03:13,141:INFO:         statsmodels: 0.14.0
2023-11-06 10:03:13,141:INFO:              sktime: 0.21.1
2023-11-06 10:03:13,141:INFO:               tbats: 1.1.3
2023-11-06 10:03:13,141:INFO:            pmdarima: 2.0.4
2023-11-06 10:03:13,141:INFO:              psutil: 5.9.6
2023-11-06 10:03:13,141:INFO:          markupsafe: 2.1.3
2023-11-06 10:03:13,141:INFO:             pickle5: Not installed
2023-11-06 10:03:13,141:INFO:         cloudpickle: 2.2.1
2023-11-06 10:03:13,141:INFO:         deprecation: 2.1.0
2023-11-06 10:03:13,141:INFO:              xxhash: 3.4.1
2023-11-06 10:03:13,141:INFO:           wurlitzer: 3.0.3
2023-11-06 10:03:13,141:INFO:PyCaret optional dependencies:
2023-11-06 10:03:13,141:INFO:                shap: Not installed
2023-11-06 10:03:13,141:INFO:           interpret: Not installed
2023-11-06 10:03:13,141:INFO:                umap: Not installed
2023-11-06 10:03:13,141:INFO:     ydata_profiling: Not installed
2023-11-06 10:03:13,141:INFO:  explainerdashboard: Not installed
2023-11-06 10:03:13,141:INFO:             autoviz: Not installed
2023-11-06 10:03:13,141:INFO:           fairlearn: Not installed
2023-11-06 10:03:13,141:INFO:          deepchecks: Not installed
2023-11-06 10:03:13,141:INFO:             xgboost: 2.0.0
2023-11-06 10:03:13,141:INFO:            catboost: Not installed
2023-11-06 10:03:13,141:INFO:              kmodes: Not installed
2023-11-06 10:03:13,141:INFO:             mlxtend: Not installed
2023-11-06 10:03:13,141:INFO:       statsforecast: Not installed
2023-11-06 10:03:13,141:INFO:        tune_sklearn: Not installed
2023-11-06 10:03:13,142:INFO:                 ray: Not installed
2023-11-06 10:03:13,142:INFO:            hyperopt: 0.2.7
2023-11-06 10:03:13,142:INFO:              optuna: 3.4.0
2023-11-06 10:03:13,142:INFO:               skopt: Not installed
2023-11-06 10:03:13,142:INFO:              mlflow: Not installed
2023-11-06 10:03:13,142:INFO:              gradio: Not installed
2023-11-06 10:03:13,142:INFO:             fastapi: Not installed
2023-11-06 10:03:13,142:INFO:             uvicorn: Not installed
2023-11-06 10:03:13,142:INFO:              m2cgen: Not installed
2023-11-06 10:03:13,142:INFO:           evidently: Not installed
2023-11-06 10:03:13,142:INFO:               fugue: Not installed
2023-11-06 10:03:13,142:INFO:           streamlit: Not installed
2023-11-06 10:03:13,142:INFO:             prophet: Not installed
2023-11-06 10:03:13,142:INFO:None
2023-11-06 10:03:13,142:INFO:Set up data.
2023-11-06 10:03:13,192:INFO:Set up folding strategy.
2023-11-06 10:03:13,197:INFO:Set up train/test split.
2023-11-06 10:03:13,242:INFO:Set up index.
2023-11-06 10:03:13,245:INFO:Assigning column types.
2023-11-06 10:03:13,264:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-06 10:03:13,264:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,271:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,279:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,388:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,462:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,463:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:13,466:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:13,467:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,474:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,483:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,581:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,639:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,640:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:13,643:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:13,644:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-06 10:03:13,650:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,656:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,740:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,808:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,809:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:13,813:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:13,819:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,827:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 10:03:13,946:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,025:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,026:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:14,031:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:14,031:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-06 10:03:14,047:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,133:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,188:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,189:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:14,192:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:14,205:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,303:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,378:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,380:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:14,384:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:14,384:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-06 10:03:14,494:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,560:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,561:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:14,565:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:14,669:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,729:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,729:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:14,733:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:14,733:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-06 10:03:14,842:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:14,955:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:14,963:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:15,171:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 10:03:15,316:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:15,323:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:15,324:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-06 10:03:15,578:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:15,584:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:16,027:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:16,038:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:16,041:INFO:Preparing preprocessing pipeline...
2023-11-06 10:03:16,041:INFO:Set up simple imputation.
2023-11-06 10:03:16,057:INFO:Set up encoding of ordinal features.
2023-11-06 10:03:16,092:INFO:Set up encoding of categorical features.
2023-11-06 10:03:16,098:INFO:Set up column name cleaning.
2023-11-06 10:03:16,945:INFO:Finished creating preprocessing pipeline.
2023-11-06 10:03:17,114:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-06 10:03:17,115:INFO:Creating final display dataframe.
2023-11-06 10:03:17,517:INFO:Setup _display_container:                     Description          Value
0                    Session id            123
1                        Target         target
2                   Target type     Regression
3           Original data shape    (26028, 57)
4        Transformed data shape    (26028, 63)
5   Transformed train set shape    (18219, 63)
6    Transformed test set shape     (7809, 63)
7              Ordinal features              2
8              Numeric features             52
9          Categorical features              4
10                   Preprocess           True
11              Imputation type         simple
12           Numeric imputation           mean
13       Categorical imputation           mode
14     Maximum one-hot encoding             25
15              Encoding method           None
16               Fold Generator          KFold
17                  Fold Number             10
18                     CPU Jobs             -1
19                      Use GPU          False
20               Log Experiment          False
21              Experiment Name  rf_experiment
22                          USI           dad7
2023-11-06 10:03:17,828:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:17,833:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:18,048:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 10:03:18,052:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 10:03:18,053:INFO:setup() successfully completed in 4.92s...............
2023-11-06 10:03:18,053:INFO:Initializing create_model()
2023-11-06 10:03:18,053:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac78dbca0>, estimator=rf, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-06 10:03:18,053:INFO:Checking exceptions
2023-11-06 10:03:18,056:INFO:Importing libraries
2023-11-06 10:03:18,056:INFO:Copying training dataset
2023-11-06 10:03:18,081:INFO:Defining folds
2023-11-06 10:03:18,081:INFO:Declaring metric variables
2023-11-06 10:03:18,081:INFO:Importing untrained model
2023-11-06 10:03:18,082:INFO:Random Forest Regressor Imported successfully
2023-11-06 10:03:18,083:INFO:Starting cross validation
2023-11-06 10:03:18,087:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 10:05:27,241:INFO:Calculating mean and std
2023-11-06 10:05:27,249:INFO:Creating metrics dataframe
2023-11-06 10:05:27,252:INFO:Finalizing model
2023-11-06 10:05:49,728:INFO:Uploading results into container
2023-11-06 10:05:49,731:INFO:Uploading model into container now
2023-11-06 10:05:49,744:INFO:_master_model_container: 1
2023-11-06 10:05:49,745:INFO:_display_container: 2
2023-11-06 10:05:49,746:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-06 10:05:49,747:INFO:create_model() successfully completed......................................
2023-11-06 10:05:50,369:INFO:Initializing tune_model()
2023-11-06 10:05:50,370:INFO:tune_model(estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac78dbca0>)
2023-11-06 10:05:50,370:INFO:Checking exceptions
2023-11-06 10:05:50,502:INFO:Copying training dataset
2023-11-06 10:05:50,561:INFO:Checking base model
2023-11-06 10:05:50,562:INFO:Base model : Random Forest Regressor
2023-11-06 10:05:50,563:INFO:Declaring metric variables
2023-11-06 10:05:50,563:INFO:Defining Hyperparameters
2023-11-06 10:05:50,875:INFO:Tuning with n_jobs=-1
2023-11-06 10:05:50,875:INFO:Initializing RandomizedSearchCV
2023-11-06 11:11:09,785:INFO:best_params: {'actual_estimator__n_estimators': 200, 'actual_estimator__min_samples_split': 7, 'actual_estimator__min_samples_leaf': 5, 'actual_estimator__min_impurity_decrease': 0.0002, 'actual_estimator__max_features': 'sqrt', 'actual_estimator__max_depth': 11, 'actual_estimator__criterion': 'squared_error', 'actual_estimator__bootstrap': False}
2023-11-06 11:11:09,826:INFO:Hyperparameter search completed
2023-11-06 11:11:09,826:INFO:SubProcess create_model() called ==================================
2023-11-06 11:11:09,853:INFO:Initializing create_model()
2023-11-06 11:11:09,854:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac78dbca0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9abce66bb0>, model_only=True, return_train_score=False, kwargs={'n_estimators': 200, 'min_samples_split': 7, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0002, 'max_features': 'sqrt', 'max_depth': 11, 'criterion': 'squared_error', 'bootstrap': False})
2023-11-06 11:11:09,854:INFO:Checking exceptions
2023-11-06 11:11:09,871:INFO:Importing libraries
2023-11-06 11:11:09,873:INFO:Copying training dataset
2023-11-06 11:11:09,992:INFO:Defining folds
2023-11-06 11:11:09,995:INFO:Declaring metric variables
2023-11-06 11:11:09,997:INFO:Importing untrained model
2023-11-06 11:11:09,998:INFO:Declaring custom model
2023-11-06 11:11:10,010:INFO:Random Forest Regressor Imported successfully
2023-11-06 11:11:10,015:INFO:Starting cross validation
2023-11-06 11:11:10,041:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 11:11:50,881:INFO:Calculating mean and std
2023-11-06 11:11:50,890:INFO:Creating metrics dataframe
2023-11-06 11:11:50,950:INFO:Finalizing model
2023-11-06 11:11:55,777:INFO:Uploading results into container
2023-11-06 11:11:55,778:INFO:Uploading model into container now
2023-11-06 11:11:55,780:INFO:_master_model_container: 2
2023-11-06 11:11:55,781:INFO:_display_container: 3
2023-11-06 11:11:55,782:INFO:RandomForestRegressor(bootstrap=False, max_depth=11, max_features='sqrt',
                      min_impurity_decrease=0.0002, min_samples_leaf=5,
                      min_samples_split=7, n_estimators=200, n_jobs=-1,
                      random_state=123)
2023-11-06 11:11:55,783:INFO:create_model() successfully completed......................................
2023-11-06 11:11:56,703:INFO:SubProcess create_model() end ==================================
2023-11-06 11:11:56,704:INFO:choose_better activated
2023-11-06 11:11:56,705:INFO:SubProcess create_model() called ==================================
2023-11-06 11:11:56,706:INFO:Initializing create_model()
2023-11-06 11:11:56,706:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac78dbca0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-06 11:11:56,706:INFO:Checking exceptions
2023-11-06 11:11:56,707:INFO:Importing libraries
2023-11-06 11:11:56,707:INFO:Copying training dataset
2023-11-06 11:11:56,726:INFO:Defining folds
2023-11-06 11:11:56,727:INFO:Declaring metric variables
2023-11-06 11:11:56,727:INFO:Importing untrained model
2023-11-06 11:11:56,727:INFO:Declaring custom model
2023-11-06 11:11:56,727:INFO:Random Forest Regressor Imported successfully
2023-11-06 11:11:56,728:INFO:Starting cross validation
2023-11-06 11:11:56,729:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 11:13:50,621:INFO:Calculating mean and std
2023-11-06 11:13:50,627:INFO:Creating metrics dataframe
2023-11-06 11:13:50,642:INFO:Finalizing model
2023-11-06 11:14:02,693:INFO:Uploading results into container
2023-11-06 11:14:02,694:INFO:Uploading model into container now
2023-11-06 11:14:02,694:INFO:_master_model_container: 3
2023-11-06 11:14:02,694:INFO:_display_container: 4
2023-11-06 11:14:02,695:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-06 11:14:02,695:INFO:create_model() successfully completed......................................
2023-11-06 11:14:02,806:INFO:SubProcess create_model() end ==================================
2023-11-06 11:14:02,807:INFO:RandomForestRegressor(n_jobs=-1, random_state=123) result for MAE is 20.0294
2023-11-06 11:14:02,808:INFO:RandomForestRegressor(bootstrap=False, max_depth=11, max_features='sqrt',
                      min_impurity_decrease=0.0002, min_samples_leaf=5,
                      min_samples_split=7, n_estimators=200, n_jobs=-1,
                      random_state=123) result for MAE is 20.4538
2023-11-06 11:14:02,808:INFO:RandomForestRegressor(n_jobs=-1, random_state=123) is best model
2023-11-06 11:14:02,808:INFO:choose_better completed
2023-11-06 11:14:02,808:INFO:Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).
2023-11-06 11:14:02,818:INFO:_master_model_container: 3
2023-11-06 11:14:02,818:INFO:_display_container: 3
2023-11-06 11:14:02,819:INFO:RandomForestRegressor(n_jobs=-1, random_state=123)
2023-11-06 11:14:02,819:INFO:tune_model() successfully completed......................................
2023-11-06 11:14:02,934:INFO:Initializing ensemble_model()
2023-11-06 11:14:02,934:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac78dbca0>, estimator=RandomForestRegressor(n_jobs=-1, random_state=123), method=Boosting, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-06 11:14:02,934:INFO:Checking exceptions
2023-11-06 11:16:00,334:INFO:Importing libraries
2023-11-06 11:16:00,337:INFO:Copying training dataset
2023-11-06 11:16:00,338:INFO:Checking base model
2023-11-06 11:16:00,338:INFO:Base model : Random Forest Regressor
2023-11-06 11:16:00,338:INFO:Importing untrained ensembler
2023-11-06 11:16:00,338:INFO:Ensemble method set to Boosting
2023-11-06 11:16:00,339:INFO:SubProcess create_model() called ==================================
2023-11-06 11:16:00,341:INFO:Initializing create_model()
2023-11-06 11:16:00,341:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac78dbca0>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ab10b7e50>, model_only=True, return_train_score=False, kwargs={})
2023-11-06 11:16:00,341:INFO:Checking exceptions
2023-11-06 11:16:00,341:INFO:Importing libraries
2023-11-06 11:16:00,341:INFO:Copying training dataset
2023-11-06 11:16:00,360:INFO:Defining folds
2023-11-06 11:16:00,360:INFO:Declaring metric variables
2023-11-06 11:16:00,360:INFO:Importing untrained model
2023-11-06 11:16:00,360:INFO:Declaring custom model
2023-11-06 11:16:00,362:INFO:Random Forest Regressor Imported successfully
2023-11-06 11:16:00,362:INFO:Starting cross validation
2023-11-06 11:16:00,365:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 11:33:20,756:INFO:Calculating mean and std
2023-11-06 11:33:20,789:INFO:Creating metrics dataframe
2023-11-06 11:33:20,822:INFO:Finalizing model
2023-11-06 11:35:29,438:INFO:Uploading results into container
2023-11-06 11:35:29,445:INFO:Uploading model into container now
2023-11-06 11:35:29,449:INFO:_master_model_container: 4
2023-11-06 11:35:29,449:INFO:_display_container: 4
2023-11-06 11:35:29,453:INFO:AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-06 11:35:29,454:INFO:create_model() successfully completed......................................
2023-11-06 11:35:30,509:INFO:SubProcess create_model() end ==================================
2023-11-06 11:35:30,515:INFO:_master_model_container: 4
2023-11-06 11:35:30,516:INFO:_display_container: 4
2023-11-06 11:35:30,519:INFO:AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-06 11:35:30,519:INFO:ensemble_model() successfully completed......................................
2023-11-06 11:35:30,688:INFO:Initializing finalize_model()
2023-11-06 11:35:30,688:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac78dbca0>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-06 11:35:30,690:INFO:Finalizing AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123)
2023-11-06 11:35:30,736:INFO:Initializing create_model()
2023-11-06 11:35:30,737:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9ac78dbca0>, estimator=AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                       random_state=123),
                  n_estimators=10, random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-06 11:35:30,737:INFO:Checking exceptions
2023-11-06 11:35:30,739:INFO:Importing libraries
2023-11-06 11:35:30,739:INFO:Copying training dataset
2023-11-06 11:35:30,741:INFO:Defining folds
2023-11-06 11:35:30,742:INFO:Declaring metric variables
2023-11-06 11:35:30,742:INFO:Importing untrained model
2023-11-06 11:35:30,742:INFO:Declaring custom model
2023-11-06 11:35:30,743:INFO:Random Forest Regressor Imported successfully
2023-11-06 11:35:30,746:INFO:Cross validation set to False
2023-11-06 11:35:30,746:INFO:Fitting Model
2023-11-06 11:38:35,994:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))])
2023-11-06 11:38:35,997:INFO:create_model() successfully completed......................................
2023-11-06 11:38:36,637:INFO:_master_model_container: 4
2023-11-06 11:38:36,637:INFO:_display_container: 4
2023-11-06 11:38:36,723:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))])
2023-11-06 11:38:36,723:INFO:finalize_model() successfully completed......................................
2023-11-06 11:38:37,491:INFO:PyCaret RegressionExperiment
2023-11-06 11:38:37,491:INFO:Logging name: xgboost_experiment
2023-11-06 11:38:37,491:INFO:ML Usecase: MLUsecase.REGRESSION
2023-11-06 11:38:37,491:INFO:version 3.1.0
2023-11-06 11:38:37,491:INFO:Initializing setup()
2023-11-06 11:38:37,491:INFO:self.USI: c0db
2023-11-06 11:38:37,491:INFO:self._variable_keys: {'gpu_param', 'transform_target_param', 'html_param', 'n_jobs_param', 'X', 'log_plots_param', 'X_train', 'target_param', 'exp_id', 'idx', '_available_plots', 'memory', 'pipeline', 'logging_param', 'data', 'gpu_n_jobs_param', 'fold_shuffle_param', 'X_test', 'fold_generator', 'y_test', 'y_train', 'y', 'fold_groups_param', 'seed', 'exp_name_log', 'USI', '_ml_usecase'}
2023-11-06 11:38:37,492:INFO:Checking environment
2023-11-06 11:38:37,492:INFO:python_version: 3.9.6
2023-11-06 11:38:37,492:INFO:python_build: ('v3.9.6:db3ff76da1', 'Jun 28 2021 11:49:53')
2023-11-06 11:38:37,492:INFO:machine: x86_64
2023-11-06 11:38:37,492:INFO:platform: macOS-10.16-x86_64-i386-64bit
2023-11-06 11:38:37,492:INFO:Memory: svmem(total=8589934592, available=2777100288, percent=67.7, used=4137115648, free=909758464, active=1864261632, inactive=1866727424, wired=2272854016)
2023-11-06 11:38:37,492:INFO:Physical Core: 4
2023-11-06 11:38:37,492:INFO:Logical Core: 8
2023-11-06 11:38:37,492:INFO:Checking libraries
2023-11-06 11:38:37,493:INFO:System:
2023-11-06 11:38:37,493:INFO:    python: 3.9.6 (v3.9.6:db3ff76da1, Jun 28 2021, 11:49:53)  [Clang 6.0 (clang-600.0.57)]
2023-11-06 11:38:37,493:INFO:executable: /usr/local/bin/python3.9
2023-11-06 11:38:37,493:INFO:   machine: macOS-10.16-x86_64-i386-64bit
2023-11-06 11:38:37,493:INFO:PyCaret required dependencies:
2023-11-06 11:38:37,493:INFO:                 pip: 23.3.1
2023-11-06 11:38:37,493:INFO:          setuptools: 56.0.0
2023-11-06 11:38:37,493:INFO:             pycaret: 3.1.0
2023-11-06 11:38:37,493:INFO:             IPython: 7.28.0
2023-11-06 11:38:37,494:INFO:          ipywidgets: 8.1.1
2023-11-06 11:38:37,494:INFO:                tqdm: 4.66.1
2023-11-06 11:38:37,494:INFO:               numpy: 1.23.5
2023-11-06 11:38:37,494:INFO:              pandas: 1.5.3
2023-11-06 11:38:37,494:INFO:              jinja2: 3.0.1
2023-11-06 11:38:37,494:INFO:               scipy: 1.10.1
2023-11-06 11:38:37,494:INFO:              joblib: 1.3.2
2023-11-06 11:38:37,494:INFO:             sklearn: 1.1.3
2023-11-06 11:38:37,494:INFO:                pyod: 1.1.1
2023-11-06 11:38:37,494:INFO:            imblearn: 0.11.0
2023-11-06 11:38:37,494:INFO:   category_encoders: 2.6.3
2023-11-06 11:38:37,494:INFO:            lightgbm: 4.1.0
2023-11-06 11:38:37,494:INFO:               numba: 0.58.1
2023-11-06 11:38:37,495:INFO:            requests: 2.31.0
2023-11-06 11:38:37,495:INFO:          matplotlib: 3.4.2
2023-11-06 11:38:37,495:INFO:          scikitplot: 0.3.7
2023-11-06 11:38:37,495:INFO:         yellowbrick: 1.5
2023-11-06 11:38:37,495:INFO:              plotly: 5.18.0
2023-11-06 11:38:37,495:INFO:    plotly-resampler: Not installed
2023-11-06 11:38:37,495:INFO:             kaleido: 0.2.1
2023-11-06 11:38:37,495:INFO:           schemdraw: 0.15
2023-11-06 11:38:37,495:INFO:         statsmodels: 0.14.0
2023-11-06 11:38:37,495:INFO:              sktime: 0.21.1
2023-11-06 11:38:37,495:INFO:               tbats: 1.1.3
2023-11-06 11:38:37,495:INFO:            pmdarima: 2.0.4
2023-11-06 11:38:37,496:INFO:              psutil: 5.9.6
2023-11-06 11:38:37,496:INFO:          markupsafe: 2.1.3
2023-11-06 11:38:37,496:INFO:             pickle5: Not installed
2023-11-06 11:38:37,496:INFO:         cloudpickle: 2.2.1
2023-11-06 11:38:37,496:INFO:         deprecation: 2.1.0
2023-11-06 11:38:37,496:INFO:              xxhash: 3.4.1
2023-11-06 11:38:37,496:INFO:           wurlitzer: 3.0.3
2023-11-06 11:38:37,496:INFO:PyCaret optional dependencies:
2023-11-06 11:38:37,496:INFO:                shap: Not installed
2023-11-06 11:38:37,496:INFO:           interpret: Not installed
2023-11-06 11:38:37,496:INFO:                umap: Not installed
2023-11-06 11:38:37,497:INFO:     ydata_profiling: Not installed
2023-11-06 11:38:37,497:INFO:  explainerdashboard: Not installed
2023-11-06 11:38:37,497:INFO:             autoviz: Not installed
2023-11-06 11:38:37,497:INFO:           fairlearn: Not installed
2023-11-06 11:38:37,497:INFO:          deepchecks: Not installed
2023-11-06 11:38:37,497:INFO:             xgboost: 2.0.0
2023-11-06 11:38:37,497:INFO:            catboost: Not installed
2023-11-06 11:38:37,497:INFO:              kmodes: Not installed
2023-11-06 11:38:37,498:INFO:             mlxtend: Not installed
2023-11-06 11:38:37,498:INFO:       statsforecast: Not installed
2023-11-06 11:38:37,498:INFO:        tune_sklearn: Not installed
2023-11-06 11:38:37,498:INFO:                 ray: Not installed
2023-11-06 11:38:37,498:INFO:            hyperopt: 0.2.7
2023-11-06 11:38:37,498:INFO:              optuna: 3.4.0
2023-11-06 11:38:37,498:INFO:               skopt: Not installed
2023-11-06 11:38:37,498:INFO:              mlflow: Not installed
2023-11-06 11:38:37,498:INFO:              gradio: Not installed
2023-11-06 11:38:37,498:INFO:             fastapi: Not installed
2023-11-06 11:38:37,499:INFO:             uvicorn: Not installed
2023-11-06 11:38:37,499:INFO:              m2cgen: Not installed
2023-11-06 11:38:37,499:INFO:           evidently: Not installed
2023-11-06 11:38:37,499:INFO:               fugue: Not installed
2023-11-06 11:38:37,499:INFO:           streamlit: Not installed
2023-11-06 11:38:37,499:INFO:             prophet: Not installed
2023-11-06 11:38:37,499:INFO:None
2023-11-06 11:38:37,499:INFO:Set up data.
2023-11-06 11:38:37,610:INFO:Set up folding strategy.
2023-11-06 11:38:37,610:INFO:Set up train/test split.
2023-11-06 11:38:37,653:INFO:Set up index.
2023-11-06 11:38:37,656:INFO:Assigning column types.
2023-11-06 11:38:37,675:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2023-11-06 11:38:37,675:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,684:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,693:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,765:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,817:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,817:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:37,820:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:37,820:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,825:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,830:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,908:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,962:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,963:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:37,965:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:37,966:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2023-11-06 11:38:37,971:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 11:38:37,976:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,047:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,102:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,103:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:38,107:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:38,115:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,121:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,201:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,253:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,253:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:38,256:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:38,256:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2023-11-06 11:38:38,269:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,345:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,397:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,398:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:38,401:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:38,413:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,495:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,549:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,550:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:38,553:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:38,554:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2023-11-06 11:38:38,646:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,714:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,715:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:38,719:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:38,815:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,871:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2023-11-06 11:38:38,872:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:38,875:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:38,875:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2023-11-06 11:38:38,968:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:39,023:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:39,026:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:39,111:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2023-11-06 11:38:39,165:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:39,167:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:39,168:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2023-11-06 11:38:39,314:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:39,317:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:39,471:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:39,475:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:39,479:INFO:Preparing preprocessing pipeline...
2023-11-06 11:38:39,479:INFO:Set up simple imputation.
2023-11-06 11:38:39,486:INFO:Set up encoding of ordinal features.
2023-11-06 11:38:39,499:INFO:Set up encoding of categorical features.
2023-11-06 11:38:39,501:INFO:Set up column name cleaning.
2023-11-06 11:38:39,721:INFO:Finished creating preprocessing pipeline.
2023-11-06 11:38:39,779:INFO:Pipeline: Pipeline(memory=FastMemory(location=/var/folders/lk/92_1dnqx149btpf8768zcxmw0000gn/T/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             '...
                                                                         'data_type': dtype('float64'),
                                                                         'mapping': 0.0    0
1.0    1
NaN   -1
dtype: int64}]))),
                ('onehot_encoding',
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames()))])
2023-11-06 11:38:39,780:INFO:Creating final display dataframe.
2023-11-06 11:38:40,318:INFO:Setup _display_container:                     Description               Value
0                    Session id                 123
1                        Target              target
2                   Target type          Regression
3           Original data shape         (26028, 57)
4        Transformed data shape         (26028, 63)
5   Transformed train set shape         (18219, 63)
6    Transformed test set shape          (7809, 63)
7              Ordinal features                   2
8              Numeric features                  52
9          Categorical features                   4
10                   Preprocess                True
11              Imputation type              simple
12           Numeric imputation                mean
13       Categorical imputation                mode
14     Maximum one-hot encoding                  25
15              Encoding method                None
16               Fold Generator               KFold
17                  Fold Number                  10
18                     CPU Jobs                  -1
19                      Use GPU               False
20               Log Experiment               False
21              Experiment Name  xgboost_experiment
22                          USI                c0db
2023-11-06 11:38:40,460:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:40,463:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:40,598:INFO:Soft dependency imported: xgboost: 2.0.0
2023-11-06 11:38:40,602:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2023-11-06 11:38:40,603:INFO:setup() successfully completed in 3.13s...............
2023-11-06 11:38:40,603:INFO:Initializing create_model()
2023-11-06 11:38:40,603:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=xgboost, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs=None, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=True, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-06 11:38:40,603:INFO:Checking exceptions
2023-11-06 11:38:40,607:INFO:Importing libraries
2023-11-06 11:38:40,607:INFO:Copying training dataset
2023-11-06 11:38:40,626:INFO:Defining folds
2023-11-06 11:38:40,626:INFO:Declaring metric variables
2023-11-06 11:38:40,626:INFO:Importing untrained model
2023-11-06 11:38:40,627:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 11:38:40,628:INFO:Starting cross validation
2023-11-06 11:38:40,631:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 11:39:04,160:INFO:Calculating mean and std
2023-11-06 11:39:04,206:INFO:Creating metrics dataframe
2023-11-06 11:39:04,215:INFO:Finalizing model
2023-11-06 11:39:05,672:INFO:Uploading results into container
2023-11-06 11:39:05,673:INFO:Uploading model into container now
2023-11-06 11:39:05,683:INFO:_master_model_container: 1
2023-11-06 11:39:05,683:INFO:_display_container: 2
2023-11-06 11:39:05,685:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-06 11:39:05,686:INFO:create_model() successfully completed......................................
2023-11-06 11:39:05,910:INFO:Initializing tune_model()
2023-11-06 11:39:05,910:INFO:tune_model(estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=None, round=4, n_iter=10, custom_grid=None, optimize=MAE, custom_scorer=None, search_library=scikit-learn, search_algorithm=None, early_stopping=False, early_stopping_max_iters=10, choose_better=True, fit_kwargs=None, groups=None, return_tuner=False, verbose=True, tuner_verbose=True, return_train_score=False, kwargs={}, self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>)
2023-11-06 11:39:05,910:INFO:Checking exceptions
2023-11-06 11:39:05,926:INFO:Copying training dataset
2023-11-06 11:39:05,941:INFO:Checking base model
2023-11-06 11:39:05,941:INFO:Base model : Extreme Gradient Boosting
2023-11-06 11:39:05,942:INFO:Declaring metric variables
2023-11-06 11:39:05,942:INFO:Defining Hyperparameters
2023-11-06 11:39:06,053:INFO:Tuning with n_jobs=-1
2023-11-06 11:39:06,053:INFO:Initializing RandomizedSearchCV
2023-11-06 11:41:23,411:INFO:best_params: {'actual_estimator__subsample': 0.7, 'actual_estimator__scale_pos_weight': 37.1, 'actual_estimator__reg_lambda': 0.7, 'actual_estimator__reg_alpha': 2, 'actual_estimator__n_estimators': 290, 'actual_estimator__min_child_weight': 3, 'actual_estimator__max_depth': 7, 'actual_estimator__learning_rate': 0.15, 'actual_estimator__colsample_bytree': 0.9}
2023-11-06 11:41:23,419:INFO:Hyperparameter search completed
2023-11-06 11:41:23,420:INFO:SubProcess create_model() called ==================================
2023-11-06 11:41:23,425:INFO:Initializing create_model()
2023-11-06 11:41:23,425:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ab0261ee0>, model_only=True, return_train_score=False, kwargs={'subsample': 0.7, 'scale_pos_weight': 37.1, 'reg_lambda': 0.7, 'reg_alpha': 2, 'n_estimators': 290, 'min_child_weight': 3, 'max_depth': 7, 'learning_rate': 0.15, 'colsample_bytree': 0.9})
2023-11-06 11:41:23,425:INFO:Checking exceptions
2023-11-06 11:41:23,425:INFO:Importing libraries
2023-11-06 11:41:23,426:INFO:Copying training dataset
2023-11-06 11:41:23,475:INFO:Defining folds
2023-11-06 11:41:23,475:INFO:Declaring metric variables
2023-11-06 11:41:23,475:INFO:Importing untrained model
2023-11-06 11:41:23,476:INFO:Declaring custom model
2023-11-06 11:41:23,484:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 11:41:23,485:INFO:Starting cross validation
2023-11-06 11:41:23,489:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 11:41:52,189:INFO:Calculating mean and std
2023-11-06 11:41:52,201:INFO:Creating metrics dataframe
2023-11-06 11:41:52,217:INFO:Finalizing model
2023-11-06 11:41:56,247:INFO:Uploading results into container
2023-11-06 11:41:56,249:INFO:Uploading model into container now
2023-11-06 11:41:56,252:INFO:_master_model_container: 2
2023-11-06 11:41:56,253:INFO:_display_container: 3
2023-11-06 11:41:56,255:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-06 11:41:56,256:INFO:create_model() successfully completed......................................
2023-11-06 11:41:57,039:INFO:SubProcess create_model() end ==================================
2023-11-06 11:41:57,040:INFO:choose_better activated
2023-11-06 11:41:57,040:INFO:SubProcess create_model() called ==================================
2023-11-06 11:41:57,043:INFO:Initializing create_model()
2023-11-06 11:41:57,043:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2023-11-06 11:41:57,044:INFO:Checking exceptions
2023-11-06 11:41:57,047:INFO:Importing libraries
2023-11-06 11:41:57,048:INFO:Copying training dataset
2023-11-06 11:41:57,076:INFO:Defining folds
2023-11-06 11:41:57,078:INFO:Declaring metric variables
2023-11-06 11:41:57,079:INFO:Importing untrained model
2023-11-06 11:41:57,079:INFO:Declaring custom model
2023-11-06 11:41:57,082:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 11:41:57,083:INFO:Starting cross validation
2023-11-06 11:41:57,086:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 11:42:09,544:INFO:Calculating mean and std
2023-11-06 11:42:09,550:INFO:Creating metrics dataframe
2023-11-06 11:42:09,558:INFO:Finalizing model
2023-11-06 11:42:10,881:INFO:Uploading results into container
2023-11-06 11:42:10,882:INFO:Uploading model into container now
2023-11-06 11:42:10,883:INFO:_master_model_container: 3
2023-11-06 11:42:10,883:INFO:_display_container: 4
2023-11-06 11:42:10,885:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-06 11:42:10,885:INFO:create_model() successfully completed......................................
2023-11-06 11:42:11,043:INFO:SubProcess create_model() end ==================================
2023-11-06 11:42:11,046:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=None, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=None, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=None, max_leaves=None,
             min_child_weight=None, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=None, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) result for MAE is 20.2636
2023-11-06 11:42:11,047:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) result for MAE is 19.2035
2023-11-06 11:42:11,049:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...) is best model
2023-11-06 11:42:11,049:INFO:choose_better completed
2023-11-06 11:42:11,057:INFO:_master_model_container: 3
2023-11-06 11:42:11,058:INFO:_display_container: 3
2023-11-06 11:42:11,060:INFO:XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...)
2023-11-06 11:42:11,060:INFO:tune_model() successfully completed......................................
2023-11-06 11:42:11,176:INFO:Initializing ensemble_model()
2023-11-06 11:42:11,176:INFO:ensemble_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=XGBRegressor(base_score=None, booster='gbtree', callbacks=None,
             colsample_bylevel=None, colsample_bynode=None,
             colsample_bytree=0.9, device=None, early_stopping_rounds=None,
             enable_categorical=False, eval_metric=None, feature_types=None,
             gamma=None, grow_policy=None, importance_type=None,
             interaction_constraints=None, learning_rate=0.15, max_bin=None,
             max_cat_threshold=None, max_cat_to_onehot=None,
             max_delta_step=None, max_depth=7, max_leaves=None,
             min_child_weight=3, missing=nan, monotone_constraints=None,
             multi_strategy=None, n_estimators=290, n_jobs=-1,
             num_parallel_tree=None, random_state=123, ...), method=Bagging, fold=None, n_estimators=10, round=4, choose_better=False, optimize=R2, fit_kwargs=None, groups=None, probability_threshold=None, verbose=True, return_train_score=False)
2023-11-06 11:42:11,176:INFO:Checking exceptions
2023-11-06 11:42:11,193:INFO:Importing libraries
2023-11-06 11:42:11,193:INFO:Copying training dataset
2023-11-06 11:42:11,193:INFO:Checking base model
2023-11-06 11:42:11,194:INFO:Base model : Extreme Gradient Boosting
2023-11-06 11:42:11,194:INFO:Importing untrained ensembler
2023-11-06 11:42:11,194:INFO:Ensemble method set to Bagging
2023-11-06 11:42:11,194:INFO:SubProcess create_model() called ==================================
2023-11-06 11:42:11,197:INFO:Initializing create_model()
2023-11-06 11:42:11,198:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=True, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7f9ac95f1e20>, model_only=True, return_train_score=False, kwargs={})
2023-11-06 11:42:11,198:INFO:Checking exceptions
2023-11-06 11:42:11,198:INFO:Importing libraries
2023-11-06 11:42:11,198:INFO:Copying training dataset
2023-11-06 11:42:11,214:INFO:Defining folds
2023-11-06 11:42:11,214:INFO:Declaring metric variables
2023-11-06 11:42:11,214:INFO:Importing untrained model
2023-11-06 11:42:11,214:INFO:Declaring custom model
2023-11-06 11:42:11,216:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 11:42:11,217:INFO:Starting cross validation
2023-11-06 11:42:11,219:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2023-11-06 11:45:21,880:INFO:Calculating mean and std
2023-11-06 11:45:21,895:INFO:Creating metrics dataframe
2023-11-06 11:45:21,912:INFO:Finalizing model
2023-11-06 11:45:56,723:INFO:Uploading results into container
2023-11-06 11:45:56,727:INFO:Uploading model into container now
2023-11-06 11:45:56,729:INFO:_master_model_container: 4
2023-11-06 11:45:56,730:INFO:_display_container: 4
2023-11-06 11:45:56,737:INFO:BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-06 11:45:56,737:INFO:create_model() successfully completed......................................
2023-11-06 11:45:57,322:INFO:SubProcess create_model() end ==================================
2023-11-06 11:45:57,326:INFO:_master_model_container: 4
2023-11-06 11:45:57,326:INFO:_display_container: 4
2023-11-06 11:45:57,329:INFO:BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-06 11:45:57,330:INFO:ensemble_model() successfully completed......................................
2023-11-06 11:45:57,474:INFO:Initializing finalize_model()
2023-11-06 11:45:57,474:INFO:finalize_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2023-11-06 11:45:57,477:INFO:Finalizing BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123)
2023-11-06 11:45:57,501:INFO:Initializing create_model()
2023-11-06 11:45:57,501:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=BaggingRegressor(base_estimator=XGBRegressor(base_score=None, booster='gbtree',
                                             callbacks=None,
                                             colsample_bylevel=None,
                                             colsample_bynode=None,
                                             colsample_bytree=0.9, device=None,
                                             early_stopping_rounds=None,
                                             enable_categorical=False,
                                             eval_metric=None,
                                             feature_types=None, gamma=None,
                                             grow_policy=None,
                                             importance_type=None,
                                             interaction_constraints=None,
                                             learning_rate=0.15, max_bin=None,
                                             max_cat_threshold=None,
                                             max_cat_to_onehot=None,
                                             max_delta_step=None, max_depth=7,
                                             max_leaves=None,
                                             min_child_weight=3, missing=nan,
                                             monotone_constraints=None,
                                             multi_strategy=None,
                                             n_estimators=290, n_jobs=-1,
                                             num_parallel_tree=None,
                                             random_state=123, ...),
                 random_state=123), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, kwargs={})
2023-11-06 11:45:57,501:INFO:Checking exceptions
2023-11-06 11:45:57,502:INFO:Importing libraries
2023-11-06 11:45:57,503:INFO:Copying training dataset
2023-11-06 11:45:57,506:INFO:Defining folds
2023-11-06 11:45:57,506:INFO:Declaring metric variables
2023-11-06 11:45:57,506:INFO:Importing untrained model
2023-11-06 11:45:57,506:INFO:Declaring custom model
2023-11-06 11:45:57,509:INFO:Extreme Gradient Boosting Imported successfully
2023-11-06 11:45:57,511:INFO:Cross validation set to False
2023-11-06 11:45:57,511:INFO:Fitting Model
2023-11-06 11:46:38,865:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))])
2023-11-06 11:46:38,869:INFO:create_model() successfully completed......................................
2023-11-06 11:46:39,232:INFO:_master_model_container: 4
2023-11-06 11:46:39,232:INFO:_display_container: 4
2023-11-06 11:46:39,535:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))])
2023-11-06 11:46:39,535:INFO:finalize_model() successfully completed......................................
2023-11-06 11:46:40,223:INFO:Initializing predict_model()
2023-11-06 11:46:40,223:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                 TransformerWrapper(include=['dew_or_rime:idx',
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 BaggingRegressor(base_estimator=LGBMRegressor(n_jobs=-1,
                                                               random_state=123),
                                  random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9a5fda6430>)
2023-11-06 11:46:40,223:INFO:Checking exceptions
2023-11-06 11:46:40,224:INFO:Preloading libraries
2023-11-06 11:46:40,224:INFO:Set up data.
2023-11-06 11:46:40,272:INFO:Set up index.
2023-11-06 11:46:40,978:INFO:Initializing predict_model()
2023-11-06 11:46:40,978:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                             'precip_type_5min:idx'],
                                    transformer=OneHotEncoder(cols=['dew_or_rime:idx',
                                                                    'precip_type_5min:idx'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 AdaBoostRegressor(base_estimator=RandomForestRegressor(n_jobs=-1,
                                                                        random_state=123),
                                   n_estimators=10, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9a5fda6430>)
2023-11-06 11:46:40,979:INFO:Checking exceptions
2023-11-06 11:46:40,979:INFO:Preloading libraries
2023-11-06 11:46:40,979:INFO:Set up data.
2023-11-06 11:46:41,030:INFO:Set up index.
2023-11-06 11:46:43,114:INFO:Initializing predict_model()
2023-11-06 11:46:43,114:INFO:predict_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x7f9abce3e8b0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['absolute_humidity_2m:gm3',
                                             'air_density_2m:kgm3',
                                             'ceiling_height_agl:m',
                                             'clear_sky_energy_1h:J',
                                             'clear_sky_rad:W',
                                             'cloud_base_agl:m',
                                             'dew_point_2m:K', 'diffuse_rad:W',
                                             'diffuse_rad_1h:J', 'direct_rad:W',
                                             'direct_rad_1h:J',
                                             'effective_cloud_cover:p',
                                             'fresh_snow_...
                                                              grow_policy=None,
                                                              importance_type=None,
                                                              interaction_constraints=None,
                                                              learning_rate=0.15,
                                                              max_bin=None,
                                                              max_cat_threshold=None,
                                                              max_cat_to_onehot=None,
                                                              max_delta_step=None,
                                                              max_depth=7,
                                                              max_leaves=None,
                                                              min_child_weight=3,
                                                              missing=nan,
                                                              monotone_constraints=None,
                                                              multi_strategy=None,
                                                              n_estimators=290,
                                                              n_jobs=-1,
                                                              num_parallel_tree=None,
                                                              random_state=123, ...),
                                  random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7f9a5fda6430>)
2023-11-06 11:46:43,114:INFO:Checking exceptions
2023-11-06 11:46:43,114:INFO:Preloading libraries
2023-11-06 11:46:43,115:INFO:Set up data.
2023-11-06 11:46:43,164:INFO:Set up index.
