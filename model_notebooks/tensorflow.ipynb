{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')\n",
    "\n",
    "train_targets_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "train_targets_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "train_targets_c = pd.read_parquet('../data/C/train_targets.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "researching which columns have the most null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: ceiling_height_agl:m has 3919 NULL values\n",
      "Column: cloud_base_agl:m has 2094 NULL values\n",
      "Column: snow_density:kgm3 has 15769 NULL values\n",
      "Column: ceiling_height_agl:m has 22247 NULL values\n",
      "Column: cloud_base_agl:m has 8066 NULL values\n",
      "Column: snow_density:kgm3 has 115945 NULL values\n"
     ]
    }
   ],
   "source": [
    "for column in X_train_estimated_a.columns:\n",
    "    null_c =  X_train_estimated_a[column].isna().sum()\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} NULL values')\n",
    "\n",
    "for column in X_train_observed_a.columns:\n",
    "    null_c = X_train_observed_a[column].isna().sum()\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} NULL values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on output we choose to drop `snow_density:kgm3`, `ceiling_height_agl:m`, `cloud_base_agl:m`\n",
    "\n",
    "```\n",
    "X_ESTIMATED\n",
    "Column: ceiling_height_agl:m has 3919 NULL values\n",
    "Column: cloud_base_agl:m has 2094 NULL values\n",
    "Column: snow_density:kgm3 has 15769 NULL values\n",
    "\n",
    "X_OBSERVED:\n",
    "Column: ceiling_height_agl:m has 22247 NULL values\n",
    "Column: cloud_base_agl:m has 8066 NULL values\n",
    "Column: snow_density:kgm3 has 115945 NULL values\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" for column in X_train_estimated_a.columns:\\n    null_c =  X_train_estimated_a[column].value_counts()[0]\\n    if null_c > 0: \\n        print(f'Column: {column} has {null_c} 0 values')\\n\\nfor column in X_train_observed_a.columns:\\n    null_c = X_train_observed_a[column].value_counts()[0]\\n    if null_c > 0: \\n        print(f'Column: {column} has {null_c} 0 values') \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" for column in X_train_estimated_a.columns:\n",
    "    null_c =  X_train_estimated_a[column].value_counts()[0]\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} 0 values')\n",
    "\n",
    "for column in X_train_observed_a.columns:\n",
    "    null_c = X_train_observed_a[column].value_counts()[0]\n",
    "    if null_c > 0: \n",
    "        print(f'Column: {column} has {null_c} 0 values') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data set A, B and C clean up\n",
    "\n",
    "def data_clean_up(x_train_est, x_train_observe, y_train):\n",
    "\n",
    "  if 'date_calc' in x_train_est.columns:\n",
    "    x_train_est.drop(columns=\"date_calc\", inplace=True)\n",
    "\n",
    "  x_train = pd.concat([x_train_observe, x_train_est])\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_train.columns[1:]}\n",
    "  X_train_downscaled = x_train.groupby(x_train.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  y_train.dropna(inplace=True)\n",
    "  combined_data = pd.merge(X_train_downscaled, y_train, left_on='date_forecast', right_on='time')\n",
    "  combined_data.drop(columns=['snow_density:kgm3', 'ceiling_height_agl:m', 'cloud_base_agl:m'], inplace=True)\n",
    "  combined_data.dropna(inplace=True)\n",
    "  y_train = combined_data[['pv_measurement', 'date_forecast']]\n",
    "\n",
    "  if 'time' and 'pv_measurement' in combined_data.columns:\n",
    "    combined_data.drop(columns=\"time\", inplace=True)\n",
    "    combined_data.drop(columns=\"pv_measurement\", inplace=True)\n",
    "\n",
    "  return combined_data, y_train\n",
    "\n",
    "def count_null_in_column(df: pd.DataFrame, column_name: str):\n",
    "  return df[column_name].value_counts(None)\n",
    "\n",
    "x_train_a, y_train_a = data_clean_up(X_train_estimated_a, X_train_observed_a, train_targets_a)\n",
    "x_train_b, y_train_b = data_clean_up(X_train_estimated_b, X_train_observed_b, train_targets_b)\n",
    "x_train_c, y_train_c = data_clean_up(X_train_estimated_c, X_train_observed_c, train_targets_c)\n",
    "\n",
    "\n",
    "def data_clean_up_test(x_test_est):\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_test_est.columns[1:]}\n",
    "  X_test_downscaled = x_test_est.groupby(x_test_est.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  \"\"\"\n",
    "  if 'date_forecast' in X_test_downscaled.columns:\n",
    "    X_test_downscaled.drop(columns=\"date_forecast\", inplace=True)\n",
    "  \"\"\"\n",
    "  return X_test_downscaled\n",
    "\n",
    "X_test_estimated_a = data_clean_up_test(X_test_estimated_a)\n",
    "X_test_estimated_b = data_clean_up_test(X_test_estimated_b)\n",
    "X_test_estimated_c = data_clean_up_test(X_test_estimated_c)\n",
    "print(len(X_test_estimated_a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping this average for now since its the best aggreagation we have at this point. However i do believe that using avg or mean together with delta (total change within the hour) might be a better solution. Also some columns might need other aggregations than avg. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to tell the model something about how time seasonality works. E.g. night and day, as well as yearly seasonality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   absolute_humidity_2m:gm3  air_density_2m:kgm3  clear_sky_energy_1h:J  \\\n",
      "0                    8.0250             1.230625               0.000000   \n",
      "1                    7.9000             1.228750               0.000000   \n",
      "2                    8.0125             1.224750               0.000000   \n",
      "3                    8.3125             1.223250             104.324997   \n",
      "4                    8.6625             1.222500           16234.075195   \n",
      "\n",
      "   clear_sky_rad:W  dew_or_rime:idx  dew_point_2m:K  diffuse_rad:W  \\\n",
      "0            0.000              0.5      280.787506         0.0000   \n",
      "1            0.000              0.5      280.574982         0.0000   \n",
      "2            0.000              0.5      280.787506         0.0000   \n",
      "3            0.375              0.5      281.362488         0.1500   \n",
      "4           11.550              0.5      281.924988         5.9875   \n",
      "\n",
      "   diffuse_rad_1h:J  direct_rad:W  direct_rad_1h:J  ...  total_cloud_cover:p  \\\n",
      "0          0.000000         0.000         0.000000  ...           100.000000   \n",
      "1          0.000000         0.000         0.000000  ...           100.000000   \n",
      "2          0.000000         0.000         0.000000  ...           100.000000   \n",
      "3        263.387512         0.000         0.000000  ...           100.000000   \n",
      "4      11034.474609         0.075       141.487503  ...            99.612503   \n",
      "\n",
      "   visibility:m  wind_speed_10m:ms  wind_speed_u_10m:ms  wind_speed_v_10m:ms  \\\n",
      "0  30549.500000             2.1500              -1.9500               0.0750   \n",
      "1  19697.412109             2.0625              -1.4625               0.4000   \n",
      "2   8417.962891             2.2750              -0.9500               0.8625   \n",
      "3   2782.675049             2.1500              -0.8000               1.0375   \n",
      "4   7081.625000             2.3750              -0.2625               1.0500   \n",
      "\n",
      "   wind_speed_w_1000hPa:ms  sinus_day  cosine_day  sinus_year  cosinus_year  \n",
      "0                      0.0   1.500000    2.866025    2.487287      1.126758  \n",
      "1                      0.0   1.741181    2.965926    2.486661      1.126409  \n",
      "2                      0.0   2.000000    3.000000    2.486034      1.126060  \n",
      "3                      0.0   2.258819    2.965926    2.485408      1.125712  \n",
      "4                      0.0   2.500000    2.866025    2.484781      1.125364  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_unixtime(datetime: pd.Series) -> pd.Series:\n",
    "    unixtime = (datetime - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "    return unixtime\n",
    "\n",
    "xat_datetime = x_train_a['date_forecast']\n",
    "xat_unixtime = get_unixtime(xat_datetime)\n",
    "\n",
    "## We now need functions for assigning daily and yearly cycles (described in datanalysis docu on Peter branch)\n",
    "# plus 2 avoids 0 and negative values\n",
    "day = 24*60*60\n",
    "year = (365.2425)*day\n",
    "\n",
    "def sinus_day(unix_time):\n",
    "    return 2 + np.sin(unix_time * (2 * np.pi / day)) # since it is seconds since 1.1.1970 we divide by seconds in a day to get seasonal changes throughout the dat\n",
    "\n",
    "def sinus_year(unix_time):\n",
    "    return 2+ np.sin(unix_time * (2 * np.pi / year))\n",
    "\n",
    "def cosinus_day(unix_time):\n",
    "    return 2+np.cos(unix_time * (2 * np.pi / day))\n",
    "\n",
    "def cosinus_year(unix_time):\n",
    "    return 2+np.cos(unix_time * (2 * np.pi / year))\n",
    "\n",
    "# function for returning two series with the daily cycles (sine and cosine)\n",
    "def get_daycycle(unixtime: pd.Series) -> (pd.Series, pd.Series):\n",
    "    sinus_daytime = unixtime.apply(sinus_day)\n",
    "    sinus_daytime = sinus_daytime.rename('sinus_day') \n",
    "    cosinus_daytime = unixtime.apply(cosinus_day)\n",
    "    cosinus_daytime = cosinus_daytime.rename('cosine_day')\n",
    "    return sinus_daytime, cosinus_daytime\n",
    "\n",
    "# Function for returning two series with the yearly cycles\n",
    "def get_yearcycle(unixtime: pd.Series) -> (pd.Series, pd.Series):\n",
    "    sinus_yeartime = unixtime.apply(sinus_year)\n",
    "    sinus_yeartime = sinus_yeartime.rename('sinus_year')\n",
    "    cosinus_yeartime = unixtime.apply(cosinus_year)\n",
    "    cosinus_yeartime = cosinus_yeartime.rename('cosinus_year')\n",
    "    return sinus_yeartime, cosinus_yeartime\n",
    "\n",
    "xat_day_sin, xat_day_cos = get_daycycle(xat_unixtime)\n",
    "xat_year_sin, xat_year_cos = get_yearcycle(xat_unixtime)\n",
    "\n",
    "xta_feat = x_train_a.join([xat_day_sin, xat_day_cos, xat_year_sin, xat_year_cos])\n",
    "xta_feat.drop(columns=['date_forecast'], inplace=True)\n",
    "print(xta_feat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to do some extra feature engineering and make shure that all the elements in the dataset is of the same type. A lot of this is explained in the TensorFlow documentation https://www.tensorflow.org/tutorials/load_data/pandas_dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/94/wm45lcfj7sv746_0rnyvdk840000gn/T/ipykernel_31201/4044865919.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  target.drop(columns='date_forecast', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "SHUFFLE_BUFFER = 500\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "xta_tensor = tf.convert_to_tensor(xta_feat)\n",
    "\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "normalizer.adapt(xta_tensor)\n",
    "target = y_train_a\n",
    "target.drop(columns='date_forecast', inplace=True)\n",
    "print(len(target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "14834/14834 [==============================] - 8s 523us/step - loss: -2155586.0000 - accuracy: 0.0623\n",
      "Epoch 2/15\n",
      "14834/14834 [==============================] - 8s 517us/step - loss: -14605885.0000 - accuracy: 0.1643\n",
      "Epoch 3/15\n",
      "14834/14834 [==============================] - 8s 516us/step - loss: -39073988.0000 - accuracy: 0.3192\n",
      "Epoch 4/15\n",
      "14834/14834 [==============================] - 8s 523us/step - loss: -74925280.0000 - accuracy: 0.3406\n",
      "Epoch 5/15\n",
      "14834/14834 [==============================] - 8s 532us/step - loss: -122671664.0000 - accuracy: 0.3551\n",
      "Epoch 6/15\n",
      "14834/14834 [==============================] - 8s 518us/step - loss: -182290768.0000 - accuracy: 0.3642\n",
      "Epoch 7/15\n",
      "14834/14834 [==============================] - 8s 517us/step - loss: -252791728.0000 - accuracy: 0.3675\n",
      "Epoch 8/15\n",
      "14834/14834 [==============================] - 8s 515us/step - loss: -334157728.0000 - accuracy: 0.3695\n",
      "Epoch 9/15\n",
      "14834/14834 [==============================] - 8s 531us/step - loss: -427725056.0000 - accuracy: 0.3740\n",
      "Epoch 10/15\n",
      "14834/14834 [==============================] - 8s 540us/step - loss: -532462432.0000 - accuracy: 0.3752\n",
      "Epoch 11/15\n",
      "14834/14834 [==============================] - 8s 510us/step - loss: -647939072.0000 - accuracy: 0.3770\n",
      "Epoch 12/15\n",
      "14834/14834 [==============================] - 8s 516us/step - loss: -775646464.0000 - accuracy: 0.3775\n",
      "Epoch 13/15\n",
      "14834/14834 [==============================] - 8s 511us/step - loss: -914210368.0000 - accuracy: 0.3805\n",
      "Epoch 14/15\n",
      "14834/14834 [==============================] - 8s 515us/step - loss: -1063909312.0000 - accuracy: 0.3817\n",
      "Epoch 15/15\n",
      "14834/14834 [==============================] - 8s 518us/step - loss: -1226583040.0000 - accuracy: 0.3822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x289d3bc40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_basic_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    #tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(optimizer='adam',\n",
    "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "\n",
    "\n",
    "model = get_basic_model()\n",
    "model.fit(xta_tensor, target, epochs=15, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some more stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (0) does not match length of index (2160)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb Cell 24\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_test_pred \u001b[39m=\u001b[39m y_pred\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/test.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m test[\u001b[39m'\u001b[39;49m\u001b[39mprediction\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m y_test_pred\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m sample_submission \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/sample_submission.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/peterlawrence/Repos/TDT4173-ML/model_notebooks/tensorflow.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m submission \u001b[39m=\u001b[39m sample_submission[[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mmerge(test[[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m'\u001b[39m]], on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/pandas/core/frame.py:4094\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4091\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4092\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4093\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 4094\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/pandas/core/frame.py:4303\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4294\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4295\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4296\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4301\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4302\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4303\u001b[0m     value, refs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4305\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4306\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4307\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4308\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4309\u001b[0m     ):\n\u001b[1;32m   4310\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4311\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/pandas/core/frame.py:5042\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5039\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   5041\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5042\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   5043\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Repos/TDT4173-ML/venv/lib/python3.9/site-packages/pandas/core/common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (0) does not match length of index (2160)"
     ]
    }
   ],
   "source": [
    "y_test_pred = y_pred\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test['prediction'] = y_test_pred\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
