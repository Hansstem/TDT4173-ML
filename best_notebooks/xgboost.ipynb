{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "import data_func.read_data as read_data\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"is_sparse is deprecated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = read_data.get_training_data()\n",
    "X_frames_train = dataframes[0]\n",
    "Y_frames_train = dataframes[1]\n",
    "X_frames_test = read_data.get_test_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTRA = X_frames_train[0]\n",
    "XTRB = X_frames_train[1]\n",
    "XTRC = X_frames_train[2]\n",
    "\n",
    "YA = Y_frames_train[0]\n",
    "YB = Y_frames_train[1]\n",
    "YC = Y_frames_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before B drop:  32848\n",
      "after B drop:  20628\n",
      "before C drop:  32155\n",
      "after C drop:  31827\n"
     ]
    }
   ],
   "source": [
    "start_times_b = [\n",
    "    '2019-03-24 00:00:00',\n",
    "'2019-05-31 00:00:00',\n",
    "'2019-10-28 14:00:00',\n",
    "'2020-02-23 17:00:00',\n",
    "'2020-03-26 14:00:00',\n",
    "'2020-04-02 03:00:00',\n",
    "'2020-07-12 23:00:00',\n",
    "'2020-09-24 14:00:00',\n",
    "'2021-01-15 10:00:00',\n",
    "'2021-04-29 00:00:00',\n",
    "'2021-06-05 03:00:00',\n",
    "'2021-06-13 04:00:00',\n",
    "'2021-06-22 03:00:00',\n",
    "'2021-07-03 15:00:00',\n",
    "'2021-08-26 00:00:00',\n",
    "'2021-09-08 15:00:00',\n",
    "'2021-09-19 02:00:00',\n",
    "'2021-01-30 15:00:00',\n",
    "'2022-02-10 21:00:00',\n",
    "'2022-02-16 14:00:00',\n",
    "'2022-03-19 15:00:00',\n",
    "'2022-12-05 17:00:00',\n",
    "'2023-02-24 01:00:00',\n",
    "'2023-03-07 10:00:00',\n",
    "'2023-03-25 23:00:00'\n",
    "]\n",
    "\n",
    "end_times_b = [\n",
    "    '2019-03-28 00:00:00',\n",
    "'2019-06-03 14:00:00',\n",
    "'2019-10-30 23:00:00',\n",
    "'2020-03-06 06:00:00',\n",
    "'2020-03-27 22:00:00',\n",
    "'2020-04-16 08:00:00',\n",
    "'2020-08-25 23:00:00',\n",
    "'2020-09-25 23:00:00',\n",
    "'2021-04-19 09:00:00',\n",
    "'2021-05-01 23:00:00',\n",
    "'2021-06-07 08:00:00',\n",
    "'2021-06-14 10:00:00',\n",
    "'2021-06-24 08:00:00',\n",
    "'2021-07-06 07:00:00',\n",
    "'2021-09-03 22:00:00',\n",
    "'2021-09-14 13:00:00',\n",
    "'2021-09-27 10:00:00',\n",
    "'2022-02-04 09:00:00',\n",
    "'2022-02-13 07:00:00',\n",
    "'2022-02-24 06:00:00',\n",
    "'2022-04-13 06:00:00',\n",
    "'2023-01-05 08:00:00',\n",
    "'2023-02-27 05:00:00',\n",
    "'2023-03-10 01:00:00',\n",
    "'2023-03-28 02:00:00'\n",
    "]\n",
    "\n",
    "print(\"before B drop: \", len(YB))\n",
    "\n",
    "for i in range(len(start_times_b)):\n",
    "    a = pd.to_datetime(start_times_b[i])\n",
    "    b = pd.to_datetime(end_times_b[i])\n",
    "    ind = YB[ (YB['time'] >= a) & (YB['time'] <= b)].index\n",
    "    \n",
    "    YB.drop(ind, inplace=True)\n",
    "\n",
    "print(\"after B drop: \", len(YB))\n",
    "\n",
    "print(\"before C drop: \", len(YC))\n",
    "\n",
    "a = pd.to_datetime(\"2020-02-23 17:00:00\")\n",
    "b = pd.to_datetime('2020-03-08 08:00:00')\n",
    "ind = YC[ (YC['time'] >= a) & (YC['time'] <= b)].index\n",
    "    \n",
    "YC.drop(ind, inplace=True)\n",
    "print(\"after C drop: \", len(YC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data clean up an aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29667 29667\n",
      "17161 17161\n",
      "22813 22813\n",
      "720\n",
      "720\n",
      "720\n"
     ]
    }
   ],
   "source": [
    "# Making sure that target values line up with x_values\n",
    "import data_func.aggregation as data_agg\n",
    "\n",
    "categorical_col = ['dew_or_rime:idx', 'precip_type_5min:idx', 'is_day:idx', 'is_in_shadow:idx']\n",
    "\n",
    "def aggregate_correct_x(x: pd.DataFrame) -> pd.DataFrame:\n",
    "   '''\n",
    "   Takes a given dataframe and returns an aggregated dataframe based on selected categorical functions. \n",
    "   Assumes grouping of 4.\n",
    "   '''\n",
    "   categorical = x[[\"date_forecast\"] + categorical_col]\n",
    "   mean = x.drop(columns=categorical_col)\n",
    "\n",
    "   categorical = data_agg.gen_agg(categorical, agg_type=data_agg.stocastic_median)\n",
    "   mean = data_agg.gen_agg(mean, \"mean\")\n",
    "\n",
    "   return pd.merge(categorical, mean, on=\"date_forecast\")\n",
    "\n",
    "def data_allign(x_train, y_train):\n",
    "\n",
    "  y_train.dropna(inplace=True)\n",
    "  x_train = aggregate_correct_x(x_train)\n",
    "  combined_data = pd.merge(x_train, y_train, left_on='date_forecast', right_on='time')\n",
    "  y_train = combined_data['pv_measurement']\n",
    "\n",
    "  if 'time' and 'pv_measurement' in combined_data.columns:\n",
    "    combined_data.drop(columns=['time', 'pv_measurement'], inplace=True)\n",
    "    \n",
    "  return combined_data, y_train\n",
    "\n",
    "\n",
    "\n",
    "X_train = [XTRA, XTRB, XTRC]\n",
    "Y_train = [YA, YB, YC]\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i], Y_train[i] = data_allign(X_train[i], Y_train[i])\n",
    "\n",
    "for j in range(len(X_frames_test)):\n",
    "    X_frames_test[j] = aggregate_correct_x(X_frames_test[j])\n",
    "\n",
    "\n",
    "print(len(X_train[0]), len(Y_train[0]))\n",
    "print(len(X_train[1]), len(Y_train[1]))\n",
    "print(len(X_train[2]), len(Y_train[2]))\n",
    "\n",
    "print(len(X_frames_test[0]))\n",
    "print(len(X_frames_test[1]))\n",
    "print(len(X_frames_test[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_func.timeseasonality as DTS\n",
    "import data_func.date_forecast as DTF\n",
    "import data_func.one_hot_encoding as OHE\n",
    "\n",
    "categorical_features = ['dew_or_rime:idx', 'precip_type_5min:idx']\n",
    "\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = DTS.append_seasonal_columns(X_train[i])\n",
    "    X_train[i] = DTF.date_forecast_columns(X_train[i])\n",
    "    # X_train[i] = OHE.one_hot_encode(X_train[i], categorical_features)\n",
    "    X_train[i].drop(columns=['snow_drift:idx'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(X_frames_test)):\n",
    "    X_frames_test[i] = DTS.append_seasonal_columns(X_frames_test[i])\n",
    "    X_frames_test[i] = DTF.date_forecast_columns(X_frames_test[i])\n",
    "    # X_frames_test[i] = OHE.one_hot_encode(X_frames_test[i], categorical_features)\n",
    "    X_frames_test[i].drop(columns=['snow_drift:idx'], inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8763563332327975, device=None,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.08609213174337473, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=727, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8763563332327975, device=None,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.08609213174337473, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=727, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=0.8763563332327975, device=None,\n",
       "             early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=None, feature_types=None, gamma=None, grow_policy=None,\n",
       "             importance_type=None, interaction_constraints=None,\n",
       "             learning_rate=0.08609213174337473, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=8, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=727, n_jobs=None,\n",
       "             num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "x_train_a, x_val_a, y_train_a, y_val_a = train_test_split(X_train[0], Y_train[0], test_size=0.17, random_state=42)\n",
    "x_train_b, x_val_b, y_train_b, y_val_b = train_test_split(X_train[1], Y_train[1], test_size=0.17, random_state=42)\n",
    "x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(X_train[2], Y_train[2], test_size=0.17, random_state=42)\n",
    "\n",
    "# Use params from hyperparameter tuning using optuna\n",
    "params_a = {'random_state': 42, 'n_estimators': 980, 'max_depth': 9, 'learning_rate': 0.029035565559484028, 'subsample': 0.8393121619033767, 'colsample_bytree': 0.7589542758688459}\n",
    "params_b = {'random_state': 42, 'n_estimators': 955, 'max_depth': 9, 'learning_rate': 0.02949625834198986, 'subsample': 0.8030196155828968, 'colsample_bytree': 0.72518389089994}\n",
    "params_c = {'random_state': 42, 'n_estimators': 727, 'max_depth': 8, 'learning_rate': 0.08609213174337473, 'subsample': 0.8107057409889747, 'colsample_bytree': 0.8763563332327975}\n",
    "model_a = xgb.XGBRegressor(**params_a)\n",
    "model_b = xgb.XGBRegressor(**params_b)\n",
    "model_c = xgb.XGBRegressor(**params_c)\n",
    "\n",
    "\n",
    "model_a.fit(x_train_a, y_train_a)\n",
    "model_b.fit(x_train_b, y_train_b)\n",
    "model_c.fit(x_train_c, y_train_c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_squared_error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e0074e30707c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Evaluate the model based on the validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmae_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MAE for A: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmae_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmae_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_b\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_squared_error' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model based on the validation data\n",
    "\n",
    "mae_a = mean_absolute_error(y_val_a, model_a.predict(x_val_a))\n",
    "print(\"MAE for A: \", mae_a)\n",
    "mae_b = mean_absolute_error(y_val_b, model_b.predict(x_val_b))\n",
    "print(\"MAE for B: \", mae_b)\n",
    "mae_c = mean_absolute_error(y_val_c, model_c.predict(x_val_c))\n",
    "print(\"MAE for C: \", mae_c)\n",
    "print(\"Mean MAE: \", (mae_a + mae_b + mae_c) / 3)\n",
    "\n",
    "# Evaluate the predictions\n",
    "\n",
    "score_a = model_a.score(x_val_a, y_val_a)\n",
    "score_b = model_b.score(x_val_b, y_val_b)\n",
    "score_c = model_c.score(x_val_c, y_val_c)\n",
    "\n",
    "print(\"Score A: \", score_a)\n",
    "print(\"Score B: \", score_b)\n",
    "print(\"Score C: \", score_c)\n",
    "print('')\n",
    "\n",
    "# Get feature importance scores\n",
    "models = [(model_a, 'A'), (model_b, 'B'), (model_c, 'C')]\n",
    "for model in models:\n",
    "\n",
    "    feature_importance_scores = model[0].feature_importances_\n",
    "\n",
    "# Create a DataFrame to associate features with their importance scores\n",
    "    feature_importance_df1 = pd.DataFrame({'Feature': x_train_a.columns, 'Importance': feature_importance_scores})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "    feature_importance_df1 = feature_importance_df1.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print or visualize the feature importance scores\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    feature_importance_df1.head(500)\n",
    "    print(f'Model {model[1]}')\n",
    "    print(feature_importance_df1)\n",
    "    pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Most_common = ['direct_rad:W', 'clear_sky_rad:W']\n",
    "\n",
    "MSE for A:  155326.11984010294\n",
    "MSE for B:  4311.822664627681\n",
    "MSE for C:  2484.332046556924\n",
    "Mean MSE:  54040.75818376252\n",
    "Score A:  0.8869367102250868\n",
    "Score B:  0.8880678863853381\n",
    "Score C:  0.9167532450100108\n",
    "\n",
    "Model A\n",
    "                 Feature  Importance\n",
    "9           direct_rad:W    0.558548 <-------- 3\n",
    "7          diffuse_rad:W    0.077622 <-------- 2\n",
    "18      is_in_shadow:idx    0.028293 <--------2\n",
    "3        clear_sky_rad:W    0.026308 <-------- 3\n",
    "40          cosinus_year    0.024731\n",
    "24     snow_density:kgm3    0.022149 <-------- 2\n",
    "29         sun_azimuth:d    0.021677\n",
    "20  precip_type_5min:idx    0.016280\n",
    "6         dew_point_2m:K    0.015750\n",
    "19        precip_5min:mm    0.014176 <-------- 2\n",
    "\n",
    "Model B\n",
    "             Feature  Importance\n",
    "9       direct_rad:W    0.369634     <--------- 3\n",
    "30   sun_elevation:d    0.165624     <-------- 2\n",
    "3    clear_sky_rad:W    0.084479     <--------- 3\n",
    "18  is_in_shadow:idx    0.074470     <---------\n",
    "17        is_day:idx    0.037519\n",
    "22   rain_water:kgm2    0.028054\n",
    "40      cosinus_year    0.024928\n",
    "7      diffuse_rad:W    0.019828     <--------- 2\n",
    "39        sinus_year    0.016917\n",
    "16  fresh_snow_6h:cm    0.015372     <-------- 1/2\n",
    "\n",
    "Model C\n",
    "                 Feature  Importance\n",
    "30       sun_elevation:d    0.737467 <--------- 2\n",
    "3        clear_sky_rad:W    0.110762 <--------- 3\n",
    "9           direct_rad:W    0.029844 <--------- 3\n",
    "10       direct_rad_1h:J    0.023283\n",
    "20  precip_type_5min:idx    0.010948\n",
    "12     fresh_snow_12h:cm    0.010607\n",
    "24     snow_density:kgm3    0.009615 <-------- 2\n",
    "14     fresh_snow_24h:cm    0.007877 <-------- 1/2\n",
    "6         dew_point_2m:K    0.005590\n",
    "19        precip_5min:mm    0.005087 <-------- 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the entire training data\n",
    "model_a.fit(X_train[0], Y_train[0])\n",
    "model_b.fit(X_train[1], Y_train[1])\n",
    "model_c.fit(X_train[2], Y_train[2])\n",
    "\n",
    "y_pred_a = model_a.predict(X_frames_test[0])\n",
    "y_pred_b = model_b.predict(X_frames_test[1])\n",
    "y_pred_c = model_c.predict(X_frames_test[2])\n",
    "print(len(y_pred_a))\n",
    "\n",
    "y_pred = np.concatenate((y_pred_a, y_pred_b, y_pred_c), axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720 720\n",
      "720 720\n",
      "720 720\n"
     ]
    }
   ],
   "source": [
    "print(len(y_pred_a), len(X_frames_test[0]))\n",
    "print(len(y_pred_b), len(X_frames_test[1]))\n",
    "print(len(y_pred_c), len(X_frames_test[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] < 0: \n",
    "        y_pred[i] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = y_pred\n",
    "print(len(y_test_pred))\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test['prediction'] = y_test_pred\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
