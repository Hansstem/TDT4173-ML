{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f893e55-58ef-4a9b-8066-9681fb434b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"xgboost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f119c7d-5088-4047-ab68-d2bf44c4acb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_estimated_a = pd.read_parquet('../data/A/X_train_estimated.parquet')\n",
    "X_train_estimated_b = pd.read_parquet('../data/B/X_train_estimated.parquet')\n",
    "X_train_estimated_c = pd.read_parquet('../data/C/X_train_estimated.parquet')\n",
    "\n",
    "X_train_observed_a = pd.read_parquet('../data/A/X_train_observed.parquet')\n",
    "X_train_observed_b = pd.read_parquet('../data/B/X_train_observed.parquet')\n",
    "X_train_observed_c = pd.read_parquet('../data/C/X_train_observed.parquet')\n",
    "\n",
    "X_test_estimated_a = pd.read_parquet('../data/A/X_test_estimated.parquet')\n",
    "X_test_estimated_b = pd.read_parquet('../data/B/X_test_estimated.parquet')\n",
    "X_test_estimated_c = pd.read_parquet('../data/C/X_test_estimated.parquet')\n",
    "\n",
    "train_targets_a = pd.read_parquet('../data/A/train_targets.parquet')\n",
    "train_targets_b = pd.read_parquet('../data/B/train_targets.parquet')\n",
    "train_targets_c = pd.read_parquet('../data/C/train_targets.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fda9eb2-6d4a-4be3-8ada-b924ddf5c9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set A, B and C clean up\n",
    "\n",
    "def data_clean_up(x_train_est, x_train_observe, y_train):\n",
    "\n",
    "  if 'date_calc' in x_train_est.columns:\n",
    "    x_train_est.drop(columns=\"date_calc\", inplace=True)\n",
    "\n",
    "  x_train = pd.concat([x_train_observe, x_train_est])\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_train.columns[1:]}\n",
    "  X_train_downscaled = x_train.groupby(x_train.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  y_train.dropna(inplace=True)\n",
    "  combined_data = pd.merge(X_train_downscaled, y_train, left_on='date_forecast', right_on='time')\n",
    "  y_train = combined_data['pv_measurement']\n",
    "\n",
    "  if 'date_forecast' and 'time' and 'pv_measurement' in combined_data.columns:\n",
    "    combined_data.drop(columns=\"date_forecast\", inplace=True)\n",
    "    combined_data.drop(columns=\"time\", inplace=True)\n",
    "    combined_data.drop(columns=\"pv_measurement\", inplace=True)\n",
    "\n",
    "\n",
    "  return combined_data, y_train\n",
    "\n",
    "x_train_a, y_train_a = data_clean_up(X_train_estimated_a, X_train_observed_a, train_targets_a)\n",
    "x_train_b, y_train_b = data_clean_up(X_train_estimated_b, X_train_observed_b, train_targets_b)\n",
    "x_train_c, y_train_c = data_clean_up(X_train_estimated_c, X_train_observed_c, train_targets_c)\n",
    "\n",
    "\n",
    "def data_clean_up_test(x_test_est):\n",
    "\n",
    "\n",
    "  # Group the rows into blocks of 4 and apply the aggregation function\n",
    "  agg_func = {col: 'mean' for col in x_test_est.columns[1:]}\n",
    "  X_test_downscaled = x_test_est.groupby(x_test_est.index // 4).agg({**{'date_forecast': 'first'}, **agg_func})\n",
    "\n",
    "  if 'date_forecast' in X_test_downscaled.columns:\n",
    "    X_test_downscaled.drop(columns=\"date_forecast\", inplace=True)\n",
    "\n",
    "  return X_test_downscaled\n",
    "\n",
    "X_test_estimated_a = data_clean_up_test(X_test_estimated_a)\n",
    "X_test_estimated_b = data_clean_up_test(X_test_estimated_b)\n",
    "X_test_estimated_c = data_clean_up_test(X_test_estimated_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f48ad8-a5cd-4f14-b0d5-6cf130c06828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "\n",
    "x_train_a, x_val_a, y_train_a, y_val_a = train_test_split(x_train_a, y_train_a, test_size=0.17, random_state=42)\n",
    "x_train_b, x_val_b, y_train_b, y_val_b = train_test_split(x_train_b, y_train_b, test_size=0.17, random_state=42)\n",
    "x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(x_train_c, y_train_c, test_size=0.17, random_state=42)\n",
    "\n",
    "model_a = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.25, booster='dart')\n",
    "model_b = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.25)\n",
    "model_c = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, learning_rate=0.24)\n",
    "\n",
    "model_a.fit(x_train_a, y_train_a)\n",
    "model_b.fit(x_train_b, y_train_b)\n",
    "model_c.fit(x_train_c, y_train_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bb68b80-d6e9-4a86-984e-17c8a1820fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_pred_a = model_a.predict(X_test_estimated_a)\n",
    "y_pred_b = model_b.predict(X_test_estimated_b)\n",
    "y_pred_c = model_c.predict(X_test_estimated_c)\n",
    "\n",
    "y_pred = np.concatenate((y_pred_a, y_pred_b, y_pred_c), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f94d2ba-6cc8-427c-b4d5-8c3d7a8080f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for A:  300926.32391917834\n",
      "MSE for B:  7514.05187988106\n",
      "MSE for C:  2373.0810333321456\n",
      "Mean MSE:  103604.48561079719\n",
      "Score A:  0.79337478719318\n",
      "Score B:  0.9661553579999789\n",
      "Score C:  0.9903103401327215\n",
      "                           Feature    Importance\n",
      "33              snow_melt_10min:mm  1.129539e+00\n",
      "27                 rain_water:kgm2  1.640406e-01\n",
      "22                  precip_5min:mm  1.624723e-01\n",
      "31                   snow_depth:cm  4.352140e-02\n",
      "23            precip_type_5min:idx  3.172820e-02\n",
      "20                is_in_shadow:idx  2.538938e-02\n",
      "1              air_density_2m:kgm3  1.912353e-02\n",
      "17                fresh_snow_3h:cm  1.194959e-02\n",
      "19                      is_day:idx  1.120408e-02\n",
      "34                 snow_water:kgm2  8.709469e-03\n",
      "6                  dew_or_rime:idx  7.685732e-03\n",
      "0         absolute_humidity_2m:gm3  4.714774e-03\n",
      "41               wind_speed_10m:ms  4.323167e-03\n",
      "13                     elevation:m  3.703584e-03\n",
      "14               fresh_snow_12h:cm  2.977727e-03\n",
      "42             wind_speed_u_10m:ms  2.061492e-03\n",
      "26                     prob_rime:p  5.232210e-04\n",
      "12         effective_cloud_cover:p  1.327343e-04\n",
      "35                   sun_azimuth:d  1.174197e-04\n",
      "7                   dew_point_2m:K  5.593371e-05\n",
      "38                     t_1000hPa:K  4.557103e-05\n",
      "29                sfc_pressure:hPa  2.546490e-05\n",
      "21                msl_pressure:hPa  2.373055e-05\n",
      "24               pressure_100m:hPa  2.359449e-05\n",
      "2             ceiling_height_agl:m  2.950926e-06\n",
      "25                pressure_50m:hPa  8.526277e-07\n",
      "9                 diffuse_rad_1h:J  1.080237e-08\n",
      "32                  snow_drift:idx -0.000000e+00\n",
      "3            clear_sky_energy_1h:J -3.355498e-08\n",
      "11                 direct_rad_1h:J -1.304968e-07\n",
      "40                    visibility:m -2.461623e-07\n",
      "5                 cloud_base_agl:m -3.230673e-06\n",
      "39             total_cloud_cover:p -9.559153e-05\n",
      "28     relative_humidity_1000hPa:p -2.900412e-04\n",
      "4                  clear_sky_rad:W -3.656841e-04\n",
      "30               snow_density:kgm3 -5.503194e-04\n",
      "36                 sun_elevation:d -7.795145e-04\n",
      "8                    diffuse_rad:W -1.077986e-03\n",
      "10                    direct_rad:W -2.014471e-03\n",
      "16               fresh_snow_24h:cm -4.384577e-03\n",
      "43             wind_speed_v_10m:ms -4.767574e-03\n",
      "18                fresh_snow_6h:cm -1.914206e-02\n",
      "15                fresh_snow_1h:cm -2.324263e-02\n",
      "37  super_cooled_liquid_water:kgm2 -5.635262e-02\n",
      "44         wind_speed_w_1000hPa:ms -5.210284e-01\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model based on the validation data\n",
    "\n",
    "mse_a = mean_squared_error(y_val_a, model_a.predict(x_val_a))\n",
    "print(\"MSE for A: \", mse_a)\n",
    "mse_b = mean_squared_error(y_val_b, model_b.predict(x_val_b))\n",
    "print(\"MSE for B: \", mse_b)\n",
    "mse_c = mean_squared_error(y_val_c, model_c.predict(x_val_c))\n",
    "print(\"MSE for C: \", mse_c)\n",
    "print(\"Mean MSE: \", (mse_a + mse_b + mse_c) / 3)\n",
    "\n",
    "# Evaluate the predictions\n",
    "\n",
    "score_a = model_a.score(x_train_a, y_train_a)\n",
    "score_b = model_b.score(x_train_b, y_train_b)\n",
    "score_c = model_c.score(x_train_c, y_train_c)\n",
    "\n",
    "print(\"Score A: \", score_a)\n",
    "print(\"Score B: \", score_b)\n",
    "print(\"Score C: \", score_c)\n",
    "\n",
    "# Get feature importance scores\n",
    "model_a.fit(x_train_a, y_train_a)\n",
    "feature_importance_scores = model_a.feature_importances_\n",
    "\n",
    "# Create a DataFrame to associate features with their importance scores\n",
    "feature_importance_df = pd.DataFrame({'Feature': x_train_a.columns, 'Importance': feature_importance_scores})\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print or visualize the feature importance scores\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca7bfb66-4944-4b78-abc8-fd808a0d59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = y_pred\n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "test['prediction'] = y_test_pred\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "submission = sample_submission[['id']].merge(test[['id', 'prediction']], on='id', how='left')\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d1dd94-dacc-4562-8601-39b1a751c337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
