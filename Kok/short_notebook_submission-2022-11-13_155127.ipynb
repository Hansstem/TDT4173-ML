{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Group 62] Machines of ML\n",
    "\n",
    "Frimann Bergvik Garmann         - 527245\n",
    "Brage Bergsmyr                  - 514881\n",
    "Magnus Christian Kvist Jacobsen - 506626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n",
    "!pip install geopandas\n",
    "!pip install folium\n",
    "!pip install matplotlib\n",
    "!pip install mapclassify\n",
    "!pip install shapely\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install xgboost\n",
    "!pip install catboost\n",
    "!pip install scipy\n",
    "!pip install lightgbm\n",
    "!pip install h2o\n",
    "!pip install pandas\n",
    "!pip install dataprep\n",
    "!pip install geopy\n",
    "!pip install fiona\n",
    "!pip install pyproj\n",
    "!pip install packaging\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import math\n",
    "import geopy.distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataprep import eda\n",
    "import xgboost as xgb\n",
    "from shapely.geometry import Point\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "busstops_norway                     = pd.read_csv('../data/busstops_norway.csv')\n",
    "grunnkrets_age_distribution         = pd.read_csv('../data/grunnkrets_age_distribution.csv')\n",
    "grunnkrets_households_num_persons   = pd.read_csv('../data/grunnkrets_households_num_persons.csv')\n",
    "grunnkrets_income_households        = pd.read_csv('../data/grunnkrets_income_households.csv')\n",
    "grunnkrets_norway_stripped          = pd.read_csv('../data/grunnkrets_norway_stripped.csv')\n",
    "plaace_hierarchy                    = pd.read_csv('../data/plaace_hierarchy.csv')\n",
    "sample_submission                   = pd.read_csv('../data/sample_submission.csv')\n",
    "stores_extra                        = pd.read_csv('../data/stores_extra.csv')\n",
    "stores_test                         = pd.read_csv('../data/stores_test.csv')\n",
    "stores_train                        = pd.read_csv('../data/stores_train.csv')\n",
    "\n",
    "#eda.create_report(stores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def left_merge(X, Z, on):\n",
    "    return pd.merge(X, Z, how='left', on=on)\n",
    "\n",
    "def replace_missing(X, column, replacement):\n",
    "    _X = X.copy()\n",
    "    _X[column] = _X[column].replace(np.nan, replacement)\n",
    "    return _X\n",
    "\n",
    "def remove_nan_rows(X, column):\n",
    "    _X = X.copy()\n",
    "    return _X[_X[column].notna()]\n",
    "\n",
    "def remove_column(X, column):\n",
    "    _X = X.copy()\n",
    "    return _X.drop(column, axis=1)\n",
    "\n",
    "def remove_year(X):\n",
    "    X_copy = X.copy()\n",
    "    columns = X.columns\n",
    "    X_copy = X_copy.loc[X_copy.groupby('grunnkrets_id')['year'].idxmax()]\n",
    "    X_copy = X_copy.drop(columns=['year'])\n",
    "    #X_copy = X_copy.groupby(['grunnkrets_id'], as_index=False).agg(dict)\n",
    "    return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature functions\n",
    "def sum_people(X):\n",
    "    y_grunnkrets = X.grunnkrets_id\n",
    "    X_copy = X.drop(columns=['grunnkrets_id'])\n",
    "    X_sum = X_copy.sum(axis=1)\n",
    "    return pd.merge(y_grunnkrets.rename('grunnkrets_id'), X_sum.rename('sum_people'), left_index=True, right_index=True)\n",
    "\n",
    "def sum_stores(X):\n",
    "    X_sum = X['grunnkrets_id'].groupby('grunnkrets_id').sum()\n",
    "    return X_sum\n",
    "\n",
    "def jeg_er_lei_meg(X):\n",
    "    # Adds a num_of_stores to the district\n",
    "\n",
    "    districts=X.loc[:,\"district_name\"].unique()\n",
    "    dict={\"district_name\":[],\"num_of_stores\":[]}\n",
    "    for district in districts:\n",
    "        num_of_stores=(X.loc[X[\"district_name\"] == district])[\"store_id\"].count()\n",
    "        dict[\"district_name\"]+=[district]\n",
    "        dict[\"num_of_stores\"]+=[num_of_stores]\n",
    "    return pd.DataFrame.from_dict(dict)\n",
    "\n",
    "\n",
    "def clostest_rival(df, shop_id):\n",
    "    #hente ut samme hierarky\n",
    "    #sammenligne distansen til nærmeste\n",
    "\n",
    "    hierarchy = df.loc[df[\"store_id\"] == shop_id][\"plaace_hierarchy_id\"].values\n",
    "    coords_1 = df.loc[df[\"store_id\"] == shop_id][[\"lon\",\"lat\"]].values\n",
    "    entry_index= df.loc[df[\"store_id\"] == shop_id].index\n",
    "    \n",
    "    check = df.loc[df[\"plaace_hierarchy_id\"] == hierarchy[0]]\n",
    "    #print(check)\n",
    "    distance=math.inf\n",
    "    best_row=None\n",
    "    #finds=\"\"\n",
    "    for index, row in check.iterrows():\n",
    "        coords_2 = row[[\"lon\",\"lat\"]].values\n",
    "        current_distance= geopy.distance.geodesic(coords_1, coords_2).km\n",
    "\n",
    "        if (current_distance < distance and index != entry_index):\n",
    "            distance=current_distance\n",
    "            best_row=row\n",
    "     #       finds+=str(row[\"store_id\"])\n",
    "    #print(distance, \"finds\",finds)\n",
    "    return distance, best_row\n",
    "\n",
    "\n",
    "def add_avg_revenue_municipality(X): #ALL MY HOMIES HATE THIS FUNCTION\n",
    "    districts=X.loc[:,\"municipality_name\"].unique()\n",
    "    dict={\"municipality_name\":[],\"mean\":[]}\n",
    "    for district in districts:\n",
    "        mean = (X.loc[X[\"municipality_name\"] == district])[\"revenue\"].mean()\n",
    "        dict[\"district_name\"]+=[district]\n",
    "        dict[\"mean\"]+=[mean]\n",
    "    df= pd.DataFrame.from_dict(dict)\n",
    "    print(df)\n",
    "    bass = pd.merge(X, df, how=\"left\", on=\"municipality_name\")\n",
    "    X[\"avg_revenue_municipality\"] = bass[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12859, 12)\n",
      "(12610, 12)\n",
      "(12598, 12)\n",
      "1.6160189057767156\n"
     ]
    }
   ],
   "source": [
    "# remove rows with revenue outliers\n",
    "print(stores_train.shape)\n",
    "stores_train = stores_train[stores_train['revenue'] > 0.01]\n",
    "print(stores_train.shape)\n",
    "big_chungus = stores_train[stores_train['revenue'] > 135.0] \n",
    "stores_train = stores_train[stores_train['revenue'] < 150.0] # 180 for å fjerne øverste 5\n",
    "print(stores_train.shape)\n",
    "\n",
    "stores_train['revenue'] = np.log1p(stores_train['revenue'])\n",
    "\n",
    "MEAN_REVENUE = stores_train[\"revenue\"].mean()\n",
    "print(MEAN_REVENUE)\n",
    "#eda.create_report(big_chungus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>year</th>\n",
       "      <th>store_name</th>\n",
       "      <th>plaace_hierarchy_id</th>\n",
       "      <th>sales_channel_name</th>\n",
       "      <th>grunnkrets_id</th>\n",
       "      <th>address</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>chain_name</th>\n",
       "      <th>mall_name</th>\n",
       "      <th>lv1</th>\n",
       "      <th>lv1_desc</th>\n",
       "      <th>lv2</th>\n",
       "      <th>lv2_desc</th>\n",
       "      <th>lv3</th>\n",
       "      <th>lv3_desc</th>\n",
       "      <th>lv4</th>\n",
       "      <th>lv4_desc</th>\n",
       "      <th>district_name</th>\n",
       "      <th>municipality_name</th>\n",
       "      <th>area_km2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>914206820-914239427-717245</td>\n",
       "      <td>2016</td>\n",
       "      <td>VÅLERENGA HALAL BURGER AS</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>3012704</td>\n",
       "      <td>STRØMSVEIEN 25 A</td>\n",
       "      <td>59.908672</td>\n",
       "      <td>10.787031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Dining and Experiences</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>Vålerenga</td>\n",
       "      <td>Oslo</td>\n",
       "      <td>0.057027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>916789157-916823770-824309</td>\n",
       "      <td>2016</td>\n",
       "      <td>BURGER KING MYREN</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>8061401</td>\n",
       "      <td>MYREN 1</td>\n",
       "      <td>59.201467</td>\n",
       "      <td>9.588243</td>\n",
       "      <td>BURGER KING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Dining and Experiences</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>Gulset</td>\n",
       "      <td>Skien</td>\n",
       "      <td>0.165993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>913341082-977479363-2948</td>\n",
       "      <td>2016</td>\n",
       "      <td>BURGER KING STOVNER</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>3013917</td>\n",
       "      <td>STOVNER SENTER 3</td>\n",
       "      <td>59.962146</td>\n",
       "      <td>10.924524</td>\n",
       "      <td>BURGER KING</td>\n",
       "      <td>Stovner Senter</td>\n",
       "      <td>1</td>\n",
       "      <td>Dining and Experiences</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>Fossum</td>\n",
       "      <td>Oslo</td>\n",
       "      <td>0.236628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     store_id  year                 store_name  \\\n",
       "0  914206820-914239427-717245  2016  VÅLERENGA HALAL BURGER AS   \n",
       "1  916789157-916823770-824309  2016          BURGER KING MYREN   \n",
       "2    913341082-977479363-2948  2016        BURGER KING STOVNER   \n",
       "\n",
       "  plaace_hierarchy_id     sales_channel_name  grunnkrets_id           address  \\\n",
       "0             1.1.1.0  Hamburger restaurants        3012704  STRØMSVEIEN 25 A   \n",
       "1             1.1.1.0  Hamburger restaurants        8061401           MYREN 1   \n",
       "2             1.1.1.0  Hamburger restaurants        3013917  STOVNER SENTER 3   \n",
       "\n",
       "         lat        lon   chain_name       mall_name  lv1  \\\n",
       "0  59.908672  10.787031          NaN             NaN    1   \n",
       "1  59.201467   9.588243  BURGER KING             NaN    1   \n",
       "2  59.962146  10.924524  BURGER KING  Stovner Senter    1   \n",
       "\n",
       "                 lv1_desc  lv2    lv2_desc    lv3               lv3_desc  \\\n",
       "0  Dining and Experiences  1.1  Restaurant  1.1.1  Hamburger restaurants   \n",
       "1  Dining and Experiences  1.1  Restaurant  1.1.1  Hamburger restaurants   \n",
       "2  Dining and Experiences  1.1  Restaurant  1.1.1  Hamburger restaurants   \n",
       "\n",
       "       lv4               lv4_desc district_name municipality_name  area_km2  \n",
       "0  1.1.1.0  Hamburger restaurants     Vålerenga              Oslo  0.057027  \n",
       "1  1.1.1.0  Hamburger restaurants        Gulset             Skien  0.165993  \n",
       "2  1.1.1.0  Hamburger restaurants        Fossum              Oslo  0.236628  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plaace_hierarchy=plaace_hierarchy.drop(columns=[\"sales_channel_name\"])\n",
    "\n",
    "stores_train=left_merge(stores_train, plaace_hierarchy, on=\"plaace_hierarchy_id\")\n",
    "stores_test=left_merge(stores_test, plaace_hierarchy, on=\"plaace_hierarchy_id\")\n",
    "\n",
    "# removing year\n",
    "grunnkrets_age_distribution = remove_year(grunnkrets_age_distribution)\n",
    "grunnkrets_households_num_persons = remove_year(grunnkrets_households_num_persons)\n",
    "grunnkrets_income_households = remove_year(grunnkrets_income_households)\n",
    "grunnkrets_norway_stripped = remove_year(grunnkrets_norway_stripped)\n",
    "\n",
    "stores_train = left_merge(stores_train, grunnkrets_norway_stripped[['grunnkrets_id', 'district_name', \"municipality_name\", \"area_km2\"]], on='grunnkrets_id')\n",
    "stores_test = left_merge(stores_test, grunnkrets_norway_stripped[['grunnkrets_id', 'district_name',\"municipality_name\", \"area_km2\"]], on='grunnkrets_id')\n",
    "\n",
    "stores_test.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_train = replace_missing(stores_train, \"chain_name\", \"no chain\")\n",
    "stores_train = replace_missing(stores_train, \"mall_name\", \"no mall\")\n",
    "stores_train = replace_missing(stores_train, 'district_name', 'No district')\n",
    "stores_train = replace_missing(stores_train, 'municipality_name' , \"No municipality_name\")\n",
    "stores_train = replace_missing(stores_train, 'area_km2' , 0.0)\n",
    "\n",
    "\n",
    "\n",
    "stores_test = replace_missing(stores_test, \"chain_name\", \"no chain\")\n",
    "stores_test = replace_missing(stores_test, \"mall_name\", \"no mall\")\n",
    "stores_test = replace_missing(stores_test, 'district_name', 'No district')\n",
    "stores_test = replace_missing(stores_test, 'municipality_name' , \"No municipality_name\")\n",
    "stores_test = replace_missing(stores_test, 'area_km2' , 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_stores_district(X): # Jeg er lei meg v2.0\n",
    "    districts=X.loc[:,\"district_name\"].unique()\n",
    "    dict={\"district_name\":[],\"num_of_stores_district\":[]}\n",
    "    for district in districts:\n",
    "        num_of_stores=(X.loc[X[\"district_name\"] == district])[\"store_id\"].count()\n",
    "        dict[\"district_name\"]+=[district]\n",
    "        dict[\"num_of_stores_district\"]+=[num_of_stores]\n",
    "    return pd.DataFrame.from_dict(dict)\n",
    "\n",
    "def find_num_stores_municipality(X):\n",
    "    municipalities=X.loc[:,\"municipality_name\"].unique()\n",
    "    dict={\"municipality_name\":[],\"num_of_stores_municipality\":[]}\n",
    "    for muni in municipalities:\n",
    "        num_of_stores = (X.loc[X[\"municipality_name\"] == muni])[\"store_id\"].count()\n",
    "        dict[\"municipality_name\"]+=[muni]\n",
    "        dict[\"num_of_stores_municipality\"]+=[num_of_stores]\n",
    "    return pd.DataFrame.from_dict(dict)\n",
    "\n",
    "def find_num_stores_grunnkrets(X):\n",
    "    grunnkretser=X.loc[:,\"grunnkrets_id\"].unique()\n",
    "    dict={\"grunnkrets_id\":[],\"num_of_stores_grunnkrets\":[]}\n",
    "    for grunnkrets in grunnkretser:\n",
    "        num_of_stores = (X.loc[X[\"grunnkrets_id\"] == grunnkrets])[\"store_id\"].count()\n",
    "        dict[\"grunnkrets_id\"]+=[grunnkrets]\n",
    "        dict[\"num_of_stores_grunnkrets\"]+=[num_of_stores]\n",
    "    return pd.DataFrame.from_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stores = stores_train.drop(columns=['revenue']).append(stores_test, ignore_index=True).append(stores_extra, ignore_index=True)\n",
    "all_stores=pd.merge(all_stores,grunnkrets_norway_stripped, how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "#distr=find_num_stores_district(all_stores)\n",
    "#muni=find_num_stores_municipality(all_stores)\n",
    "grunkr=find_num_stores_grunnkrets(all_stores)\n",
    "\n",
    "\n",
    "#stores_train= left_merge(stores_train, distr, on=\"district_name\")\n",
    "#stores_train= left_merge(stores_train, muni, on=\"municipality_name\")\n",
    "stores_train= left_merge(stores_train, grunkr, on=\"grunnkrets_id\")\n",
    "\n",
    "#stores_test= left_merge(stores_test, distr, on=\"district_name\")\n",
    "#stores_test= left_merge(stores_test, muni, on=\"municipality_name\")\n",
    "stores_test= left_merge(stores_test, grunkr, on=\"grunnkrets_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def mall_df(X):\n",
    "    #alls=X.loc[:,\"mall_name\"].unique()\n",
    "    #mallss = malls[np.logical_not(pd.isnull(malls))]\n",
    "    all_stores = np.array(X.values)\n",
    "    df69 = X.groupby('mall_name', as_index=False)[\"lat\",\"lon\"].mean()\n",
    "    df69_nump = np.array(X.groupby('mall_name')[\"lat\",\"lon\"].mean())\n",
    "\n",
    "\n",
    "    lat_ind = X.columns.get_loc(\"lat\")\n",
    "    lon_ind = X.columns.get_loc(\"lon\")\n",
    "    store_id_ind = X.columns.get_loc(\"store_id\")\n",
    "\n",
    "    dict={\"store_id\":[], \"mall_distance\":[], \"closest_mall_name\":[]}\n",
    "\n",
    "    i=0\n",
    "    maxxx=len(all_stores)\n",
    "    for store in all_stores:\n",
    "        if i%5000==0:\n",
    "            print(100*(i/maxxx),\"%\")\n",
    "        i+=1\n",
    "\n",
    "        #distance=math.inf\n",
    "        #closest_mall=None\n",
    "        lat = store[lat_ind]\n",
    "        lon = store[lon_ind]\n",
    "        store_id=store[store_id_ind]\n",
    "        coords_1 = [lat,lon]\n",
    "\n",
    "        #A = df69_nump[spatial.KDTree(df69_nump).query(coords_1)[1]] # <-- the nearest point \n",
    "        B = spatial.KDTree(df69_nump).query(coords_1)\n",
    "        distance=B[0]\n",
    "        mall =df69.iloc[[B[1]]]\n",
    "        dict[\"store_id\"]+=[store_id]\n",
    "        dict[\"mall_distance\"]+=[distance]\n",
    "        dict[\"closest_mall_name\"]+=[mall.values[0][0]]\n",
    "        #print(distance)\n",
    "        #print(mall)\n",
    "        #print(\"A\", A)\n",
    "\n",
    "\n",
    "    return pd.DataFrame.from_dict(dict)\n",
    "\n",
    "\n",
    "#df69 = all_stores.groupby('mall_name')[\"lat\",\"lon\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1434626/1184942796.py:7: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df69 = X.groupby('mall_name', as_index=False)[\"lat\",\"lon\"].mean()\n",
      "/tmp/ipykernel_1434626/1184942796.py:8: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df69_nump = np.array(X.groupby('mall_name')[\"lat\",\"lon\"].mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "10.028279748891874 %\n",
      "20.05655949778375 %\n",
      "30.084839246675628 %\n",
      "40.1131189955675 %\n",
      "50.14139874445937 %\n",
      "60.169678493351256 %\n",
      "70.19795824224313 %\n",
      "80.226237991135 %\n",
      "90.25451774002687 %\n"
     ]
    }
   ],
   "source": [
    "mall_df = mall_df(all_stores)\n",
    "\n",
    "stores_train = pd.merge(stores_train, mall_df, how=\"left\", on=\"store_id\")\n",
    "stores_test = pd.merge(stores_test, mall_df, how=\"left\", on=\"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chain_info(stores_train):\n",
    "    df69 = stores_train.groupby('chain_name', as_index=True)[\"store_id\"].count()\n",
    "    big_chain = df69.loc[df69 >= 1] # Frimann\n",
    "    big_chain = big_chain.dropna()\n",
    "\n",
    "    dict = {\"chain_name\":[], \"avg_rev_big_chain\":[]}\n",
    "\n",
    "    for i in range(len(big_chain)):\n",
    "        avg_rev = stores_train.loc[stores_train[\"chain_name\"] == big_chain.index[i]][\"revenue\"].mean()\n",
    "        dict[\"chain_name\"]+=[big_chain.index[i]]\n",
    "        dict[\"avg_rev_big_chain\"]+=[avg_rev]\n",
    "        #print(big_chain.index[i], avg_rev)\n",
    "    big_chain_df = pd.DataFrame.from_dict(dict)\n",
    "    return big_chain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rev_big_chain= chain_info(stores_train)\n",
    "\n",
    "stores_train = left_merge(stores_train, avg_rev_big_chain, on=\"chain_name\")\n",
    "stores_test = left_merge(stores_test, avg_rev_big_chain, on=\"chain_name\")\n",
    "\n",
    "stores_train = replace_missing(stores_train, 'avg_rev_big_chain' , MEAN_REVENUE)\n",
    "stores_test = replace_missing(stores_test, 'avg_rev_big_chain' , MEAN_REVENUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds avg_rev for different hierarchy levels\n",
    "def avg_rev_hierarchy_levels(X):\n",
    "    lv_1_hierarkies = (X.loc[:,\"lv1_desc\"]).unique()\n",
    "    lv_2_hierarkies = (X.loc[:,\"lv2_desc\"]).unique()\n",
    "    lv_3_hierarkies = (X.loc[:,\"lv3_desc\"]).unique()\n",
    "    lv_4_hierarkies = (X.loc[:,\"lv4_desc\"]).unique()\n",
    "\n",
    "    dict = {\"lv1_desc\":[],\"lv1_avg_rev\":[]}\n",
    "    for lv1 in lv_1_hierarkies:\n",
    "        avg_rev = (X.loc[X[\"lv1_desc\"] == lv1])[\"revenue\"].mean()\n",
    "        dict[\"lv1_desc\"]+=[lv1]\n",
    "        dict[\"lv1_avg_rev\"]+=[avg_rev]\n",
    "    lv1_df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "    dict = {\"lv2_desc\":[],\"lv2_avg_rev\":[]}\n",
    "    for lv2 in lv_2_hierarkies:\n",
    "        avg_rev = (X.loc[X[\"lv2_desc\"] == lv2])[\"revenue\"].mean()\n",
    "        dict[\"lv2_desc\"]+=[lv2]\n",
    "        dict[\"lv2_avg_rev\"]+=[avg_rev]\n",
    "    lv2_df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "    dict = {\"lv3_desc\":[],\"lv3_avg_rev\":[]}\n",
    "    for lv3 in lv_3_hierarkies:\n",
    "        avg_rev = (X.loc[X[\"lv3_desc\"] == lv3])[\"revenue\"].mean()\n",
    "        dict[\"lv3_desc\"]+=[lv3]\n",
    "        dict[\"lv3_avg_rev\"]+=[avg_rev]\n",
    "    lv3_df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "    dict = {\"lv4_desc\":[],\"lv4_avg_rev\":[]}\n",
    "    for lv4 in lv_4_hierarkies:\n",
    "        avg_rev = (X.loc[X[\"lv4_desc\"] == lv4])[\"revenue\"].mean()\n",
    "        dict[\"lv4_desc\"]+=[lv4]\n",
    "        dict[\"lv4_avg_rev\"]+=[avg_rev]\n",
    "    lv4_df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "    return lv1_df, lv2_df, lv3_df, lv4_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lv4_avg_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.639258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.624691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.453620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.255351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.509913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.894660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.102165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lv4_avg_rev\n",
       "count    90.000000\n",
       "mean      1.639258\n",
       "std       0.624691\n",
       "min       0.453620\n",
       "25%       1.255351\n",
       "50%       1.509913\n",
       "75%       1.894660\n",
       "max       4.102165"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_,_,lv3,lv4 = avg_rev_hierarchy_levels(stores_train)\n",
    "\n",
    "stores_train = pd.merge(stores_train, lv3, how=\"left\", on=\"lv3_desc\")\n",
    "stores_train = pd.merge(stores_train, lv4, how=\"left\", on=\"lv4_desc\")\n",
    "\n",
    "stores_test = pd.merge(stores_test, lv3, how=\"left\", on=\"lv3_desc\")\n",
    "stores_test = pd.merge(stores_test, lv4, how=\"left\", on=\"lv4_desc\")\n",
    "\n",
    "lv4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_id                   918023925-918087567-872230\n",
      "lat                                         68.779439\n",
      "lon                                         16.566267\n",
      "geometry    POINT (68.77943861059249 16.566267033764)\n",
      "cluster                                            33\n",
      "Name: 15, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1434626/1164115309.py:29: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  cluster_center = all_stores.groupby('cluster')['lat','lon'].mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nei\n"
     ]
    }
   ],
   "source": [
    "# magnus cluster features\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "revenues = stores_train[['store_id', 'revenue']]\n",
    "all_stores = stores_train.drop(columns=['revenue']).append(stores_test, ignore_index=True).append(stores_extra, ignore_index=True)\n",
    "\n",
    "df=all_stores[['store_id','lat','lon']]\n",
    "geometry = [Point(xy) for xy in zip(df.lat, df.lon)]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "latlon = all_stores[['lat','lon']]\n",
    "X = np.radians(np.array(latlon, dtype='float64'))\n",
    "\n",
    "clustering = DBSCAN(eps=2.0/6371.0, min_samples=250, metric='haversine', algorithm='ball_tree').fit(X)\n",
    "\n",
    "y_clusters = clustering.labels_\n",
    "\n",
    "gdf['cluster'] = y_clusters\n",
    "\n",
    "all_stores['cluster'] = y_clusters\n",
    "gdf=gdf[gdf['cluster']!=-1]\n",
    "gdf.crs = \"EPSG:9672\"\n",
    "print(gdf.loc[gdf['cluster'].idxmax()])\n",
    "\n",
    "cluster_center = all_stores.groupby('cluster')['lat','lon'].mean()\n",
    "\n",
    "cluster_center.rename(columns = {'lat':'center_lat', 'lon':'center_lon'}, inplace = True)\n",
    "\n",
    "all_stores = left_merge(all_stores, cluster_center, on='cluster')\n",
    "\n",
    "all_stores\n",
    "def distance(lat1, lon1, lat2, lon2, cluster):\n",
    "    if cluster == -1:\n",
    "        return 15\n",
    "    else:\n",
    "        return np.linalg.norm(np.array([lat1,lon1])-np.array([lat2, lon2]))\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2, cluster):\n",
    "    max_distance = 20\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    if cluster == -1:\n",
    "        return max_distance\n",
    "    else:\n",
    "        # convert decimal degrees to radians \n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        # haversine formula \n",
    "        dlon = lon2 - lon1 \n",
    "        dlat = lat2 - lat1 \n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        km = 6371 * c\n",
    "        if km >= max_distance:\n",
    "            return max_distance\n",
    "        else:\n",
    "            return km\n",
    "    \n",
    "all_stores['center_distance'] = np.exp(all_stores[['lon', 'lat','center_lon','center_lat', 'cluster']].apply(lambda row: haversine(row['lon'], row['lat'], row['center_lon'], row['center_lat'], row['cluster']), axis=1))\n",
    "\n",
    "stores_train = all_stores[all_stores['store_id'].isin(stores_train['store_id'])]\n",
    "stores_train = left_merge(stores_train, revenues, on='store_id')\n",
    "stores_test = all_stores[all_stores['store_id'].isin(stores_test['store_id'])]\n",
    "stores_extra = all_stores[all_stores['store_id'].isin(stores_extra['store_id'])]\n",
    "df=all_stores[['store_id','lat','lon', 'cluster', 'center_distance', 'store_name']]\n",
    "geometry = [Point(xy) for xy in zip(df.lat, df.lon)]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "gdf=gdf[gdf['cluster']!=-1]\n",
    "sample=gdf.sample(frac=0.25)\n",
    "sample.crs = \"EPSG:9672\"\n",
    "#sample.explore(\"cluster\", marker_type=\"circle_marker\",marker_kwds={\"radius\":8}, legend=False)\n",
    "\n",
    "stores_train.head(5)\n",
    "\n",
    "\n",
    "# TODO Add revenue for rival\n",
    "print(\"nei\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nei\n"
     ]
    }
   ],
   "source": [
    "# magnus rival features\n",
    "\n",
    "# find closest rival in stores_train (those with revenue)\n",
    "from scipy import spatial\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    max_distance = 20\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    return 6371 * c\n",
    "\n",
    "def haversine_vec(coord1, coord2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    lat1, lon1 = coord1[0], coord1[1]\n",
    "    lat2, lon2 = coord2[0], coord2[1]\n",
    "    # convert decimal degrees to radians \n",
    "    return haversine(lon1, lat1, lon2, lat2)\n",
    "'''\n",
    "\n",
    "def calc_rivals_fast(X, df_with_revenue):\n",
    "    \n",
    "    X_copy = X.copy()\n",
    "    X_copy['closest_rival'] = np.nan\n",
    "    plaace_hierarchies = X['plaace_hierarchy_id'].unique()\n",
    "\n",
    "    plaace_hierarchies = plaace_hierarchies[~pd.isnull(plaace_hierarchies)]\n",
    "    for hierarchy in plaace_hierarchies:\n",
    "        X_sub = X_copy[X_copy['plaace_hierarchy_id'] == hierarchy]\n",
    "        df_sub = df_with_revenue[df_with_revenue['plaace_hierarchy_id'] == hierarchy]\n",
    "        X_sub.apply(lambda col: col.drop_duplicates().reset_index(drop=True))\n",
    "        df_sub.apply(lambda col: col.drop_duplicates().reset_index(drop=True))\n",
    "\n",
    "        mat = spatial.distance.cdist(X_sub[['lat','lon']], \n",
    "                              df_sub[['lat','lon']], metric=haversine_vec)\n",
    "        \n",
    "        new_df = pd.DataFrame(mat, index=X_sub['store_id'], columns=df_sub['store_id'])\n",
    "        stores = new_df.columns\n",
    "        for store_id in stores:\n",
    "            new_df.loc[store_id, store_id] = math.inf\n",
    "\n",
    "        \n",
    "        arr = new_df.values\n",
    "        try:\n",
    "            closest = np.where(arr == np.nanmin(arr, axis=1)[:,None],new_df.columns,False)\n",
    "\n",
    "            X_sub['closest'] = [i[i.astype(bool)].tolist() for i in closest]\n",
    "\n",
    "            X_sub['closest_rival'] = [i[i.astype(bool)] for i in closest]\n",
    "\n",
    "            c = X_copy.closest_rival\n",
    "            c.update(X_sub.closest_rival)\n",
    "            X_copy['closest_rival'] = c\n",
    "            X_copy.reset_index(drop=True)\n",
    "            X_copy.loc[X_copy['plaace_hierarchy_id'] == hierarchy, 'closest']= X_sub['closest']\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return X_copy[['store_id', 'closest_rival']]\n",
    "\n",
    "rival_col = 'rival_id'\n",
    "all_stores = stores_train.drop(columns=['revenue']).append(stores_test, ignore_index=True).append(stores_extra, ignore_index=True)\n",
    "\n",
    "closest_rival = calc_rivals_fast(all_stores, stores_train)\n",
    "\n",
    "# clean id-s, get single closest store\n",
    "closest_rival[rival_col] = closest_rival['closest_rival'].astype(str)\n",
    "\n",
    "closest_rival[rival_col] = closest_rival[rival_col].apply(lambda x: x.split(' ')[0])\n",
    "closest_rival[rival_col] = closest_rival[rival_col].apply(lambda x: x.replace('[', '')).apply(lambda x: x.replace(']', ''))\n",
    "\n",
    "closest_rival[rival_col] = closest_rival[rival_col].apply(lambda x: x.replace(\"'\", ''))\n",
    "\n",
    "all_stores[rival_col] = closest_rival[rival_col]\n",
    "\n",
    "rival_stores = stores_train.copy()\n",
    "\n",
    "rival_stores['rival_lat'] = rival_stores['lat']\n",
    "rival_stores['rival_lon'] = rival_stores['lon']\n",
    "rival_stores['rival_revenue'] = rival_stores['revenue']\n",
    "rival_stores['rival_chain'] = rival_stores['chain_name']\n",
    "rival_stores[rival_col] = rival_stores['store_id']\n",
    "\n",
    "revenue = stores_train[['store_id', 'revenue']]\n",
    "\n",
    "all_stores = pd.merge(all_stores, rival_stores[[rival_col, 'rival_lat', 'rival_lon', 'rival_revenue', 'rival_chain']], left_on=[rival_col], right_on= [rival_col], how = 'left')\n",
    "print(\"\")\n",
    "all_stores['rival_dist'] = all_stores[['lat', 'lon','rival_lat','rival_lon']].apply(lambda row: haversine(row['lon'], row['lat'], row['rival_lon'], row['rival_lat']), axis=1)\n",
    "\n",
    "all_stores\n",
    "'''\n",
    "print(\"nei\")\n",
    "\n",
    "#stores_train = pd.merge(stores_train, all_stores[[\"store_id\", \"rival_dist\"]], how=\"left\", on=\"store_id\")\n",
    "#stores_test = pd.merge(stores_test, all_stores[[\"store_id\", \"rival_dist\"]], how=\"left\", on=\"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" \\nfrom shapely import wkt\\n\\nbsn = busstops_norway\\nbsn = bsn[(bsn['importance_level'] == 'Lokalt knutepunkt') | (bsn['importance_level'] == 'Annen viktig holdeplass') | (bsn['importance_level'] == 'Nasjonalt knutepunkt') | (bsn['importance_level'] == 'Regionalt knutepunkt')]\\nbsn['geometry'] = bsn.geometry.apply(wkt.loads)\\nbsn = gpd.GeoDataFrame(bsn, geometry='geometry')\\nbsn['lat'] = bsn.geometry.apply(lambda p: p.y)\\nbsn['lon'] = bsn.geometry.apply(lambda p: p.x)\\n\\n\\n# Find number of busstops within busstops_radius\\n\\n# SET THIS PARAMETER IN UNIT KM!\\nbusstops_radius = 0.5\\n\\nall_stores = stores_train.drop(columns=['revenue']).append(stores_test, ignore_index=True) #.append(stores_extra, ignore_index=True)\\n\\nrevenue = stores_train[['store_id', 'revenue']]\\n\\ndef busstops_within_radius(X, busstops, radius):\\n    \\n    X_copy = X.copy()\\n    \\n    col_name = ''.join(['n_busstops_within_', str(radius), 'km'])\\n    \\n    X_copy[col_name] = np.nan\\n\\n    mat = spatial.distance.cdist(X_copy[['lat','lon']], \\n                          busstops[['lat','lon']], metric=haversine_vec)\\n\\n    new_df = pd.DataFrame(mat, index=X_copy['store_id'], columns=busstops['busstop_id'])\\n\\n    arr = new_df.values\\n    \\n    closest = np.where(arr <= (radius),new_df.columns,False)\\n    # Get column names ignoring false.\\n    X_copy[col_name] = [i[i.astype(bool)] for i in closest]\\n    update_stores = X_copy['store_id'].unique()\\n    print('hei')\\n    for store in update_stores:\\n        arry = X_copy.loc[X_copy['store_id'] == store, col_name]\\n        #print(arry)\\n        j = 0\\n        for i in arry:\\n            j = i\\n            continue\\n        #print(len(arry))\\n        X_copy.loc[X_copy['store_id'] == store, col_name] = len(j)\\n    return X_copy[['store_id', col_name]]\\n\\nbusstops_within_radius = busstops_within_radius(all_stores, bsn, busstops_radius)\\n\\nall_stores = pd.merge(all_stores, busstops_within_radius, how='left', on='store_id')\\n\\nstores_train = all_stores[all_stores['store_id'].isin(stores_train['store_id'])]\\nstores_test = all_stores[all_stores['store_id'].isin(stores_test['store_id'])]\\n#stores_extra = all_stores[all_stores['store_id'].isin(stores_extra['store_id'])]\\nstores_train = pd.merge(stores_train, revenue, how='left', on='store_id')\\nbusstops_within_radius\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# magnus bus features\n",
    "\n",
    "''' \n",
    "from shapely import wkt\n",
    "\n",
    "bsn = busstops_norway\n",
    "bsn = bsn[(bsn['importance_level'] == 'Lokalt knutepunkt') | (bsn['importance_level'] == 'Annen viktig holdeplass') | (bsn['importance_level'] == 'Nasjonalt knutepunkt') | (bsn['importance_level'] == 'Regionalt knutepunkt')]\n",
    "bsn['geometry'] = bsn.geometry.apply(wkt.loads)\n",
    "bsn = gpd.GeoDataFrame(bsn, geometry='geometry')\n",
    "bsn['lat'] = bsn.geometry.apply(lambda p: p.y)\n",
    "bsn['lon'] = bsn.geometry.apply(lambda p: p.x)\n",
    "\n",
    "\n",
    "# Find number of busstops within busstops_radius\n",
    "\n",
    "# SET THIS PARAMETER IN UNIT KM!\n",
    "busstops_radius = 0.5\n",
    "\n",
    "all_stores = stores_train.drop(columns=['revenue']).append(stores_test, ignore_index=True) #.append(stores_extra, ignore_index=True)\n",
    "\n",
    "revenue = stores_train[['store_id', 'revenue']]\n",
    "\n",
    "def busstops_within_radius(X, busstops, radius):\n",
    "    \n",
    "    X_copy = X.copy()\n",
    "    \n",
    "    col_name = ''.join(['n_busstops_within_', str(radius), 'km'])\n",
    "    \n",
    "    X_copy[col_name] = np.nan\n",
    "\n",
    "    mat = spatial.distance.cdist(X_copy[['lat','lon']], \n",
    "                          busstops[['lat','lon']], metric=haversine_vec)\n",
    "\n",
    "    new_df = pd.DataFrame(mat, index=X_copy['store_id'], columns=busstops['busstop_id'])\n",
    "\n",
    "    arr = new_df.values\n",
    "    \n",
    "    closest = np.where(arr <= (radius),new_df.columns,False)\n",
    "    # Get column names ignoring false.\n",
    "    X_copy[col_name] = [i[i.astype(bool)] for i in closest]\n",
    "    update_stores = X_copy['store_id'].unique()\n",
    "    print('hei')\n",
    "    for store in update_stores:\n",
    "        arry = X_copy.loc[X_copy['store_id'] == store, col_name]\n",
    "        #print(arry)\n",
    "        j = 0\n",
    "        for i in arry:\n",
    "            j = i\n",
    "            continue\n",
    "        #print(len(arry))\n",
    "        X_copy.loc[X_copy['store_id'] == store, col_name] = len(j)\n",
    "    return X_copy[['store_id', col_name]]\n",
    "\n",
    "busstops_within_radius = busstops_within_radius(all_stores, bsn, busstops_radius)\n",
    "\n",
    "all_stores = pd.merge(all_stores, busstops_within_radius, how='left', on='store_id')\n",
    "\n",
    "stores_train = all_stores[all_stores['store_id'].isin(stores_train['store_id'])]\n",
    "stores_test = all_stores[all_stores['store_id'].isin(stores_test['store_id'])]\n",
    "#stores_extra = all_stores[all_stores['store_id'].isin(stores_extra['store_id'])]\n",
    "stores_train = pd.merge(stores_train, revenue, how='left', on='store_id')\n",
    "busstops_within_radius\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores_train = pd.merge(stores_train, all_stores[[\"store_id\", \"rival_revenue\"]], how=\"left\", on=\"store_id\")\n",
    "#stores_test = pd.merge(stores_test, all_stores[[\"store_id\", \"rival_revenue\"]], how=\"left\", on=\"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               0                       1  \\\n",
      "grunnkrets_id                            6020303                 3010306   \n",
      "chain_name                             MCDONALDS               MCDONALDS   \n",
      "mall_name                      Magasinet Drammen                 no mall   \n",
      "lv1_desc                  Dining and Experiences  Dining and Experiences   \n",
      "lv2_desc                              Restaurant              Restaurant   \n",
      "lv3_desc                   Hamburger restaurants   Hamburger restaurants   \n",
      "lv4_desc                   Hamburger restaurants   Hamburger restaurants   \n",
      "district_name                  Bragernes sentrum               Sentrum 3   \n",
      "municipality_name                        Drammen                    Oslo   \n",
      "area_km2                                0.155779                0.264278   \n",
      "num_of_stores_grunnkrets                   153.0                   230.0   \n",
      "mall_distance                           0.001483                 0.00413   \n",
      "closest_mall_name              Magasinet Drammen                  Paleet   \n",
      "avg_rev_big_chain                       3.188035                3.188035   \n",
      "lv3_avg_rev                             2.096344                2.096344   \n",
      "lv4_avg_rev                             2.096344                2.096344   \n",
      "cluster                                        0                       1   \n",
      "center_distance                         1.653006               17.245357   \n",
      "revenue                                 2.944334                3.211972   \n",
      "\n",
      "                                               2  \n",
      "grunnkrets_id                            6050102  \n",
      "chain_name                           BURGER KING  \n",
      "mall_name                         Kuben Hønefoss  \n",
      "lv1_desc                  Dining and Experiences  \n",
      "lv2_desc                              Restaurant  \n",
      "lv3_desc                   Hamburger restaurants  \n",
      "lv4_desc                   Hamburger restaurants  \n",
      "district_name                           Hønefoss  \n",
      "municipality_name                      Ringerike  \n",
      "area_km2                                0.160152  \n",
      "num_of_stores_grunnkrets                    78.0  \n",
      "mall_distance                           0.000044  \n",
      "closest_mall_name                 Kuben Hønefoss  \n",
      "avg_rev_big_chain                       1.830112  \n",
      "lv3_avg_rev                             2.096344  \n",
      "lv4_avg_rev                             2.096344  \n",
      "cluster                                       -1  \n",
      "center_distance                  485165195.40979  \n",
      "revenue                                  2.83902  \n"
     ]
    }
   ],
   "source": [
    "#stores_train.loc[stores_train[\"cluster\"] == -1].head(10)\n",
    "\n",
    "#save id for l8r\n",
    "\n",
    "stores_test_id = np.asarray(stores_test.store_id)\n",
    "\n",
    "#stores_train = stores_train.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "remove_columns = [\n",
    "    'address',\n",
    "    'store_name',\n",
    "    'sales_channel_name',\n",
    "    'lat',\n",
    "    'lon',\n",
    "    #'Unnamed',\n",
    "    #'distance_rival',\n",
    "    'center_lat',\n",
    "    'center_lon',\n",
    "    'lv1',\n",
    "    'lv2',\n",
    "    'lv3',\n",
    "    'lv4',\n",
    "    'year',\n",
    "    'store_id',\n",
    "    'plaace_hierarchy_id',\n",
    "\n",
    "    #'stores_in_mall',\n",
    "    #'district_name',\n",
    "    #'people_per_store'\n",
    "]\n",
    "#print(stores_train.head(10))\n",
    "\n",
    "# data cleaning for train\n",
    "for column in remove_columns:\n",
    "    #print(column)\n",
    "    stores_train = remove_column(stores_train, column)\n",
    "    stores_test = remove_column(stores_test, column)\n",
    "#stores_train = remove_column(stores_train, 'grunnkrets_id')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stores_train = replace_missing(stores_train, 'district_name', 'No district')\n",
    "stores_train = replace_missing(stores_train, 'municipality_name' , \"No municipality_name\")\n",
    "stores_train = replace_missing(stores_train, 'area_km2' , 0.0)\n",
    "\n",
    "\n",
    "\n",
    "#stores_train = replace_missing(stores_train, 'stores_in_mall', 0.0)\n",
    "#stores_train = replace_missing(stores_train, 'chain_num', 0.0)\n",
    "#stores_train = replace_missing(stores_train, 'sum_people', 0.0)\n",
    "\n",
    "stores_test = replace_missing(stores_test, 'district_name', 'No district')\n",
    "stores_test = replace_missing(stores_test, 'municipality_name' , \"No municipality_name\")\n",
    "stores_test = replace_missing(stores_test, 'area_km2' , 0.0)\n",
    "\n",
    "stores_test = replace_missing(stores_test, \"lv3_avg_rev\", MEAN_REVENUE)\n",
    "stores_test = replace_missing(stores_test, \"lv4_avg_rev\", MEAN_REVENUE)\n",
    "\n",
    "\n",
    "\n",
    "#stores_train = replace_missing(stores_train, 'sum_people', 0.0)\n",
    "#eda.create_report(stores_train)\n",
    "\n",
    "\n",
    "\n",
    "#stores_train = remove_column(stores_train, \"plaace_hierarchy_id\")\n",
    "#stores_test  = remove_column(stores_test, \"plaace_hierarchy_id\")\n",
    "#stores_train = remove_column(stores_train, \"store_id\")\n",
    "#stores_test  = remove_column(stores_test, \"store_id\")\n",
    "\n",
    "\n",
    "print(stores_train.head(3).T)\n",
    "#eda.create_report(stores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['grunnkrets_id', 'chain_name', 'mall_name', 'lv1_desc', 'lv2_desc',\n",
       "       'lv3_desc', 'lv4_desc', 'district_name', 'municipality_name',\n",
       "       'area_km2', 'num_of_stores_grunnkrets', 'mall_distance',\n",
       "       'closest_mall_name', 'avg_rev_big_chain', 'lv3_avg_rev', 'lv4_avg_rev',\n",
       "       'cluster', 'center_distance', 'revenue'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stores_train.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "stores_test.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "stores_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating training data\n",
    "\n",
    "X_train = stores_train.drop(columns=['revenue'])\n",
    "y_train = stores_train.revenue\n",
    "X_test = stores_test\n",
    "\n",
    "# encode categorical features\n",
    "\n",
    "\n",
    "categorical_features = ['chain_name',\n",
    "                        'grunnkrets_id',\n",
    "                        'municipality_name',\n",
    "                        \"closest_mall_name\",\n",
    "                        'cluster',\n",
    "                        'lv1_desc',\n",
    "                        'lv2_desc',\n",
    "                        'lv3_desc',\n",
    "                        'lv4_desc',\n",
    "                        'mall_name',\n",
    "                        'district_name',\n",
    "                       ]\n",
    "\n",
    "for feature in categorical_features:\n",
    "    X_train[feature] = X_train[feature].astype('category')\n",
    "    X_test[feature] = X_test[feature].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoding for non-catboost models\n",
    "le = LabelEncoder()\n",
    "X_train_cat = X_train.copy()\n",
    "X_test_cat = X_test.copy()\n",
    "y_train_cat = y_train.copy()\n",
    "\n",
    "\n",
    "X_train_shape = X_train.shape[0]\n",
    "X_test_shape = X_test.shape[0]\n",
    "\n",
    "X_train[\"train_test_split\"] = [0 for i in range(X_train_shape)]\n",
    "X_test[\"train_test_split\"] = [1 for i in range(X_test_shape)]\n",
    "\n",
    "X_train_test = X_train.append(X_test, ignore_index=True)\n",
    "\n",
    "X_train_test[categorical_features] = X_train_test[categorical_features].apply(le.fit_transform)\n",
    "X_train = X_train_test.loc[X_train_test[\"train_test_split\"] == 0]\n",
    "X_test = X_train_test.loc[X_train_test[\"train_test_split\"] == 1]\n",
    "\n",
    "X_train=X_train.drop([\"train_test_split\"], axis=1)\n",
    "X_test=X_test.drop([\"train_test_split\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLEMetric(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        # the larger metric value the better\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "        preds = np.array(approxes[0])\n",
    "        target = np.array(target)\n",
    "        preds = np.exp(preds) - 1\n",
    "        target = np.exp(target) - 1\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i]<0:\n",
    "                preds[i] = 0\n",
    "        assert (target >= 0).all(), 'Received negative target values'\n",
    "        assert (preds >= 0).all(), 'Received negative pred values'\n",
    "        assert target.shape == preds.shape, 'target and pred have different shapes'\n",
    "        target_log1p = np.log1p(target)  # log(1 + y_true)\n",
    "        preds_log1p = np.log1p(preds)  # log(1 + y_pred)\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square(preds_log1p - target_log1p))), 0\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Logarithmic Error \n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): n-dimensional vector of ground-truth values \n",
    "        y_pred (np.array): n-dimensional vecotr of predicted values \n",
    "    \n",
    "    Returns:\n",
    "        A scalar float with the rmsle value \n",
    "    \n",
    "    Note: You can alternatively use sklearn and just do: \n",
    "        `sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5`\n",
    "    \"\"\"\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]<0:\n",
    "            y_pred[i] = 0\n",
    "    assert (y_true >= 0).all(), 'Received negative y_true values'\n",
    "    assert (y_pred >= 0).all(), 'Received negative y_pred values'\n",
    "    assert y_true.shape == y_pred.shape, 'y_true and y_pred have different shapes'\n",
    "    y_true = np.exp(y_true) - 1\n",
    "    y_pred = np.exp(y_pred) - 1\n",
    "    y_true_log1p = np.log1p(y_true)  # log(1 + y_true)\n",
    "    y_pred_log1p = np.log1p(y_pred)  # log(1 + y_pred)\n",
    "    print(np.sqrt(np.mean(np.square(y_pred_log1p - y_true_log1p))))\n",
    "    return np.sqrt(np.mean(np.square(y_pred_log1p - y_true_log1p)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eda.create_report(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m   - Of which 4 did not match due to:\n",
      "   Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "     With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m    Rejected as the implementation raised a specific error:\n",
      "      TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "    \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "     \n",
      "     >>> stub(array(float64, 1d, C))\n",
      "     \n",
      "    There are 2 candidate implementations:\n",
      "    \u001b[1m  - Of which 2 did not match due to:\n",
      "      Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "    \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "         TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "      raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "    \u001b[0m\n",
      "    \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "    \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "    \u001b[0m\n",
      "    \u001b[1m\n",
      "    File \"<string>\", line 3:\u001b[0m\n",
      "    \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1434626/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1434626/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "/lhome/frimanng/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "                 0\n",
      "count  8577.000000\n",
      "mean      6.464397\n",
      "std      10.490753\n",
      "min       0.467144\n",
      "25%       2.023454\n",
      "50%       3.218689\n",
      "75%       5.322310\n",
      "max      93.860871\n",
      "                 0\n",
      "count  8577.000000\n",
      "mean      6.444790\n",
      "std      10.647843\n",
      "min       0.600460\n",
      "25%       2.083118\n",
      "50%       3.323108\n",
      "75%       5.238721\n",
      "max      79.038356\n",
      "                 0\n",
      "count  8577.000000\n",
      "mean      6.326476\n",
      "std      10.033677\n",
      "min       0.359554\n",
      "25%       2.056633\n",
      "50%       3.260302\n",
      "75%       5.284761\n",
      "max      76.558748\n",
      "                 0\n",
      "count  8577.000000\n",
      "mean      6.390781\n",
      "std      10.375998\n",
      "min       0.528633\n",
      "25%       2.041240\n",
      "50%       3.245517\n",
      "75%       5.311893\n",
      "max      75.513023\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "\n",
    "cat_params =     {\n",
    "    \"random_state\":8,\n",
    "    \"learning_rate\":0.01,\n",
    "    \"max_depth\":8,\n",
    "    \"eval_metric\":RMSLEMetric(),\n",
    "    \"cat_features\":categorical_features,\n",
    "    \"n_estimators\":3000,\n",
    "    \"od_type\":\"Iter\",\n",
    "    \"silent\":True\n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    \"n_estimators\":100,\n",
    "    \"max_depth\":9,\n",
    "    \"random_state\":8,\n",
    "    \n",
    "    \"max_features\":None,\n",
    "}\n",
    "\n",
    "light_gbm_params = {\n",
    "    \"num_leaves\":8,\n",
    "    \"max_depth\":8, \n",
    "    \"random_state\":8,\n",
    "    \"silent\":True, \n",
    "    \"metric\":'rmsle',\n",
    "    \"n_jobs\":-1,\n",
    "    #\"min_data_in_leaf\": 20,\n",
    "    \"lambda_l1\": 0.5,\n",
    "    \"n_estimators\":1500,\n",
    "    \"colsample_bytree\":0.95,\n",
    "    \"subsample\":0.2,\n",
    "    \"learning_rate\":0.008,\n",
    "}\n",
    "xgboost_params = {\n",
    "            'lambda': 2.5 ,#2.5463783373378037,\n",
    "             'alpha': 0.0023357293126326307,\n",
    "             'colsample_bytree': 1.0,\n",
    "             'subsample': 0.6, #0.6,\n",
    "             'learning_rate': 0.005, #0.012,\n",
    "             'max_depth': 5, #4,\n",
    "             'random_state': 8,\n",
    "             'min_child_weight': 26,\n",
    "             #\"enable_categorical\":\"True\",\n",
    "            'n_estimators': 1500}\n",
    "\n",
    "model_catboost= CatBoostRegressor(**cat_params)\n",
    "model_random_forest = RandomForestRegressor(**random_forest_params)\n",
    "model_light_gbm = lgb.LGBMRegressor(**light_gbm_params)\n",
    "model_xgboost = xgb.XGBRegressor(**xgboost_params)\n",
    "\n",
    "model_catboost.fit(X_train, y_train)\n",
    "model_random_forest.fit(X_train, y_train)\n",
    "model_light_gbm.fit(X_train, y_train)\n",
    "model_xgboost.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_random_forest.fit(X_train, y_train)\n",
    "# print(\"fitted\")\n",
    "# cv = KFold(n_splits = 4, random_state=42, shuffle=True)\n",
    "# score = cross_val_score(model_random_forest, X_train, y_train, cv=cv, scoring=make_scorer(rmsle))\n",
    "\n",
    "# print(\"scores:\",score,\"\\n\")\n",
    "# print(\"mean:\\n\",np.mean(score))\n",
    "\n",
    "pred_1 = np.exp(model_catboost.predict(X_test)) -1\n",
    "pred_2 = np.exp(model_random_forest.predict(X_test)) -1\n",
    "pred_3 = np.exp(model_light_gbm.predict(X_test)) -1\n",
    "pred_4 = np.exp(model_xgboost.predict(X_test)) -1\n",
    "\n",
    "# print(pred[:12], \"\\n\", pred[-5:])\n",
    "''' \n",
    "xgboost:\n",
    "\n",
    "n_estimators:\n",
    "2000 - 0.6798\n",
    "1000 - 0.6798\n",
    "\n",
    "lambda:\n",
    "2.54.. - 0.67977\n",
    "1 - 0.67980 \n",
    "3 - 0.67984\n",
    "2 - 0.\n",
    "2.5 - 0.67975\n",
    "\n",
    "subsample:\n",
    "1 - 0.682\n",
    "0.5 - 0.67984\n",
    "0.7 - 0.67988\n",
    "0.6 - 0.67975\n",
    "\n",
    "\n",
    "estimators: \n",
    "4000 - 6.8...\n",
    "1500 - 0.6791\n",
    "'''\n",
    "preds=[pred_1,pred_2,pred_3,pred_4]\n",
    "for i in range(4):\n",
    "    print(pd.DataFrame(preds[i]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>xgboost_importance</th>\n",
       "      <th>Ligt_gbm_importance</th>\n",
       "      <th>CatBoost_importance</th>\n",
       "      <th>Random_Forest_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avg_rev_big_chain</td>\n",
       "      <td>0.514971</td>\n",
       "      <td>1535</td>\n",
       "      <td>26.809333</td>\n",
       "      <td>0.688675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lv4_avg_rev</td>\n",
       "      <td>0.196445</td>\n",
       "      <td>608</td>\n",
       "      <td>6.290917</td>\n",
       "      <td>0.117929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lv3_avg_rev</td>\n",
       "      <td>0.039564</td>\n",
       "      <td>670</td>\n",
       "      <td>2.738361</td>\n",
       "      <td>0.014992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mall_name</td>\n",
       "      <td>0.025018</td>\n",
       "      <td>388</td>\n",
       "      <td>3.353332</td>\n",
       "      <td>0.006927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>center_distance</td>\n",
       "      <td>0.024789</td>\n",
       "      <td>864</td>\n",
       "      <td>2.740349</td>\n",
       "      <td>0.025299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>num_of_stores_grunnkrets</td>\n",
       "      <td>0.023212</td>\n",
       "      <td>918</td>\n",
       "      <td>5.632760</td>\n",
       "      <td>0.023312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lv1_desc</td>\n",
       "      <td>0.022900</td>\n",
       "      <td>142</td>\n",
       "      <td>3.223451</td>\n",
       "      <td>0.000888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mall_distance</td>\n",
       "      <td>0.020345</td>\n",
       "      <td>1082</td>\n",
       "      <td>5.243051</td>\n",
       "      <td>0.034782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lv3_desc</td>\n",
       "      <td>0.015965</td>\n",
       "      <td>503</td>\n",
       "      <td>4.588549</td>\n",
       "      <td>0.003524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lv2_desc</td>\n",
       "      <td>0.015618</td>\n",
       "      <td>336</td>\n",
       "      <td>4.594739</td>\n",
       "      <td>0.004452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cluster</td>\n",
       "      <td>0.014619</td>\n",
       "      <td>173</td>\n",
       "      <td>4.694064</td>\n",
       "      <td>0.004175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>grunnkrets_id</td>\n",
       "      <td>0.013666</td>\n",
       "      <td>783</td>\n",
       "      <td>2.996630</td>\n",
       "      <td>0.016187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>area_km2</td>\n",
       "      <td>0.013502</td>\n",
       "      <td>837</td>\n",
       "      <td>3.590881</td>\n",
       "      <td>0.019680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lv4_desc</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>316</td>\n",
       "      <td>5.935219</td>\n",
       "      <td>0.003406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>closest_mall_name</td>\n",
       "      <td>0.012230</td>\n",
       "      <td>463</td>\n",
       "      <td>5.704265</td>\n",
       "      <td>0.014185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>municipality_name</td>\n",
       "      <td>0.011650</td>\n",
       "      <td>498</td>\n",
       "      <td>4.542875</td>\n",
       "      <td>0.008222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>district_name</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>280</td>\n",
       "      <td>4.073142</td>\n",
       "      <td>0.011264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>chain_name</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>104</td>\n",
       "      <td>3.248082</td>\n",
       "      <td>0.002099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Features  xgboost_importance  Ligt_gbm_importance  \\\n",
       "0          avg_rev_big_chain            0.514971                 1535   \n",
       "1                lv4_avg_rev            0.196445                  608   \n",
       "2                lv3_avg_rev            0.039564                  670   \n",
       "3                  mall_name            0.025018                  388   \n",
       "4            center_distance            0.024789                  864   \n",
       "5   num_of_stores_grunnkrets            0.023212                  918   \n",
       "6                   lv1_desc            0.022900                  142   \n",
       "7              mall_distance            0.020345                 1082   \n",
       "8                   lv3_desc            0.015965                  503   \n",
       "9                   lv2_desc            0.015618                  336   \n",
       "10                   cluster            0.014619                  173   \n",
       "11             grunnkrets_id            0.013666                  783   \n",
       "12                  area_km2            0.013502                  837   \n",
       "13                  lv4_desc            0.013384                  316   \n",
       "14         closest_mall_name            0.012230                  463   \n",
       "15         municipality_name            0.011650                  498   \n",
       "16             district_name            0.011519                  280   \n",
       "17                chain_name            0.010603                  104   \n",
       "\n",
       "    CatBoost_importance  Random_Forest_importance  \n",
       "0             26.809333                  0.688675  \n",
       "1              6.290917                  0.117929  \n",
       "2              2.738361                  0.014992  \n",
       "3              3.353332                  0.006927  \n",
       "4              2.740349                  0.025299  \n",
       "5              5.632760                  0.023312  \n",
       "6              3.223451                  0.000888  \n",
       "7              5.243051                  0.034782  \n",
       "8              4.588549                  0.003524  \n",
       "9              4.594739                  0.004452  \n",
       "10             4.694064                  0.004175  \n",
       "11             2.996630                  0.016187  \n",
       "12             3.590881                  0.019680  \n",
       "13             5.935219                  0.003406  \n",
       "14             5.704265                  0.014185  \n",
       "15             4.542875                  0.008222  \n",
       "16             4.073142                  0.011264  \n",
       "17             3.248082                  0.002099  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "\n",
    "\n",
    "print(\"#\")\n",
    "'''\n",
    "features_for_model = [f for f in X_train]\n",
    "feat_import = [t for t in zip(features_for_model,model_random_forest.feature_importances_)]\n",
    "feat_import_df = pd.DataFrame(feat_import, columns = ['Features', 'Random_Forest_importance'])\n",
    "df_rf =feat_import_df.sort_values(\"Random_Forest_importance\", ascending= False)\n",
    "\n",
    "features_for_model = [f for f in X_train]\n",
    "feat_import = [t for t in zip(features_for_model,model_catboost.get_feature_importance())]\n",
    "feat_import_df = pd.DataFrame(feat_import, columns = ['Features', 'CatBoost_importance'])\n",
    "df_cat =feat_import_df.sort_values(\"CatBoost_importance\", ascending= False)\n",
    "df_cat\n",
    "\n",
    "\n",
    "\n",
    "features_for_model = [f for f in X_train]\n",
    "feat_import = [t for t in zip(features_for_model,model_light_gbm.feature_importances_)]\n",
    "feat_import_df = pd.DataFrame(feat_import, columns = ['Features', 'Ligt_gbm_importance'])\n",
    "df_lgb =feat_import_df.sort_values(\"Ligt_gbm_importance\", ascending= False)\n",
    "df_lgb = pd.merge(df_lgb,df_cat, how=\"left\", on=\"Features\")\n",
    "\n",
    "df_lgb\n",
    "\n",
    "features_for_model = [f for f in X_train]\n",
    "feat_import = [t for t in zip(features_for_model,model_xgboost.feature_importances_)]\n",
    "feat_import_df = pd.DataFrame(feat_import, columns = ['Features', 'xgboost_importance'])\n",
    "df_xgb =feat_import_df.sort_values(\"xgboost_importance\", ascending= False)\n",
    "df = pd.merge(df_xgb,df_lgb,how=\"left\", on=\"Features\")\n",
    "df = pd.merge(df, df_rf, how=\"left\", on=\"Features\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "0.6695972314836428\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "0.665783704608548\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "0.6782971922195811\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "0.6729265176710479\n",
      "0.6741360410481396\n",
      "0.6731674382501849\n",
      "0.6831965392934801\n",
      "0.6724105760218367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m      - Of which 4 did not match due to:\n",
      "      Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "       \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "        \n",
      "        >>> stub(array(float64, 1d, C))\n",
      "        \n",
      "       There are 2 candidate implementations:\n",
      "       \u001b[1m  - Of which 2 did not match due to:\n",
      "         Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "           With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "       \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "            TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "         raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "       \u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "       \u001b[0m\n",
      "       \u001b[1m\n",
      "       File \"<string>\", line 3:\u001b[0m\n",
      "       \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1434626/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1434626/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6668964569929188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m      - Of which 4 did not match due to:\n",
      "      Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "       \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "        \n",
      "        >>> stub(array(float64, 1d, C))\n",
      "        \n",
      "       There are 2 candidate implementations:\n",
      "       \u001b[1m  - Of which 2 did not match due to:\n",
      "         Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "           With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "       \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "            TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "         raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "       \u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "       \u001b[0m\n",
      "       \u001b[1m\n",
      "       File \"<string>\", line 3:\u001b[0m\n",
      "       \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1434626/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1434626/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6638102163036352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m      - Of which 4 did not match due to:\n",
      "      Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "       \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "        \n",
      "        >>> stub(array(float64, 1d, C))\n",
      "        \n",
      "       There are 2 candidate implementations:\n",
      "       \u001b[1m  - Of which 2 did not match due to:\n",
      "         Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "           With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "       \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "            TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "         raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "       \u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "       \u001b[0m\n",
      "       \u001b[1m\n",
      "       File \"<string>\", line 3:\u001b[0m\n",
      "       \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1434626/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1434626/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6741096068033473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m      - Of which 4 did not match due to:\n",
      "      Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "       \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "        \n",
      "        >>> stub(array(float64, 1d, C))\n",
      "        \n",
      "       There are 2 candidate implementations:\n",
      "       \u001b[1m  - Of which 2 did not match due to:\n",
      "         Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "           With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "       \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "            TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "         raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "       \u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "       \u001b[0m\n",
      "       \u001b[1m\n",
      "       File \"<string>\", line 3:\u001b[0m\n",
      "       \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1434626/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1434626/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.661792312953942\n",
      "0.6675319038673946\n",
      "0.6653077661447461\n",
      "0.6750166554228\n",
      "0.669309114205358\n",
      "Scores for all folds: \n",
      "lgbm: [0.66959723 0.6657837  0.67829719 0.67292652]\n",
      "cat: [0.66689646 0.66381022 0.67410961 0.66179231]\n",
      "xg boost: [0.6675319  0.66530777 0.67501666 0.66930911]\n",
      "random forest: [0.67413604 0.67316744 0.68319654 0.67241058]\n",
      "\n",
      "\n",
      "Mean of scores:\n",
      "lgbm: 0.6716511614957048\n",
      "cat: 0.6666521482634609\n",
      "xg boost: 0.6692913599100747\n",
      "random forest: 0.6757276486534103\n",
      "\n",
      "\n",
      "Standard deviation of scores: \n",
      "0.004594614689334306\n",
      "0.004673541246456987\n",
      "0.003596666122130395\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits = 4, random_state=42, shuffle=True)\n",
    "score_lgbm = cross_val_score(model_light_gbm, X_train, y_train, cv=cv, scoring=make_scorer(rmsle))\n",
    "score_random_forest = cross_val_score(model_random_forest, X_train, y_train, cv=cv, scoring=make_scorer(rmsle))\n",
    "score_cat = cross_val_score(model_catboost, X_train, y_train, cv=cv, scoring=make_scorer(rmsle))\n",
    "score_xgboost = cross_val_score(model_xgboost, X_train, y_train, cv=cv, scoring=make_scorer(rmsle))\n",
    "\n",
    "print('Scores for all folds: ')\n",
    "print(\"lgbm:\", score_lgbm)\n",
    "print(\"cat:\", score_cat)\n",
    "print(\"xg boost:\", score_xgboost)\n",
    "print(\"random forest:\", score_random_forest)\n",
    "print('\\n')\n",
    "print('Mean of scores:')\n",
    "print(\"lgbm:\", np.mean(score_lgbm))\n",
    "print(\"cat:\", np.mean(score_cat))\n",
    "print(\"xg boost:\", np.mean(score_xgboost))\n",
    "print(\"random forest:\", np.mean(score_random_forest))\n",
    "print('\\n')\n",
    "print('Standard deviation of scores: ')\n",
    "print(np.std(score_lgbm))\n",
    "print(np.std(score_cat))\n",
    "print(np.std(score_xgboost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of scores:\n",
      "lgbm: 0.6716511614957048\n",
      "cat: 0.6666521482634609\n",
      "xg boost: 0.6692913599100747\n",
      "random forest: 0.6757276486534103\n"
     ]
    }
   ],
   "source": [
    "print('Mean of scores:')\n",
    "print(\"lgbm:\", np.mean(score_lgbm))\n",
    "print(\"cat:\", np.mean(score_cat))\n",
    "print(\"xg boost:\", np.mean(score_xgboost))\n",
    "print(\"random forest:\", np.mean(score_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.03669231  5.76493326  5.25375659  6.77026178 31.06777182  6.03123552\n",
      " 20.35538613  6.55554674 21.67773028 19.20102285  4.79972115  4.73765937]\n",
      "6.464396694313658\n",
      "[ 3.98361459  6.58738719  5.63518013  6.96839946 29.36479051  5.8380423\n",
      " 25.89240166  6.78107016 25.40994032 25.42546808  5.65797146  5.49791283]\n",
      "6.326476155790505\n",
      "[ 3.93491893  6.50860087  4.54215024  7.01963164 30.61977756  5.64603818\n",
      " 28.21770013  7.17047891 26.29401174 25.03039636  5.67054537  4.77274716]\n",
      "6.390769885543436\n",
      "[ 3.95405003  6.21309296  4.51043676  6.47797904 29.9116488   4.89288311\n",
      " 26.7265181   6.12325547 23.85612631 27.37715527  4.33000068  5.07873032]\n",
      "6.444790037063013\n"
     ]
    }
   ],
   "source": [
    "# mean of scores\n",
    "# 0.6781866272821699 - 11-12-123108 - kaggle: 0.68349\n",
    "\n",
    "# mean of scores 3- stack\n",
    "# 6781866272821699 / 0.6839925788085071 / 0.679119033159766 - 11-12-132139 - kaggle: 0.68413\n",
    "\n",
    "# mean of scores models:\n",
    "# 0.6976428658736472 - random forest - kaggle: 0.70203\n",
    "# 0.6857052809131627 - random forest - new hyper params\n",
    "\n",
    "# mean of scores after adding rival_revenue\n",
    "# lgbm: 0.6824655631333725 \n",
    "# cat:  0.6769482640447066 - with mean => kaggle: 0.68335\n",
    "\n",
    "# mean of scores after removing rival_revenue, adding rival_distance\n",
    "# lgbm: 0.6819900731158717\n",
    "# cat: 0.6778082165931306 - with mean => kaggle: 0.68290\n",
    "\n",
    "\n",
    "# Mean of scores: - changed avg avg to not >3\n",
    "# lgbm: 0.6726921524863859\n",
    "# cat: 0.6695708730130597\n",
    "# xg boost: 0.6711098372410809\n",
    "\n",
    "\n",
    "# Mean of scores: - changed outliers of removing rev>150\n",
    "# lgbm: 0.6716511614957048\n",
    "# cat: 0.6666521482634609\n",
    "# xg boost: 0.6692913599100747\n",
    "# random forest: 0.6757276486534103\n",
    "\n",
    "# switched avg_rev_chain back to >3, removing overall rev > 100 (removed around 70 stores, where 40 are rema 1000)\n",
    "# Mean of scores:\n",
    "# lgbm: 0.6752373567799875\n",
    "# cat: 0.6725937329378754\n",
    "# xg boost: 0.672993772025054\n",
    "# random forest: 0.6794574168532559\n",
    "\n",
    "# Mean of scores: #removed rev>135, added bus stops\n",
    "# lgbm: 0.6785290668037811\n",
    "# cat: 0.6741125811018365\n",
    "# xg boost: 0.6753203790091535\n",
    "# random forest: 0.6820343581205036\n",
    "\n",
    "# Mean of scores:  => busstop distance =500meters\n",
    "# lgbm: 0.6785512739369779\n",
    "# cat: 0.6740236184052476\n",
    "# xg boost: 0.675544169615468\n",
    "# random forest: 0.6819594860144613\n",
    "\n",
    "\n",
    "X_test = replace_missing(X_test, \"lv3_avg_rev\", MEAN_REVENUE)\n",
    "X_test = replace_missing(X_test, \"lv4_avg_rev\", MEAN_REVENUE)\n",
    "\n",
    "\n",
    "catboost_prediction = model_catboost.predict(X_test)\n",
    "light_gbm_prediction= model_light_gbm.predict(X_test)\n",
    "random_forest_prediction = model_random_forest.predict(X_test)\n",
    "xgboost_prediction = model_xgboost.predict(X_test)\n",
    "\n",
    "#all_predictions= np.array([random_forest_prediction])#np.array([catboost_prediction, light_gbm_prediction, random_forest_prediction])\n",
    "all_predictions = np.array([catboost_prediction, light_gbm_prediction, xgboost_prediction, random_forest_prediction])\n",
    "for prd in all_predictions:\n",
    "    print( np.exp(prd[:12]) -1 )\n",
    "    print(np.mean(np.exp(prd)-1))\n",
    "    #print(pd.DataFrame(np.exp(prd) -1 ).describe())\n",
    "\n",
    "\n",
    "\n",
    "final_prediction = np.mean( all_predictions , axis=0)\n",
    "for i in range(len(final_prediction)):\n",
    "    if final_prediction[i]<0:\n",
    "        final_prediction[i] = 0\n",
    "        \n",
    "final_prediction = np.exp(final_prediction) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            id  predicted\n",
      "0   914206820-914239427-717245   3.709260\n",
      "1   916789157-916823770-824309   6.261211\n",
      "2     913341082-977479363-2948   4.966429\n",
      "3    889682582-889697172-28720   6.806140\n",
      "4   997991699-998006945-417222  30.234173\n",
      "5   914931487-815162862-756427   5.587392\n",
      "6     967062979-972338656-6209  25.116778\n",
      "7   914631734-914748119-740036   6.648175\n",
      "8    970976361-973961837-23171  24.247417\n",
      "9    979425031-979584385-54031  24.058866\n",
      "10  914852625-914864976-744489   5.087086\n",
      "11  916756097-816761972-821991   5.014174                               id  predicted\n",
      "8572  917323003-917383529-844309   7.364807\n",
      "8573  917353379-917411824-845904   3.414573\n",
      "8574  917072302-917089248-833647   7.652538\n",
      "8575  916960557-916993161-829908   2.555419\n",
      "8576   987280891-972040746-45320   2.717629\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.383359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.298164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.579663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.064235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.262634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.242266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>77.512161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predicted\n",
       "count  8577.000000\n",
       "mean      6.383359\n",
       "std      10.298164\n",
       "min       0.579663\n",
       "25%       2.064235\n",
       "50%       3.262634\n",
       "75%       5.242266\n",
       "max      77.512161"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate .csv-submission\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = stores_test_id\n",
    "submission['predicted'] = np.asarray(final_prediction)\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now() # current date and time\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "path = '../predictions'\n",
    "prefix = 'submission'\n",
    "suffix = '.csv'\n",
    "\n",
    "filename = \"\".join([\"-\".join([prefix,timestamp]),suffix])\n",
    "\n",
    "#eda.create_report(submission)\n",
    "#submission.to_csv(\"/\".join([path,filename]), index=False)\n",
    "\n",
    "print(submission[:12], submission[-5:])\n",
    "submission.describe()\n",
    "\n",
    "\n",
    "# same as submission-2022-11-13-155127"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
