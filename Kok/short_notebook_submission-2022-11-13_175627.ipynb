{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Group 62] Machines of ML\n",
    "\n",
    "Frimann Bergvik Garmann         - 527245\n",
    "Brage Bergsmyr                  - 514881\n",
    "Magnus Christian Kvist Jacobsen - 506626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install catboost\n",
    "!pip install geopandas\n",
    "!pip install folium\n",
    "!pip install matplotlib\n",
    "!pip install mapclassify\n",
    "!pip install shapely\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install xgboost\n",
    "!pip install catboost\n",
    "!pip install scipy\n",
    "!pip install lightgbm\n",
    "!pip install h2o\n",
    "!pip install pandas\n",
    "!pip install dataprep\n",
    "!pip install geopy\n",
    "!pip install fiona\n",
    "!pip install pyproj\n",
    "!pip install packaging\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import math\n",
    "import geopy.distance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataprep import eda\n",
    "import xgboost as xgb\n",
    "from shapely.geometry import Point\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_log_error\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "busstops_norway                     = pd.read_csv('../data/busstops_norway.csv')\n",
    "grunnkrets_age_distribution         = pd.read_csv('../data/grunnkrets_age_distribution.csv')\n",
    "grunnkrets_households_num_persons   = pd.read_csv('../data/grunnkrets_households_num_persons.csv')\n",
    "grunnkrets_income_households        = pd.read_csv('../data/grunnkrets_income_households.csv')\n",
    "grunnkrets_norway_stripped          = pd.read_csv('../data/grunnkrets_norway_stripped.csv')\n",
    "plaace_hierarchy                    = pd.read_csv('../data/plaace_hierarchy.csv')\n",
    "sample_submission                   = pd.read_csv('../data/sample_submission.csv')\n",
    "stores_extra                        = pd.read_csv('../data/stores_extra.csv')\n",
    "stores_test                         = pd.read_csv('../data/stores_test.csv')\n",
    "stores_train                        = pd.read_csv('../data/stores_train.csv')\n",
    "\n",
    "#eda.create_report(stores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def left_merge(X, Z, on):\n",
    "    return pd.merge(X, Z, how='left', on=on)\n",
    "\n",
    "def replace_missing(X, column, replacement):\n",
    "    _X = X.copy()\n",
    "    _X[column] = _X[column].replace(np.nan, replacement)\n",
    "    return _X\n",
    "\n",
    "def remove_nan_rows(X, column):\n",
    "    _X = X.copy()\n",
    "    return _X[_X[column].notna()]\n",
    "\n",
    "def remove_column(X, column):\n",
    "    _X = X.copy()\n",
    "    return _X.drop(column, axis=1)\n",
    "\n",
    "def remove_year(X):\n",
    "    X_copy = X.copy()\n",
    "    columns = X.columns\n",
    "    X_copy = X_copy.loc[X_copy.groupby('grunnkrets_id')['year'].idxmax()]\n",
    "    X_copy = X_copy.drop(columns=['year'])\n",
    "    #X_copy = X_copy.groupby(['grunnkrets_id'], as_index=False).agg(dict)\n",
    "    return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature functions\n",
    "def sum_people(X):\n",
    "    y_grunnkrets = X.grunnkrets_id\n",
    "    X_copy = X.drop(columns=['grunnkrets_id'])\n",
    "    X_sum = X_copy.sum(axis=1)\n",
    "    return pd.merge(y_grunnkrets.rename('grunnkrets_id'), X_sum.rename('sum_people'), left_index=True, right_index=True)\n",
    "\n",
    "def sum_stores(X):\n",
    "    X_sum = X['grunnkrets_id'].groupby('grunnkrets_id').sum()\n",
    "    return X_sum\n",
    "\n",
    "def jeg_er_lei_meg(X):\n",
    "    # Adds a num_of_stores to the district\n",
    "\n",
    "    districts=X.loc[:,\"district_name\"].unique()\n",
    "    dict={\"district_name\":[],\"num_of_stores\":[]}\n",
    "    for district in districts:\n",
    "        num_of_stores=(X.loc[X[\"district_name\"] == district])[\"store_id\"].count()\n",
    "        dict[\"district_name\"]+=[district]\n",
    "        dict[\"num_of_stores\"]+=[num_of_stores]\n",
    "    return pd.DataFrame.from_dict(dict)\n",
    "\n",
    "\n",
    "def clostest_rival(df, shop_id):\n",
    "    #hente ut samme hierarky\n",
    "    #sammenligne distansen til nærmeste\n",
    "\n",
    "    hierarchy = df.loc[df[\"store_id\"] == shop_id][\"plaace_hierarchy_id\"].values\n",
    "    coords_1 = df.loc[df[\"store_id\"] == shop_id][[\"lon\",\"lat\"]].values\n",
    "    entry_index= df.loc[df[\"store_id\"] == shop_id].index\n",
    "    \n",
    "    check = df.loc[df[\"plaace_hierarchy_id\"] == hierarchy[0]]\n",
    "    #print(check)\n",
    "    distance=math.inf\n",
    "    best_row=None\n",
    "    #finds=\"\"\n",
    "    for index, row in check.iterrows():\n",
    "        coords_2 = row[[\"lon\",\"lat\"]].values\n",
    "        current_distance= geopy.distance.geodesic(coords_1, coords_2).km\n",
    "\n",
    "        if (current_distance < distance and index != entry_index):\n",
    "            distance=current_distance\n",
    "            best_row=row\n",
    "     #       finds+=str(row[\"store_id\"])\n",
    "    #print(distance, \"finds\",finds)\n",
    "    return distance, best_row\n",
    "\n",
    "\n",
    "def add_avg_revenue_municipality(X): #ALL MY HOMIES HATE THIS FUNCTION\n",
    "    districts=X.loc[:,\"municipality_name\"].unique()\n",
    "    dict={\"municipality_name\":[],\"mean\":[]}\n",
    "    for district in districts:\n",
    "        mean = (X.loc[X[\"municipality_name\"] == district])[\"revenue\"].mean()\n",
    "        dict[\"district_name\"]+=[district]\n",
    "        dict[\"mean\"]+=[mean]\n",
    "    df= pd.DataFrame.from_dict(dict)\n",
    "    print(df)\n",
    "    bass = pd.merge(X, df, how=\"left\", on=\"municipality_name\")\n",
    "    X[\"avg_revenue_municipality\"] = bass[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12859, 12)\n",
      "(12610, 12)\n",
      "(12592, 12)\n",
      "1.6144287126764343\n"
     ]
    }
   ],
   "source": [
    "# remove rows with revenue outliers\n",
    "print(stores_train.shape)\n",
    "stores_train = stores_train[stores_train['revenue'] > 0.01]\n",
    "print(stores_train.shape)\n",
    "stores_train = stores_train[stores_train['revenue'] < 135.0] # 180 for å fjerne øverste 5\n",
    "print(stores_train.shape)\n",
    "\n",
    "stores_train['revenue'] = np.log1p(stores_train['revenue'])\n",
    "\n",
    "#Saves mean revenue to fill in nans later\n",
    "MEAN_REVENUE = stores_train[\"revenue\"].mean()\n",
    "print(MEAN_REVENUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>year</th>\n",
       "      <th>store_name</th>\n",
       "      <th>plaace_hierarchy_id</th>\n",
       "      <th>sales_channel_name</th>\n",
       "      <th>grunnkrets_id</th>\n",
       "      <th>address</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>chain_name</th>\n",
       "      <th>mall_name</th>\n",
       "      <th>lv1</th>\n",
       "      <th>lv1_desc</th>\n",
       "      <th>lv2</th>\n",
       "      <th>lv2_desc</th>\n",
       "      <th>lv3</th>\n",
       "      <th>lv3_desc</th>\n",
       "      <th>lv4</th>\n",
       "      <th>lv4_desc</th>\n",
       "      <th>district_name</th>\n",
       "      <th>municipality_name</th>\n",
       "      <th>area_km2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>914206820-914239427-717245</td>\n",
       "      <td>2016</td>\n",
       "      <td>VÅLERENGA HALAL BURGER AS</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>3012704</td>\n",
       "      <td>STRØMSVEIEN 25 A</td>\n",
       "      <td>59.908672</td>\n",
       "      <td>10.787031</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Dining and Experiences</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>Vålerenga</td>\n",
       "      <td>Oslo</td>\n",
       "      <td>0.057027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>916789157-916823770-824309</td>\n",
       "      <td>2016</td>\n",
       "      <td>BURGER KING MYREN</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>8061401</td>\n",
       "      <td>MYREN 1</td>\n",
       "      <td>59.201467</td>\n",
       "      <td>9.588243</td>\n",
       "      <td>BURGER KING</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Dining and Experiences</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>Gulset</td>\n",
       "      <td>Skien</td>\n",
       "      <td>0.165993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>913341082-977479363-2948</td>\n",
       "      <td>2016</td>\n",
       "      <td>BURGER KING STOVNER</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>3013917</td>\n",
       "      <td>STOVNER SENTER 3</td>\n",
       "      <td>59.962146</td>\n",
       "      <td>10.924524</td>\n",
       "      <td>BURGER KING</td>\n",
       "      <td>Stovner Senter</td>\n",
       "      <td>1</td>\n",
       "      <td>Dining and Experiences</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>1.1.1</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>1.1.1.0</td>\n",
       "      <td>Hamburger restaurants</td>\n",
       "      <td>Fossum</td>\n",
       "      <td>Oslo</td>\n",
       "      <td>0.236628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     store_id  year                 store_name  \\\n",
       "0  914206820-914239427-717245  2016  VÅLERENGA HALAL BURGER AS   \n",
       "1  916789157-916823770-824309  2016          BURGER KING MYREN   \n",
       "2    913341082-977479363-2948  2016        BURGER KING STOVNER   \n",
       "\n",
       "  plaace_hierarchy_id     sales_channel_name  grunnkrets_id           address  \\\n",
       "0             1.1.1.0  Hamburger restaurants        3012704  STRØMSVEIEN 25 A   \n",
       "1             1.1.1.0  Hamburger restaurants        8061401           MYREN 1   \n",
       "2             1.1.1.0  Hamburger restaurants        3013917  STOVNER SENTER 3   \n",
       "\n",
       "         lat        lon   chain_name       mall_name  lv1  \\\n",
       "0  59.908672  10.787031          NaN             NaN    1   \n",
       "1  59.201467   9.588243  BURGER KING             NaN    1   \n",
       "2  59.962146  10.924524  BURGER KING  Stovner Senter    1   \n",
       "\n",
       "                 lv1_desc  lv2    lv2_desc    lv3               lv3_desc  \\\n",
       "0  Dining and Experiences  1.1  Restaurant  1.1.1  Hamburger restaurants   \n",
       "1  Dining and Experiences  1.1  Restaurant  1.1.1  Hamburger restaurants   \n",
       "2  Dining and Experiences  1.1  Restaurant  1.1.1  Hamburger restaurants   \n",
       "\n",
       "       lv4               lv4_desc district_name municipality_name  area_km2  \n",
       "0  1.1.1.0  Hamburger restaurants     Vålerenga              Oslo  0.057027  \n",
       "1  1.1.1.0  Hamburger restaurants        Gulset             Skien  0.165993  \n",
       "2  1.1.1.0  Hamburger restaurants        Fossum              Oslo  0.236628  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plaace_hierarchy=plaace_hierarchy.drop(columns=[\"sales_channel_name\"])\n",
    "\n",
    "stores_train=left_merge(stores_train, plaace_hierarchy, on=\"plaace_hierarchy_id\")\n",
    "stores_test=left_merge(stores_test, plaace_hierarchy, on=\"plaace_hierarchy_id\")\n",
    "\n",
    "# removing year\n",
    "grunnkrets_age_distribution = remove_year(grunnkrets_age_distribution)\n",
    "grunnkrets_households_num_persons = remove_year(grunnkrets_households_num_persons)\n",
    "grunnkrets_income_households = remove_year(grunnkrets_income_households)\n",
    "grunnkrets_norway_stripped = remove_year(grunnkrets_norway_stripped)\n",
    "\n",
    "stores_train = left_merge(stores_train, grunnkrets_norway_stripped[['grunnkrets_id', 'district_name', \"municipality_name\", \"area_km2\"]], on='grunnkrets_id')\n",
    "stores_test = left_merge(stores_test, grunnkrets_norway_stripped[['grunnkrets_id', 'district_name',\"municipality_name\", \"area_km2\"]], on='grunnkrets_id')\n",
    "\n",
    "stores_test.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaces missing values\n",
    "stores_train = replace_missing(stores_train, \"chain_name\", \"no chain\")\n",
    "stores_train = replace_missing(stores_train, \"mall_name\", \"no mall\")\n",
    "stores_train = replace_missing(stores_train, 'district_name', 'No district')\n",
    "stores_train = replace_missing(stores_train, 'municipality_name' , \"No municipality_name\")\n",
    "stores_train = replace_missing(stores_train, 'area_km2' , 0.0)\n",
    "\n",
    "\n",
    "\n",
    "stores_test = replace_missing(stores_test, \"chain_name\", \"no chain\")\n",
    "stores_test = replace_missing(stores_test, \"mall_name\", \"no mall\")\n",
    "stores_test = replace_missing(stores_test, 'district_name', 'No district')\n",
    "stores_test = replace_missing(stores_test, 'municipality_name' , \"No municipality_name\")\n",
    "stores_test = replace_missing(stores_test, 'area_km2' , 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds num stores grunnkrets\n",
    "def find_num_stores_grunnkrets(X):\n",
    "    grunnkretser=X.loc[:,\"grunnkrets_id\"].unique()\n",
    "    dict={\"grunnkrets_id\":[],\"num_of_stores_grunnkrets\":[]}\n",
    "    for grunnkrets in grunnkretser:\n",
    "        num_of_stores = (X.loc[X[\"grunnkrets_id\"] == grunnkrets])[\"store_id\"].count()\n",
    "        dict[\"grunnkrets_id\"]+=[grunnkrets]\n",
    "        dict[\"num_of_stores_grunnkrets\"]+=[num_of_stores]\n",
    "    return pd.DataFrame.from_dict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges num_stores_grunnkrets into dataframes\n",
    "all_stores = stores_train.drop(columns=['revenue']).append(stores_test, ignore_index=True).append(stores_extra, ignore_index=True)\n",
    "all_stores=pd.merge(all_stores,grunnkrets_norway_stripped, how=\"left\", on=\"grunnkrets_id\")\n",
    "\n",
    "grunkr=find_num_stores_grunnkrets(all_stores)\n",
    "\n",
    "stores_train= left_merge(stores_train, grunkr, on=\"grunnkrets_id\")\n",
    "\n",
    "stores_test= left_merge(stores_test, grunkr, on=\"grunnkrets_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates info about malls\n",
    "from scipy import spatial\n",
    "\n",
    "def mall_df(X):\n",
    "    #alls=X.loc[:,\"mall_name\"].unique()\n",
    "    #mallss = malls[np.logical_not(pd.isnull(malls))]\n",
    "    all_stores = np.array(X.values)\n",
    "    df69 = X.groupby('mall_name', as_index=False)[\"lat\",\"lon\"].mean()\n",
    "    df69_nump = np.array(X.groupby('mall_name')[\"lat\",\"lon\"].mean())\n",
    "\n",
    "\n",
    "    lat_ind = X.columns.get_loc(\"lat\")\n",
    "    lon_ind = X.columns.get_loc(\"lon\")\n",
    "    store_id_ind = X.columns.get_loc(\"store_id\")\n",
    "\n",
    "    dict={\"store_id\":[], \"mall_distance\":[], \"closest_mall_name\":[]}\n",
    "\n",
    "    i=0\n",
    "    maxxx=len(all_stores)\n",
    "    for store in all_stores:\n",
    "        if i%5000==0:\n",
    "            print(100*(i/maxxx),\"%\")\n",
    "        i+=1\n",
    "\n",
    "        #distance=math.inf\n",
    "        #closest_mall=None\n",
    "        lat = store[lat_ind]\n",
    "        lon = store[lon_ind]\n",
    "        store_id=store[store_id_ind]\n",
    "        coords_1 = [lat,lon]\n",
    "\n",
    "        #A = df69_nump[spatial.KDTree(df69_nump).query(coords_1)[1]] # <-- the nearest point \n",
    "        B = spatial.KDTree(df69_nump).query(coords_1)\n",
    "        distance=B[0]\n",
    "        mall =df69.iloc[[B[1]]]\n",
    "        dict[\"store_id\"]+=[store_id]\n",
    "        dict[\"mall_distance\"]+=[distance]\n",
    "        dict[\"closest_mall_name\"]+=[mall.values[0][0]]\n",
    "        #print(distance)\n",
    "        #print(mall)\n",
    "        #print(\"A\", A)\n",
    "\n",
    "\n",
    "    return pd.DataFrame.from_dict(dict)\n",
    "\n",
    "\n",
    "#df69 = all_stores.groupby('mall_name')[\"lat\",\"lon\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1433836/2057895009.py:8: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df69 = X.groupby('mall_name', as_index=False)[\"lat\",\"lon\"].mean()\n",
      "/tmp/ipykernel_1433836/2057895009.py:9: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  df69_nump = np.array(X.groupby('mall_name')[\"lat\",\"lon\"].mean())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 %\n",
      "10.029486690871162 %\n",
      "20.058973381742323 %\n",
      "30.088460072613483 %\n",
      "40.117946763484646 %\n",
      "50.147433454355806 %\n",
      "60.176920145226966 %\n",
      "70.20640683609813 %\n",
      "80.23589352696929 %\n",
      "90.26538021784046 %\n"
     ]
    }
   ],
   "source": [
    "# Merges mall info to the dataframes\n",
    "mall_df = mall_df(all_stores)\n",
    "\n",
    "stores_train = pd.merge(stores_train, mall_df, how=\"left\", on=\"store_id\")\n",
    "stores_test = pd.merge(stores_test, mall_df, how=\"left\", on=\"store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates avg_rev for chains\n",
    "def chain_info(stores_train):\n",
    "    df69 = stores_train.groupby('chain_name', as_index=True)[\"store_id\"].count()\n",
    "    big_chain = df69.loc[df69 >= 1]\n",
    "    big_chain = big_chain.dropna()\n",
    "\n",
    "    dict = {\"chain_name\":[], \"avg_rev_big_chain\":[]}\n",
    "\n",
    "    for i in range(len(big_chain)):\n",
    "        avg_rev = stores_train.loc[stores_train[\"chain_name\"] == big_chain.index[i]][\"revenue\"].mean()\n",
    "        dict[\"chain_name\"]+=[big_chain.index[i]]\n",
    "        dict[\"avg_rev_big_chain\"]+=[avg_rev]\n",
    "        #print(big_chain.index[i], avg_rev)\n",
    "    big_chain_df = pd.DataFrame.from_dict(dict)\n",
    "    return big_chain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merges avg_rev for chains into dataframes\n",
    "avg_rev_big_chain= chain_info(stores_train)\n",
    "\n",
    "stores_train = left_merge(stores_train, avg_rev_big_chain, on=\"chain_name\")\n",
    "stores_test = left_merge(stores_test, avg_rev_big_chain, on=\"chain_name\")\n",
    "\n",
    "stores_train = replace_missing(stores_train, 'avg_rev_big_chain' , MEAN_REVENUE)\n",
    "stores_test = replace_missing(stores_test, 'avg_rev_big_chain' , MEAN_REVENUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds avg_rev for different hierarchy levels\n",
    "def avg_rev_hierarchy_levels(X):\n",
    "    lv_1_hierarkies = (X.loc[:,\"lv1_desc\"]).unique()\n",
    "    lv_2_hierarkies = (X.loc[:,\"lv2_desc\"]).unique()\n",
    "    lv_3_hierarkies = (X.loc[:,\"lv3_desc\"]).unique()\n",
    "    lv_4_hierarkies = (X.loc[:,\"lv4_desc\"]).unique()\n",
    "\n",
    "    dict = {\"lv1_desc\":[],\"lv1_avg_rev\":[]}\n",
    "    for lv1 in lv_1_hierarkies:\n",
    "        avg_rev = (X.loc[X[\"lv1_desc\"] == lv1])[\"revenue\"].mean()\n",
    "        dict[\"lv1_desc\"]+=[lv1]\n",
    "        dict[\"lv1_avg_rev\"]+=[avg_rev]\n",
    "    lv1_df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "    dict = {\"lv2_desc\":[],\"lv2_avg_rev\":[]}\n",
    "    for lv2 in lv_2_hierarkies:\n",
    "        avg_rev = (X.loc[X[\"lv2_desc\"] == lv2])[\"revenue\"].mean()\n",
    "        dict[\"lv2_desc\"]+=[lv2]\n",
    "        dict[\"lv2_avg_rev\"]+=[avg_rev]\n",
    "    lv2_df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "    dict = {\"lv3_desc\":[],\"lv3_avg_rev\":[]}\n",
    "    for lv3 in lv_3_hierarkies:\n",
    "        avg_rev = (X.loc[X[\"lv3_desc\"] == lv3])[\"revenue\"].mean()\n",
    "        dict[\"lv3_desc\"]+=[lv3]\n",
    "        dict[\"lv3_avg_rev\"]+=[avg_rev]\n",
    "    lv3_df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "    dict = {\"lv4_desc\":[],\"lv4_avg_rev\":[]}\n",
    "    for lv4 in lv_4_hierarkies:\n",
    "        avg_rev = (X.loc[X[\"lv4_desc\"] == lv4])[\"revenue\"].mean()\n",
    "        dict[\"lv4_desc\"]+=[lv4]\n",
    "        dict[\"lv4_avg_rev\"]+=[avg_rev]\n",
    "    lv4_df = pd.DataFrame.from_dict(dict)\n",
    "\n",
    "    return lv1_df, lv2_df, lv3_df, lv4_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lv4_avg_rev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.639134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.624417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.453620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.255351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.509913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.894660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.102165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lv4_avg_rev\n",
       "count    90.000000\n",
       "mean      1.639134\n",
       "std       0.624417\n",
       "min       0.453620\n",
       "25%       1.255351\n",
       "50%       1.509913\n",
       "75%       1.894660\n",
       "max       4.102165"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add useful features\n",
    "_,_,lv3,lv4 = avg_rev_hierarchy_levels(stores_train)\n",
    "\n",
    "stores_train = pd.merge(stores_train, lv3, how=\"left\", on=\"lv3_desc\")\n",
    "stores_train = pd.merge(stores_train, lv4, how=\"left\", on=\"lv4_desc\")\n",
    "\n",
    "stores_test = pd.merge(stores_test, lv3, how=\"left\", on=\"lv3_desc\")\n",
    "stores_test = pd.merge(stores_test, lv4, how=\"left\", on=\"lv4_desc\")\n",
    "\n",
    "lv4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_id                   918023925-918087567-872230\n",
      "lat                                         68.779439\n",
      "lon                                         16.566267\n",
      "geometry    POINT (68.77943861059249 16.566267033764)\n",
      "cluster                                            33\n",
      "Name: 15, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1433836/398162004.py:29: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  cluster_center = all_stores.groupby('cluster')['lat','lon'].mean()\n"
     ]
    }
   ],
   "source": [
    "# magnus cluster feature\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import sklearn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "revenues = stores_train[['store_id', 'revenue']]\n",
    "all_stores = stores_train.drop(columns=['revenue']).append(stores_test, ignore_index=True).append(stores_extra, ignore_index=True)\n",
    "\n",
    "df=all_stores[['store_id','lat','lon']]\n",
    "geometry = [Point(xy) for xy in zip(df.lat, df.lon)]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "latlon = all_stores[['lat','lon']]\n",
    "X = np.radians(np.array(latlon, dtype='float64'))\n",
    "\n",
    "clustering = DBSCAN(eps=2.0/6371.0, min_samples=250, metric='haversine', algorithm='ball_tree').fit(X)\n",
    "\n",
    "y_clusters = clustering.labels_\n",
    "\n",
    "gdf['cluster'] = y_clusters\n",
    "\n",
    "all_stores['cluster'] = y_clusters\n",
    "gdf=gdf[gdf['cluster']!=-1]\n",
    "gdf.crs = \"EPSG:9672\"\n",
    "print(gdf.loc[gdf['cluster'].idxmax()])\n",
    "\n",
    "cluster_center = all_stores.groupby('cluster')['lat','lon'].mean()\n",
    "\n",
    "cluster_center.rename(columns = {'lat':'center_lat', 'lon':'center_lon'}, inplace = True)\n",
    "\n",
    "all_stores = left_merge(all_stores, cluster_center, on='cluster')\n",
    "\n",
    "all_stores\n",
    "def distance(lat1, lon1, lat2, lon2, cluster):\n",
    "    if cluster == -1:\n",
    "        return 15\n",
    "    else:\n",
    "        return np.linalg.norm(np.array([lat1,lon1])-np.array([lat2, lon2]))\n",
    "\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2, cluster):\n",
    "    max_distance = 20\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    if cluster == -1:\n",
    "        return max_distance\n",
    "    else:\n",
    "        # convert decimal degrees to radians \n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        # haversine formula \n",
    "        dlon = lon2 - lon1 \n",
    "        dlat = lat2 - lat1 \n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * asin(sqrt(a)) \n",
    "        km = 6371 * c\n",
    "        if km >= max_distance:\n",
    "            return max_distance\n",
    "        else:\n",
    "            return km\n",
    "    \n",
    "all_stores['center_distance'] = np.exp(all_stores[['lon', 'lat','center_lon','center_lat', 'cluster']].apply(lambda row: haversine(row['lon'], row['lat'], row['center_lon'], row['center_lat'], row['cluster']), axis=1))\n",
    "\n",
    "stores_train = all_stores[all_stores['store_id'].isin(stores_train['store_id'])]\n",
    "stores_train = left_merge(stores_train, revenues, on='store_id')\n",
    "stores_test = all_stores[all_stores['store_id'].isin(stores_test['store_id'])]\n",
    "stores_extra = all_stores[all_stores['store_id'].isin(stores_extra['store_id'])]\n",
    "df=all_stores[['store_id','lat','lon', 'cluster', 'center_distance', 'store_name']]\n",
    "geometry = [Point(xy) for xy in zip(df.lat, df.lon)]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "gdf=gdf[gdf['cluster']!=-1]\n",
    "sample=gdf.sample(frac=0.25)\n",
    "sample.crs = \"EPSG:9672\"\n",
    "#sample.explore(\"cluster\", marker_type=\"circle_marker\",marker_kwds={\"radius\":8}, legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores_train.loc[stores_train[\"cluster\"] == -1].head(10)\n",
    "\n",
    "#save id for later\n",
    "stores_test_id = np.asarray(stores_test.store_id)\n",
    "\n",
    "remove_columns = [\n",
    "    'address',\n",
    "    'store_name',\n",
    "    'sales_channel_name',\n",
    "    'lat',\n",
    "    'lon',\n",
    "    'center_lat',\n",
    "    'center_lon',\n",
    "    'lv1',\n",
    "    'lv2',\n",
    "    'lv3',\n",
    "    'lv4',\n",
    "    'year',\n",
    "    'store_id',\n",
    "    'plaace_hierarchy_id',\n",
    "]\n",
    "\n",
    "# data cleaning for train\n",
    "for column in remove_columns:\n",
    "    #print(column)\n",
    "    stores_train = remove_column(stores_train, column)\n",
    "    stores_test = remove_column(stores_test, column)\n",
    "#stores_train = remove_column(stores_train, 'grunnkrets_id')\n",
    "\n",
    "\n",
    "stores_train = replace_missing(stores_train, 'district_name', 'No district')\n",
    "stores_train = replace_missing(stores_train, 'municipality_name' , \"No municipality_name\")\n",
    "stores_train = replace_missing(stores_train, 'area_km2' , 0.0)\n",
    "\n",
    "stores_test = replace_missing(stores_test, 'district_name', 'No district')\n",
    "stores_test = replace_missing(stores_test, 'municipality_name' , \"No municipality_name\")\n",
    "stores_test = replace_missing(stores_test, 'area_km2' , 0.0)\n",
    "\n",
    "stores_test = replace_missing(stores_test, \"lv3_avg_rev\", MEAN_REVENUE)\n",
    "stores_test = replace_missing(stores_test, \"lv4_avg_rev\", MEAN_REVENUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace inf values\n",
    "stores_train.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "stores_test.replace([np.inf, -np.inf], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating training data\n",
    "\n",
    "X_train = stores_train.drop(columns=['revenue'])\n",
    "y_train = stores_train.revenue\n",
    "X_test = stores_test\n",
    "\n",
    "# encode categorical features\n",
    "\n",
    "\n",
    "categorical_features = ['chain_name',\n",
    "                        'grunnkrets_id',\n",
    "                        'municipality_name',\n",
    "                        \"closest_mall_name\",\n",
    "                        'cluster',\n",
    "                        'lv1_desc',\n",
    "                        'lv2_desc',\n",
    "                        'lv3_desc',\n",
    "                        'lv4_desc',\n",
    "                        'mall_name',\n",
    "                        'district_name',\n",
    "                       ]\n",
    "\n",
    "for feature in categorical_features:\n",
    "    X_train[feature] = X_train[feature].astype('category')\n",
    "    X_test[feature] = X_test[feature].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoding\n",
    "le = LabelEncoder()\n",
    "X_train_cat = X_train.copy()\n",
    "X_test_cat = X_test.copy()\n",
    "y_train_cat = y_train.copy()\n",
    "\n",
    "\n",
    "X_train_shape = X_train.shape[0]\n",
    "X_test_shape = X_test.shape[0]\n",
    "\n",
    "X_train[\"train_test_split\"] = [0 for i in range(X_train_shape)]\n",
    "X_test[\"train_test_split\"] = [1 for i in range(X_test_shape)]\n",
    "\n",
    "X_train_test = X_train.append(X_test, ignore_index=True)\n",
    "\n",
    "X_train_test[categorical_features] = X_train_test[categorical_features].apply(le.fit_transform)\n",
    "X_train = X_train_test.loc[X_train_test[\"train_test_split\"] == 0]\n",
    "X_test = X_train_test.loc[X_train_test[\"train_test_split\"] == 1]\n",
    "\n",
    "X_train=X_train.drop([\"train_test_split\"], axis=1)\n",
    "X_test=X_test.drop([\"train_test_split\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSLEMetric(object):\n",
    "    def get_final_error(self, error, weight):\n",
    "        return error\n",
    "\n",
    "    def is_max_optimal(self):\n",
    "        # the larger metric value the better\n",
    "        return False\n",
    "\n",
    "    def evaluate(self, approxes, target, weight):\n",
    "        assert len(approxes) == 1\n",
    "        assert len(target) == len(approxes[0])\n",
    "        preds = np.array(approxes[0])\n",
    "        target = np.array(target)\n",
    "        preds = np.exp(preds) - 1\n",
    "        target = np.exp(target) - 1\n",
    "        for i in range(len(preds)):\n",
    "            if preds[i]<0:\n",
    "                preds[i] = 0\n",
    "        assert (target >= 0).all(), 'Received negative target values'\n",
    "        assert (preds >= 0).all(), 'Received negative pred values'\n",
    "        assert target.shape == preds.shape, 'target and pred have different shapes'\n",
    "        target_log1p = np.log1p(target)  # log(1 + y_true)\n",
    "        preds_log1p = np.log1p(preds)  # log(1 + y_pred)\n",
    "        \n",
    "        return np.sqrt(np.mean(np.square(preds_log1p - target_log1p))), 0\n",
    "\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Root Mean Squared Logarithmic Error \n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): n-dimensional vector of ground-truth values \n",
    "        y_pred (np.array): n-dimensional vecotr of predicted values \n",
    "    \n",
    "    Returns:\n",
    "        A scalar float with the rmsle value \n",
    "    \n",
    "    Note: You can alternatively use sklearn and just do: \n",
    "        `sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5`\n",
    "    \"\"\"\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i]<0:\n",
    "            y_pred[i] = 0\n",
    "    assert (y_true >= 0).all(), 'Received negative y_true values'\n",
    "    assert (y_pred >= 0).all(), 'Received negative y_pred values'\n",
    "    assert y_true.shape == y_pred.shape, 'y_true and y_pred have different shapes'\n",
    "    y_true = np.exp(y_true) - 1\n",
    "    y_pred = np.exp(y_pred) - 1\n",
    "    y_true_log1p = np.log1p(y_true)  # log(1 + y_true)\n",
    "    y_pred_log1p = np.log1p(y_pred)  # log(1 + y_pred)\n",
    "    print(np.sqrt(np.mean(np.square(y_pred_log1p - y_true_log1p))))\n",
    "    return np.sqrt(np.mean(np.square(y_pred_log1p - y_true_log1p)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m   - Of which 4 did not match due to:\n",
      "   Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "     With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m    Rejected as the implementation raised a specific error:\n",
      "      TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "    \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "     \n",
      "     >>> stub(array(float64, 1d, C))\n",
      "     \n",
      "    There are 2 candidate implementations:\n",
      "    \u001b[1m  - Of which 2 did not match due to:\n",
      "      Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "    \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "         TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "      raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "    \u001b[0m\n",
      "    \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "    \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "    \u001b[0m\n",
      "    \u001b[1m\n",
      "    File \"<string>\", line 3:\u001b[0m\n",
      "    \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1433836/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1433836/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "/lhome/frimanng/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(colsample_bytree=0.95, lambda_l1=0.5, learning_rate=0.008,\n",
       "              max_depth=8, metric='rmsle', n_estimators=1500, num_leaves=8,\n",
       "              random_state=8, silent=True, subsample=0.2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "cat_params =     {\n",
    "    \"random_state\":8,\n",
    "    \"learning_rate\":0.01,\n",
    "    \"max_depth\":8,\n",
    "    \"eval_metric\":RMSLEMetric(),\n",
    "    \"cat_features\":categorical_features,\n",
    "    \"n_estimators\":3000,\n",
    "    \"od_type\":\"Iter\",\n",
    "    \"silent\":True\n",
    "}\n",
    "\n",
    "random_forest_params = {\n",
    "    \"n_estimators\":2000,\n",
    "    \"max_depth\":10,\n",
    "    \"random_state\":8,\n",
    "    \n",
    "    \"max_features\":None,\n",
    "}\n",
    "\n",
    "light_gbm_params = {\n",
    "    \"num_leaves\":8,\n",
    "    \"max_depth\":8, \n",
    "    \"random_state\":8,\n",
    "    \"silent\":True, \n",
    "    \"metric\":'rmsle',\n",
    "    \"n_jobs\":-1,\n",
    "    #\"min_data_in_leaf\": 20,\n",
    "    \"lambda_l1\": 0.5,\n",
    "    \"n_estimators\":1500,\n",
    "    \"colsample_bytree\":0.95,\n",
    "    \"subsample\":0.2,\n",
    "    \"learning_rate\":0.008,\n",
    "}\n",
    "\n",
    "model_light_gbm = lgb.LGBMRegressor(**light_gbm_params)\n",
    "# model_random_forest = RandomForestRegressor(**random_forest_params)\n",
    "model_catboost= CatBoostRegressor(**cat_params)\n",
    "\n",
    "model_catboost.fit(X_train, y_train)\n",
    "#model_random_forest.fit(X_train, y_train)\n",
    "model_light_gbm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Features</th>\n",
       "      <th>CatBoost_importance</th>\n",
       "      <th>Light_gbm_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avg_rev_big_chain</td>\n",
       "      <td>25.174228</td>\n",
       "      <td>1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>closest_mall_name</td>\n",
       "      <td>6.141035</td>\n",
       "      <td>460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lv4_desc</td>\n",
       "      <td>6.122676</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lv4_avg_rev</td>\n",
       "      <td>5.963669</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mall_distance</td>\n",
       "      <td>5.704027</td>\n",
       "      <td>1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>num_of_stores_grunnkrets</td>\n",
       "      <td>5.246388</td>\n",
       "      <td>932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lv3_desc</td>\n",
       "      <td>4.927102</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cluster</td>\n",
       "      <td>4.671898</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>district_name</td>\n",
       "      <td>4.563575</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>municipality_name</td>\n",
       "      <td>4.498663</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>area_km2</td>\n",
       "      <td>3.968404</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>lv1_desc</td>\n",
       "      <td>3.942529</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chain_name</td>\n",
       "      <td>3.758764</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>lv2_desc</td>\n",
       "      <td>3.556027</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mall_name</td>\n",
       "      <td>3.291732</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>grunnkrets_id</td>\n",
       "      <td>3.027269</td>\n",
       "      <td>745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>lv3_avg_rev</td>\n",
       "      <td>2.931758</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>center_distance</td>\n",
       "      <td>2.510256</td>\n",
       "      <td>816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Features  CatBoost_importance  Light_gbm_importance\n",
       "0          avg_rev_big_chain            25.174228                  1548\n",
       "1          closest_mall_name             6.141035                   460\n",
       "2                   lv4_desc             6.122676                   302\n",
       "3                lv4_avg_rev             5.963669                   631\n",
       "4              mall_distance             5.704027                  1115\n",
       "5   num_of_stores_grunnkrets             5.246388                   932\n",
       "6                   lv3_desc             4.927102                   510\n",
       "7                    cluster             4.671898                   167\n",
       "8              district_name             4.563575                   295\n",
       "9          municipality_name             4.498663                   496\n",
       "10                  area_km2             3.968404                   890\n",
       "11                  lv1_desc             3.942529                   136\n",
       "12                chain_name             3.758764                   100\n",
       "13                  lv2_desc             3.556027                   324\n",
       "14                 mall_name             3.291732                   379\n",
       "15             grunnkrets_id             3.027269                   745\n",
       "16               lv3_avg_rev             2.931758                   654\n",
       "17           center_distance             2.510256                   816"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_for_model = [f for f in X_train]\n",
    "feat_import = [t for t in zip(features_for_model,model_catboost.get_feature_importance())]\n",
    "feat_import_df = pd.DataFrame(feat_import, columns = ['Features', 'CatBoost_importance'])\n",
    "df_cat =feat_import_df.sort_values(\"CatBoost_importance\", ascending= False)\n",
    "\n",
    "\n",
    "features_for_model = [f for f in X_train]\n",
    "feat_import = [t for t in zip(features_for_model,model_light_gbm.feature_importances_)]\n",
    "feat_import_df = pd.DataFrame(feat_import, columns = ['Features', 'Light_gbm_importance'])\n",
    "df_lgbm =feat_import_df.sort_values(\"Light_gbm_importance\", ascending= False)\n",
    "\n",
    "df = pd.merge(df_cat,df_lgbm, how=\"left\", on=\"Features\")\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m      - Of which 4 did not match due to:\n",
      "      Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "       \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "        \n",
      "        >>> stub(array(float64, 1d, C))\n",
      "        \n",
      "       There are 2 candidate implementations:\n",
      "       \u001b[1m  - Of which 2 did not match due to:\n",
      "         Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "           With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "       \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "            TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "         raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "       \u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "       \u001b[0m\n",
      "       \u001b[1m\n",
      "       File \"<string>\", line 3:\u001b[0m\n",
      "       \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1433836/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1433836/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6616371686038339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m      - Of which 4 did not match due to:\n",
      "      Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "       \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "        \n",
      "        >>> stub(array(float64, 1d, C))\n",
      "        \n",
      "       There are 2 candidate implementations:\n",
      "       \u001b[1m  - Of which 2 did not match due to:\n",
      "         Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "           With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "       \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "            TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "         raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "       \u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "       \u001b[0m\n",
      "       \u001b[1m\n",
      "       File \"<string>\", line 3:\u001b[0m\n",
      "       \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1433836/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1433836/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6657786946250365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m      - Of which 4 did not match due to:\n",
      "      Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "       \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "        \n",
      "        >>> stub(array(float64, 1d, C))\n",
      "        \n",
      "       There are 2 candidate implementations:\n",
      "       \u001b[1m  - Of which 2 did not match due to:\n",
      "         Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "           With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "       \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "            TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "         raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "       \u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "       \u001b[0m\n",
      "       \u001b[1m\n",
      "       File \"<string>\", line 3:\u001b[0m\n",
      "       \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1433836/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1433836/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6706272673450171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/catboost/core.py:1759: UserWarning: Failed to optimize method \"evaluate\" in the passed object:\n",
      "Failed in nopython mode pipeline (step: nopython frontend)\n",
      "\u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<built-in function array>) found for signature:\n",
      " \n",
      " >>> array(array(float64, 1d, C))\n",
      " \n",
      "There are 4 candidate implementations:\n",
      "\u001b[1m      - Of which 4 did not match due to:\n",
      "      Overload in function '_OverloadWrapper._build.<locals>.ol_generated': File: numba/core/overload_glue.py: Line 131.\n",
      "        With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "\u001b[1m       Rejected as the implementation raised a specific error:\n",
      "         TypingError: Failed in nopython mode pipeline (step: nopython frontend)\n",
      "       \u001b[1m\u001b[1m\u001b[1mNo implementation of function Function(<intrinsic stub>) found for signature:\n",
      "        \n",
      "        >>> stub(array(float64, 1d, C))\n",
      "        \n",
      "       There are 2 candidate implementations:\n",
      "       \u001b[1m  - Of which 2 did not match due to:\n",
      "         Intrinsic in function 'stub': File: numba/core/overload_glue.py: Line 35.\n",
      "           With argument(s): '(array(float64, 1d, C))':\u001b[0m\n",
      "       \u001b[1m   Rejected as the implementation raised a specific error:\n",
      "            TypingError: \u001b[1marray(float64, 1d, C) not allowed in a homogeneous sequence\u001b[0m\u001b[0m\n",
      "         raised from /usr/lib/python3/dist-packages/numba/core/typing/npydecl.py:487\n",
      "       \u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: resolving callee type: Function(<intrinsic stub>)\u001b[0m\n",
      "       \u001b[0m\u001b[1mDuring: typing of call at <string> (3)\n",
      "       \u001b[0m\n",
      "       \u001b[1m\n",
      "       File \"<string>\", line 3:\u001b[0m\n",
      "       \u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\u001b[0m\n",
      "  raised from /usr/lib/python3/dist-packages/numba/core/typeinfer.py:1086\n",
      "\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: resolving callee type: Function(<built-in function array>)\u001b[0m\n",
      "\u001b[0m\u001b[1mDuring: typing of call at /tmp/ipykernel_1433836/1168404273.py (12)\n",
      "\u001b[0m\n",
      "\u001b[1m\n",
      "File \"../../../../tmp/ipykernel_1433836/1168404273.py\", line 12:\u001b[0m\n",
      "\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n",
      "\n",
      "  self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6694457213354614\n",
      "catboost\n",
      "Scores for all folds: \n",
      "[0.66163717 0.66577869 0.67062727 0.66944572]\n",
      "\n",
      "\n",
      "Mean of scores:\n",
      "0.6668722129773372\n",
      "\n",
      "\n",
      "Standard deviation of scores: \n",
      "0.0035115826400974083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lhome/frimanng/.local/lib/python3.10/site-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
      "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6650135025216873\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "0.6740535014115837\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "0.6743513789702983\n",
      "[LightGBM] [Warning] lambda_l1 is set=0.5, reg_alpha=0.0 will be ignored. Current value: lambda_l1=0.5\n",
      "0.6737243604951957\n",
      "light gbm\n",
      "Scores for all folds: \n",
      "[0.6650135  0.6740535  0.67435138 0.67372436]\n",
      "\n",
      "\n",
      "Mean of scores:\n",
      "0.6717856858496913\n",
      "\n",
      "\n",
      "Standard deviation of scores: \n",
      "0.003916206551661562\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits = 4, random_state=42, shuffle=True)\n",
    "score = cross_val_score(model_catboost, X_train, y_train, cv=cv, scoring=make_scorer(rmsle))\n",
    "print(\"catboost\")\n",
    "print('Scores for all folds: ')\n",
    "print(score)\n",
    "print('\\n')\n",
    "print('Mean of scores:')\n",
    "print(np.mean(score))\n",
    "print('\\n')\n",
    "print('Standard deviation of scores: ')\n",
    "print(np.std(score))\n",
    "\n",
    "cv = KFold(n_splits = 4, random_state=42, shuffle=True)\n",
    "score = cross_val_score(model_light_gbm, X_train, y_train, cv=cv, scoring=make_scorer(rmsle))\n",
    "\n",
    "print(\"light gbm\")\n",
    "print('Scores for all folds: ')\n",
    "print(score)\n",
    "print('\\n')\n",
    "print('Mean of scores:')\n",
    "print(np.mean(score))\n",
    "print('\\n')\n",
    "print('Standard deviation of scores: ')\n",
    "print(np.std(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grunnkrets_id</th>\n",
       "      <th>chain_name</th>\n",
       "      <th>mall_name</th>\n",
       "      <th>lv1_desc</th>\n",
       "      <th>lv2_desc</th>\n",
       "      <th>lv3_desc</th>\n",
       "      <th>lv4_desc</th>\n",
       "      <th>district_name</th>\n",
       "      <th>municipality_name</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>num_of_stores_grunnkrets</th>\n",
       "      <th>mall_distance</th>\n",
       "      <th>closest_mall_name</th>\n",
       "      <th>avg_rev_big_chain</th>\n",
       "      <th>lv3_avg_rev</th>\n",
       "      <th>lv4_avg_rev</th>\n",
       "      <th>cluster</th>\n",
       "      <th>center_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12592</th>\n",
       "      <td>922</td>\n",
       "      <td>345</td>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>1263</td>\n",
       "      <td>249</td>\n",
       "      <td>0.057027</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.011638</td>\n",
       "      <td>191</td>\n",
       "      <td>1.290239</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2</td>\n",
       "      <td>5.315833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12593</th>\n",
       "      <td>2189</td>\n",
       "      <td>52</td>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>329</td>\n",
       "      <td>299</td>\n",
       "      <td>0.165993</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.015869</td>\n",
       "      <td>237</td>\n",
       "      <td>1.830112</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>9</td>\n",
       "      <td>2.671965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12594</th>\n",
       "      <td>1033</td>\n",
       "      <td>52</td>\n",
       "      <td>451</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>244</td>\n",
       "      <td>249</td>\n",
       "      <td>0.236628</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>484</td>\n",
       "      <td>1.830112</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2</td>\n",
       "      <td>8626.714236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12595</th>\n",
       "      <td>3962</td>\n",
       "      <td>52</td>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>1022</td>\n",
       "      <td>356</td>\n",
       "      <td>0.983436</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>444</td>\n",
       "      <td>1.830112</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>11</td>\n",
       "      <td>13.364370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12596</th>\n",
       "      <td>1078</td>\n",
       "      <td>334</td>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>361</td>\n",
       "      <td>249</td>\n",
       "      <td>0.449502</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>228</td>\n",
       "      <td>3.397223</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2</td>\n",
       "      <td>1.395687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12597</th>\n",
       "      <td>938</td>\n",
       "      <td>52</td>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>763</td>\n",
       "      <td>249</td>\n",
       "      <td>0.313006</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>498</td>\n",
       "      <td>1.830112</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2</td>\n",
       "      <td>1106.438239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12598</th>\n",
       "      <td>3915</td>\n",
       "      <td>207</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>671</td>\n",
       "      <td>356</td>\n",
       "      <td>0.143946</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>531</td>\n",
       "      <td>3.188035</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>11</td>\n",
       "      <td>2.817447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12599</th>\n",
       "      <td>810</td>\n",
       "      <td>345</td>\n",
       "      <td>529</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>647</td>\n",
       "      <td>249</td>\n",
       "      <td>0.191613</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>127</td>\n",
       "      <td>1.290239</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2</td>\n",
       "      <td>41.561478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12600</th>\n",
       "      <td>1397</td>\n",
       "      <td>207</td>\n",
       "      <td>453</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>910</td>\n",
       "      <td>181</td>\n",
       "      <td>0.395770</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.005933</td>\n",
       "      <td>486</td>\n",
       "      <td>3.188035</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>7</td>\n",
       "      <td>2.003095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12601</th>\n",
       "      <td>3655</td>\n",
       "      <td>207</td>\n",
       "      <td>531</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>777</td>\n",
       "      <td>404</td>\n",
       "      <td>0.120695</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>573</td>\n",
       "      <td>3.188035</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>2.096344</td>\n",
       "      <td>28</td>\n",
       "      <td>1.182130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       grunnkrets_id  chain_name  mall_name  lv1_desc  lv2_desc  lv3_desc  \\\n",
       "12592            922         345        529         0        15        35   \n",
       "12593           2189          52        529         0        15        35   \n",
       "12594           1033          52        451         0        15        35   \n",
       "12595           3962          52        529         0        15        35   \n",
       "12596           1078         334        529         0        15        35   \n",
       "12597            938          52        529         0        15        35   \n",
       "12598           3915         207        496         0        15        35   \n",
       "12599            810         345        529         0        15        35   \n",
       "12600           1397         207        453         0        15        35   \n",
       "12601           3655         207        531         0        15        35   \n",
       "\n",
       "       lv4_desc  district_name  municipality_name  area_km2  \\\n",
       "12592        38           1263                249  0.057027   \n",
       "12593        38            329                299  0.165993   \n",
       "12594        38            244                249  0.236628   \n",
       "12595        38           1022                356  0.983436   \n",
       "12596        38            361                249  0.449502   \n",
       "12597        38            763                249  0.313006   \n",
       "12598        38            671                356  0.143946   \n",
       "12599        38            647                249  0.191613   \n",
       "12600        38            910                181  0.395770   \n",
       "12601        38            777                404  0.120695   \n",
       "\n",
       "       num_of_stores_grunnkrets  mall_distance  closest_mall_name  \\\n",
       "12592                       4.0       0.011638                191   \n",
       "12593                       9.0       0.015869                237   \n",
       "12594                     123.0       0.000049                484   \n",
       "12595                      15.0       0.016599                444   \n",
       "12596                      27.0       0.008053                228   \n",
       "12597                      10.0       0.001627                498   \n",
       "12598                      98.0       0.000242                531   \n",
       "12599                      90.0       0.003768                127   \n",
       "12600                      80.0       0.005933                486   \n",
       "12601                      33.0       0.001941                573   \n",
       "\n",
       "       avg_rev_big_chain  lv3_avg_rev  lv4_avg_rev  cluster  center_distance  \n",
       "12592           1.290239     2.096344     2.096344        2         5.315833  \n",
       "12593           1.830112     2.096344     2.096344        9         2.671965  \n",
       "12594           1.830112     2.096344     2.096344        2      8626.714236  \n",
       "12595           1.830112     2.096344     2.096344       11        13.364370  \n",
       "12596           3.397223     2.096344     2.096344        2         1.395687  \n",
       "12597           1.830112     2.096344     2.096344        2      1106.438239  \n",
       "12598           3.188035     2.096344     2.096344       11         2.817447  \n",
       "12599           1.290239     2.096344     2.096344        2        41.561478  \n",
       "12600           3.188035     2.096344     2.096344        7         2.003095  \n",
       "12601           3.188035     2.096344     2.096344       28         1.182130  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.57860254  7.18207468  5.50706869  7.12047749 34.02700927  6.16900091\n",
      " 21.0143839   6.42088054 21.86393185 22.20359843  4.89586579  5.0262228 ]\n",
      "                 0\n",
      "count  8577.000000\n",
      "mean      6.380816\n",
      "std      10.188587\n",
      "min       0.452414\n",
      "25%       2.031146\n",
      "50%       3.252137\n",
      "75%       5.367583\n",
      "max      88.469473\n",
      "[ 3.94541748  6.68174259  5.65180127  6.84852398 29.0164004   5.75816769\n",
      " 26.1222001   6.9470074  25.12626229 26.00934688  5.65706121  5.34737836]\n",
      "                 0\n",
      "count  8577.000000\n",
      "mean      6.306610\n",
      "std       9.929117\n",
      "min       0.328851\n",
      "25%       2.053865\n",
      "50%       3.261790\n",
      "75%       5.302835\n",
      "max      75.488876\n"
     ]
    }
   ],
   "source": [
    "# mean of scores\n",
    "# 0.6781866272821699 - 11-12-123108 - kaggle: 0.68349\n",
    "\n",
    "# mean of scores 3- stack\n",
    "# 6781866272821699 / 0.6839925788085071 / 0.679119033159766 - 11-12-132139 - kaggle: 0.68413\n",
    "\n",
    "# mean of scores models:\n",
    "# 0.6976428658736472 - random forest - kaggle: 0.70203\n",
    "# 0.6857052809131627 - random forest - new hyper params\n",
    "\n",
    "# Mean of scores:\n",
    "# 0.6754065055519785 - catboost\n",
    "# 0.6798965069269716 - light gbm\n",
    "\n",
    "# Mean of scores: reverted big chain to >1\n",
    "# 0.6701695186092096 - catboost\n",
    "# 0.6752403812552107 - light gbm\n",
    "\n",
    "# Mean of scores: reverted big chain to >=1\n",
    "# 0.6668722129773372\n",
    "# 0.6717856858496913\n",
    "\n",
    "\n",
    "X_test = replace_missing(X_test, \"lv3_avg_rev\", MEAN_REVENUE)\n",
    "X_test = replace_missing(X_test, \"lv4_avg_rev\", MEAN_REVENUE)\n",
    "\n",
    "\n",
    "catboost_prediction = model_catboost.predict(X_test)\n",
    "light_gbm_prediction= model_light_gbm.predict(X_test)\n",
    "#catboost_prediction_3 = model_catboost_3.predict(X_test)\n",
    "\n",
    "all_predictions= np.array([catboost_prediction, light_gbm_prediction])\n",
    "for prd in all_predictions:\n",
    "    print( np.exp(prd[:12]) -1 )\n",
    "    print(pd.DataFrame(np.exp(prd) -1 ).describe())\n",
    "\n",
    "\n",
    "\n",
    "final_prediction = np.mean( all_predictions , axis=0)\n",
    "for i in range(len(final_prediction)):\n",
    "    if final_prediction[i]<0:\n",
    "        final_prediction[i] = 0\n",
    "        \n",
    "final_prediction = np.exp(final_prediction) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            id  predicted\n",
      "0   914206820-914239427-717245   3.758477\n",
      "1   916789157-916823770-824309   6.927963\n",
      "2     913341082-977479363-2948   5.579037\n",
      "3    889682582-889697172-28720   6.983343\n",
      "4   997991699-998006945-417222  31.425063\n",
      "5   914931487-815162862-756427   5.960554\n",
      "6     967062979-972338656-6209  23.435190\n",
      "7   914631734-914748119-740036   6.679440\n",
      "8    970976361-973961837-23171  23.440726\n",
      "9    979425031-979584385-54031  24.034257\n",
      "10  914852625-914864976-744489   5.264913\n",
      "11  916756097-816761972-821991   5.184716\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8577.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.332031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.022437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.557473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.047476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.246694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.289865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>81.426084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         predicted\n",
       "count  8577.000000\n",
       "mean      6.332031\n",
       "std      10.022437\n",
       "min       0.557473\n",
       "25%       2.047476\n",
       "50%       3.246694\n",
       "75%       5.289865\n",
       "max      81.426084"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate .csv-submission\n",
    "\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = stores_test_id\n",
    "submission['predicted'] = np.asarray(final_prediction)\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now() # current date and time\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H%M%S\")\n",
    "\n",
    "path = '../predictions'\n",
    "prefix = 'submission'\n",
    "suffix = '.csv'\n",
    "\n",
    "filename = \"\".join([\"-\".join([prefix,timestamp]),suffix])\n",
    "\n",
    "#eda.create_report(submission)\n",
    "#submission.to_csv(\"/\".join([path,filename]), index=False)\n",
    "\n",
    "print(submission[:12])\n",
    "submission.describe()\n",
    "\n",
    "\n",
    "\n",
    "# same as submission-2022-11-13-175627"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
